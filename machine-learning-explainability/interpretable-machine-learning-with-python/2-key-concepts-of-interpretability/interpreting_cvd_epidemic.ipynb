{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "interpreting-cvd-epidemic.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPToOeB7Ko6fSx6HiwET0/g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/interpretable-machine-learning-with-python/blob/main/2-key-concepts-of-interpretability/interpreting_cvd_epidemic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1yfxc8r4Opp"
      },
      "source": [
        "## Interpreting CVD(Cardiovascular Diseases) epidemic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUA2eHBr4kVQ"
      },
      "source": [
        "Imagine you are an analyst for a national health ministry, and there's a Cardiovascular Diseases (CVDs) epidemic. The minister has made it a priority to reverse the growth and reduce the case load to a 20-year low. To this end, a task force has been created to find clues in the data to ascertain the following:\n",
        "\n",
        "- What risk factors can be addressed.\n",
        "- If future cases can be predicted, interpret predictions on a case-by-case basis.\n",
        "\n",
        "You are part of this task force!\n",
        "\n",
        "Before we dive into the data, we must gather some important details about CVD in order to do the following:\n",
        "\n",
        "- Understand the problem's context and relevance.\n",
        "- Extract domain knowledge information that can inform our data analysis and\n",
        "model interpretation.\n",
        "- Relate an expert-informed background to a dataset's features.\n",
        "\n",
        "CVDs are a group of disorders, the most common of which is coronary heart disease (also known as Ischaemic Heart Disease). According to the World Health Organization, CVD is the leading cause of death globally, killing close to 18 million people annually. Coronary heart disease and strokes (which are, for the most part, a byproduct of CVD) are the most significant contributors to that. It is estimated that 80% of CVD is made up of modifiable risk factors.\n",
        "\n",
        "In other words, some of the preventable factors that cause CVD include\n",
        "the following:\n",
        "\n",
        "- Poor diet\n",
        "- Smoking and alcohol consumption habits\n",
        "- Obesity\n",
        "- Lack of physical activity\n",
        "- Poor sleep\n",
        "\n",
        "Also, many of the risk factors are non-modifiable, and therefore known to be unavoidable, including the following:\n",
        "\n",
        "- Genetic predisposition\n",
        "- Old age\n",
        "- Male (varies with age)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTmYHzq76SzA"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcWqLy-O6UUh",
        "outputId": "5e2b4dcd-a9e6-469a-adfa-ab477ed22561",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install --upgrade machine-learning-datasets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting machine-learning-datasets\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/a1/9a6c6642cdac1d2a6458ee51a00d6f0b13664c73fbd0d75c43ba7af7a0d9/machine_learning_datasets-0.1.16.4-py3-none-any.whl\n",
            "Collecting aif360<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/e9/f2a936a00c65ce7b4d45587a33c4522fabd773f7325d505a87d133a1dabc/aif360-0.3.0-py3-none-any.whl (165kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: matplotlib<4.0.0,>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from machine-learning-datasets) (3.2.2)\n",
            "Collecting opencv-python<5.0.0,>=4.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/b3/3878691fec6babd78bbf4c71c720e1831cbb6ada61679613fe2fae080568/opencv_python-4.5.2.54-cp37-cp37m-manylinux2014_x86_64.whl (51.0MB)\n",
            "\u001b[K     |████████████████████████████████| 51.0MB 59kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy<2.0.0,>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from machine-learning-datasets) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0.0,>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from machine-learning-datasets) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.41.1 in /usr/local/lib/python3.7/dist-packages (from machine-learning-datasets) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: seaborn<0.12.0,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from machine-learning-datasets) (0.11.1)\n",
            "Collecting pathlib2<3.0.0,>=2.3.5\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/45/9c82d3666af4ef9f221cbb954e1d77ddbb513faf552aea6df5f37f1a4859/pathlib2-2.3.5-py2.py3-none-any.whl\n",
            "Collecting pycebox<0.0.2,>=0.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/43/af/85e2e9dac1603a5c16d5989973fe4a96ee0424e0dbd39e1d43ee24556056/pycebox-0.0.1.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn<0.23.0,>=0.22.2.post1 in /usr/local/lib/python3.7/dist-packages (from machine-learning-datasets) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: pandas<2.0.0,>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from machine-learning-datasets) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: mlxtend<0.15.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from machine-learning-datasets) (0.14.0)\n",
            "Collecting alibi<0.6.0,>=0.5.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/29/d971245c07646cb5b5ed7e9203a7f013573f9e90134d1b8f159cc57ea58d/alibi-0.5.8-py3-none-any.whl (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 38.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: statsmodels<0.11.0,>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from machine-learning-datasets) (0.10.2)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.2.2->machine-learning-datasets) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.2.2->machine-learning-datasets) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.2.2->machine-learning-datasets) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.2.2->machine-learning-datasets) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from pathlib2<3.0.0,>=2.3.5->machine-learning-datasets) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.23.0,>=0.22.2.post1->machine-learning-datasets) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0.0,>=1.1.5->machine-learning-datasets) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from mlxtend<0.15.0,>=0.14.0->machine-learning-datasets) (57.0.0)\n",
            "Requirement already satisfied, skipping upgrade: spacy[lookups]<4.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from alibi<0.6.0,>=0.5.5->machine-learning-datasets) (2.2.4)\n",
            "Collecting tensorflow<2.5.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/dd/a6e880c0231416eb8ff51bf51e9b04cd08c600c01abc215f33f61cb23e6f/tensorflow-2.4.2-cp37-cp37m-manylinux2010_x86_64.whl (394.5MB)\n",
            "\u001b[K     |████████████████████████████████| 394.5MB 33kB/s \n",
            "\u001b[?25hCollecting attrs<21.0.0,>=19.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/aa/cb45262569fcc047bf070b5de61813724d6726db83259222cd7b4c79821a/attrs-20.3.0-py2.py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scikit-image!=0.17.1,<0.19,>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from alibi<0.6.0,>=0.5.5->machine-learning-datasets) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from alibi<0.6.0,>=0.5.5->machine-learning-datasets) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: Pillow<9.0,>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from alibi<0.6.0,>=0.5.5->machine-learning-datasets) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: dill<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from alibi<0.6.0,>=0.5.5->machine-learning-datasets) (0.3.4)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.7.2; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from alibi<0.6.0,>=0.5.5->machine-learning-datasets) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels<0.11.0,>=0.10.2->machine-learning-datasets) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (7.4.0)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (1.0.0)\n",
            "Collecting spacy-lookups-data<0.2.0,>=0.0.5; extra == \"lookups\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/4a37ca7d0c21dc2287a8bb5d249f5f3211cdf3d598acf742bf5bb8c87169/spacy_lookups_data-0.1.0.tar.gz (28.0MB)\n",
            "\u001b[K     |████████████████████████████████| 28.0MB 114kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (0.2.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (1.6.3)\n",
            "Collecting h5py~=2.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 25.4MB/s \n",
            "\u001b[?25hCollecting grpcio~=1.32.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/54/1c8be62beafe7fb1548d2968e518ca040556b46b0275399d4f3186c56d79/grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 14.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (2.5.0)\n",
            "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/7e/622d9849abf3afb81e482ffc170758742e392ee129ce1540611199a59237/tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.19,>=0.14.2->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (2.5.1)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.19,>=0.14.2->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.19,>=0.14.2->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (2021.5.30)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy[lookups]<4.0.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (4.5.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (3.3.4)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (0.6.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (1.31.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image!=0.17.1,<0.19,>=0.14.2->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy[lookups]<4.0.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (4.2.2)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi<0.6.0,>=0.5.5->machine-learning-datasets) (0.4.8)\n",
            "Building wheels for collected packages: pycebox, spacy-lookups-data\n",
            "  Building wheel for pycebox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycebox: filename=pycebox-0.0.1-cp37-none-any.whl size=3640 sha256=69e872d15bc2a47509579584b2a077e4c2e839c3875d918a514d705f668bb61c\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/5e/65/001126ca8b1f5c71e9571d09ba579e82bb0792062e7a82e77c\n",
            "  Building wheel for spacy-lookups-data (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy-lookups-data: filename=spacy_lookups_data-0.1.0-py2.py3-none-any.whl size=28052158 sha256=b5b3a30e39a79cd193f347a027e0eb9fbc1fa15569cc81a0ed8a72c2ea7c1cfd\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/2b/0a/d6fb6235c56d014d224bca760d15d7cbdd820813085ffcd35d\n",
            "Successfully built pycebox spacy-lookups-data\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: aif360, opencv-python, pathlib2, pycebox, gast, h5py, grpcio, tensorflow-estimator, tensorflow, attrs, alibi, machine-learning-datasets, spacy-lookups-data\n",
            "  Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "  Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Found existing installation: grpcio 1.34.1\n",
            "    Uninstalling grpcio-1.34.1:\n",
            "      Successfully uninstalled grpcio-1.34.1\n",
            "  Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "  Found existing installation: tensorflow 2.5.0\n",
            "    Uninstalling tensorflow-2.5.0:\n",
            "      Successfully uninstalled tensorflow-2.5.0\n",
            "  Found existing installation: attrs 21.2.0\n",
            "    Uninstalling attrs-21.2.0:\n",
            "      Successfully uninstalled attrs-21.2.0\n",
            "Successfully installed aif360-0.3.0 alibi-0.5.8 attrs-20.3.0 gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 machine-learning-datasets-0.1.16.4 opencv-python-4.5.2.54 pathlib2-2.3.5 pycebox-0.0.1 spacy-lookups-data-0.1.0 tensorflow-2.4.2 tensorflow-estimator-2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF9-If3F6b4-",
        "outputId": "6eb15717-9387-4f67-f6a6-1796e9b881de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import math\n",
        "import machine_learning_datasets as mldatasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSTjrEe16kUp"
      },
      "source": [
        "##The approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E_QuWQU6lRH"
      },
      "source": [
        "Logistic regression is one common way to rank risk factors in medical use cases. Unlike linear regression, it doesn't try to predict a continuous value for each of your observations, but it predicts a probability score that an observation belongs to a particular class. In this case, what we are trying to predict is, given $𝑥$ data for each patient, what is the $𝑦$ probability, from 0 to 1, that they have cardiovascular disease?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlT5MEXW61kD"
      },
      "source": [
        "##Understanding and preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6UKDgog62X9"
      },
      "source": [
        "From this, you should be getting 70,000 records and 12 columns. We can take a peek at what was loaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poq6QFfn6jsW",
        "outputId": "27e856a7-7d32-4961-e212-fccaaa6f4de1"
      },
      "source": [
        "cvd_df = mldatasets.load(\"cardiovascular-disease\")\n",
        "cvd_df.info()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://raw.githubusercontent.com/caravanuden/cardio/master/cardio_train.csv downloaded to /content/data/cardio_train.csv\n",
            "1 dataset files found in /content/data folder\n",
            "parsing /content/data/cardio_train.csv\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 70000 entries, 0 to 69999\n",
            "Data columns (total 12 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   age          70000 non-null  int64  \n",
            " 1   gender       70000 non-null  int64  \n",
            " 2   height       70000 non-null  int64  \n",
            " 3   weight       70000 non-null  float64\n",
            " 4   ap_hi        70000 non-null  int64  \n",
            " 5   ap_lo        70000 non-null  int64  \n",
            " 6   cholesterol  70000 non-null  int64  \n",
            " 7   gluc         70000 non-null  int64  \n",
            " 8   smoke        70000 non-null  int64  \n",
            " 9   alco         70000 non-null  int64  \n",
            " 10  active       70000 non-null  int64  \n",
            " 11  cardio       70000 non-null  int64  \n",
            "dtypes: float64(1), int64(11)\n",
            "memory usage: 6.4 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "i3QDgLIL8PDz",
        "outputId": "3d2c5850-2e67-4a06-bd57-2da624064064"
      },
      "source": [
        "cvd_df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>ap_hi</th>\n",
              "      <th>ap_lo</th>\n",
              "      <th>cholesterol</th>\n",
              "      <th>gluc</th>\n",
              "      <th>smoke</th>\n",
              "      <th>alco</th>\n",
              "      <th>active</th>\n",
              "      <th>cardio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18393</td>\n",
              "      <td>2</td>\n",
              "      <td>168</td>\n",
              "      <td>62.0</td>\n",
              "      <td>110</td>\n",
              "      <td>80</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20228</td>\n",
              "      <td>1</td>\n",
              "      <td>156</td>\n",
              "      <td>85.0</td>\n",
              "      <td>140</td>\n",
              "      <td>90</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18857</td>\n",
              "      <td>1</td>\n",
              "      <td>165</td>\n",
              "      <td>64.0</td>\n",
              "      <td>130</td>\n",
              "      <td>70</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17623</td>\n",
              "      <td>2</td>\n",
              "      <td>169</td>\n",
              "      <td>82.0</td>\n",
              "      <td>150</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17474</td>\n",
              "      <td>1</td>\n",
              "      <td>156</td>\n",
              "      <td>56.0</td>\n",
              "      <td>100</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  gender  height  weight  ap_hi  ...  gluc  smoke  alco  active  cardio\n",
              "0  18393       2     168    62.0    110  ...     1      0     0       1       0\n",
              "1  20228       1     156    85.0    140  ...     1      0     0       1       1\n",
              "2  18857       1     165    64.0    130  ...     1      0     0       0       1\n",
              "3  17623       2     169    82.0    150  ...     1      0     0       1       1\n",
              "4  17474       1     156    56.0    100  ...     1      0     0       0       0\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48Wnlnfw7jt9"
      },
      "source": [
        "###Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqCN20EB7kf6"
      },
      "source": [
        "Age is not something we usually measure in days. In fact, for health-related predictions like this one, we might even want to bucket them into age groups since people tend to age differently.\n",
        "\n",
        "For now, we will convert all ages into years:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Lr2lPfUM7VYL",
        "outputId": "88504319-0cc0-4126-dbf3-2c6e9085c710"
      },
      "source": [
        "cvd_df[\"age\"] = cvd_df[\"age\"] / 365.24\n",
        "cvd_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>ap_hi</th>\n",
              "      <th>ap_lo</th>\n",
              "      <th>cholesterol</th>\n",
              "      <th>gluc</th>\n",
              "      <th>smoke</th>\n",
              "      <th>alco</th>\n",
              "      <th>active</th>\n",
              "      <th>cardio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50.358668</td>\n",
              "      <td>2</td>\n",
              "      <td>168</td>\n",
              "      <td>62.0</td>\n",
              "      <td>110</td>\n",
              "      <td>80</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>55.382762</td>\n",
              "      <td>1</td>\n",
              "      <td>156</td>\n",
              "      <td>85.0</td>\n",
              "      <td>140</td>\n",
              "      <td>90</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>51.629066</td>\n",
              "      <td>1</td>\n",
              "      <td>165</td>\n",
              "      <td>64.0</td>\n",
              "      <td>130</td>\n",
              "      <td>70</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>48.250465</td>\n",
              "      <td>2</td>\n",
              "      <td>169</td>\n",
              "      <td>82.0</td>\n",
              "      <td>150</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>47.842515</td>\n",
              "      <td>1</td>\n",
              "      <td>156</td>\n",
              "      <td>56.0</td>\n",
              "      <td>100</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         age  gender  height  weight  ap_hi  ...  gluc  smoke  alco  active  cardio\n",
              "0  50.358668       2     168    62.0    110  ...     1      0     0       1       0\n",
              "1  55.382762       1     156    85.0    140  ...     1      0     0       1       1\n",
              "2  51.629066       1     165    64.0    130  ...     1      0     0       0       1\n",
              "3  48.250465       2     169    82.0    150  ...     1      0     0       1       1\n",
              "4  47.842515       1     156    56.0    100  ...     1      0     0       0       0\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av4lIxlx84RJ"
      },
      "source": [
        "The result is a more understandable column because we expect age values to be between 0 and 120. We took existing data and transformed it. This is an example of feature engineering, which is when you use domain knowledge of your data to create features that better represent your problem, thereby improving your models.\n",
        "\n",
        "There's value in performing feature engineering simply to make model outcomes more interpretable as long as this doesn't hurt model performance. As regards the age column, it can't hurt it because we haven't degraded the data. This is because you still have the decimal points for the years that represent the days.\n",
        "\n",
        "Now we are going to take a peak at what the summary statistics are for each one of our features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "-R_RO2By8o0E",
        "outputId": "6e4f5551-2a12-44e9-a104-faad32ccb084"
      },
      "source": [
        "cvd_df.describe().transpose()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>70000.0</td>\n",
              "      <td>53.304309</td>\n",
              "      <td>6.755152</td>\n",
              "      <td>29.564122</td>\n",
              "      <td>48.36272</td>\n",
              "      <td>53.945351</td>\n",
              "      <td>58.391742</td>\n",
              "      <td>64.924433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender</th>\n",
              "      <td>70000.0</td>\n",
              "      <td>1.349571</td>\n",
              "      <td>0.476838</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>height</th>\n",
              "      <td>70000.0</td>\n",
              "      <td>164.359229</td>\n",
              "      <td>8.210126</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>159.00000</td>\n",
              "      <td>165.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>250.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight</th>\n",
              "      <td>70000.0</td>\n",
              "      <td>74.205690</td>\n",
              "      <td>14.395757</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>65.00000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>82.000000</td>\n",
              "      <td>200.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ap_hi</th>\n",
              "      <td>70000.0</td>\n",
              "      <td>128.817286</td>\n",
              "      <td>154.011419</td>\n",
              "      <td>-150.000000</td>\n",
              "      <td>120.00000</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>16020.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ap_lo</th>\n",
              "      <td>70000.0</td>\n",
              "      <td>96.630414</td>\n",
              "      <td>188.472530</td>\n",
              "      <td>-70.000000</td>\n",
              "      <td>80.00000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>11000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cholesterol</th>\n",
              "      <td>70000.0</td>\n",
              "      <td>1.366871</td>\n",
              "      <td>0.680250</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gluc</th>\n",
              "      <td>70000.0</td>\n",
              "      <td>1.226457</td>\n",
              "      <td>0.572270</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smoke</th>\n",
              "      <td>70000.0</td>\n",
              "      <td>0.088129</td>\n",
              "      <td>0.283484</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>alco</th>\n",
              "      <td>70000.0</td>\n",
              "      <td>0.053771</td>\n",
              "      <td>0.225568</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>active</th>\n",
              "      <td>70000.0</td>\n",
              "      <td>0.803729</td>\n",
              "      <td>0.397179</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cardio</th>\n",
              "      <td>70000.0</td>\n",
              "      <td>0.499700</td>\n",
              "      <td>0.500003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               count        mean  ...         75%           max\n",
              "age          70000.0   53.304309  ...   58.391742     64.924433\n",
              "gender       70000.0    1.349571  ...    2.000000      2.000000\n",
              "height       70000.0  164.359229  ...  170.000000    250.000000\n",
              "weight       70000.0   74.205690  ...   82.000000    200.000000\n",
              "ap_hi        70000.0  128.817286  ...  140.000000  16020.000000\n",
              "ap_lo        70000.0   96.630414  ...   90.000000  11000.000000\n",
              "cholesterol  70000.0    1.366871  ...    2.000000      3.000000\n",
              "gluc         70000.0    1.226457  ...    1.000000      3.000000\n",
              "smoke        70000.0    0.088129  ...    0.000000      1.000000\n",
              "alco         70000.0    0.053771  ...    0.000000      1.000000\n",
              "active       70000.0    0.803729  ...    1.000000      1.000000\n",
              "cardio       70000.0    0.499700  ...    1.000000      1.000000\n",
              "\n",
              "[12 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMGdLhc6-9Rn"
      },
      "source": [
        "`age` is looking good because it ranges between 29 and 65 years, which is not out of the ordinary, but there are some anomalous outliers for `ap_hi` and `ap_lo`. Blood pressure can't be negative, and the highest ever recorded was 370. These records will have to be dropped because they could lead to poor model performance and interpretability.\n",
        "\n",
        "For good measure, we ought to make sure that `ap_hi` is always higher than `ap_lo`, so any record with that discrepancy should also be dropped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGwiv10T9aVc"
      },
      "source": [
        "cvd_df = cvd_df[(cvd_df[\"ap_lo\"] <= 370) & (cvd_df[\"ap_lo\"] > 0)].reset_index(drop=True)\n",
        "cvd_df = cvd_df[(cvd_df[\"ap_hi\"] <= 370) & (cvd_df[\"ap_hi\"] > 0)].reset_index(drop=True)\n",
        "cvd_df = cvd_df[cvd_df[\"ap_hi\"] >= cvd_df[\"ap_lo\"]].reset_index(drop=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwL1aWX3AVLH"
      },
      "source": [
        "Now, in order to fit a logistic regression model, we must put all objective, examination, and subjective features together as $𝑋$ and the target feature alone as $𝑦$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHV7lQsGANF3",
        "outputId": "177ed291-298c-4f1a-b252-f09e8475ff59"
      },
      "source": [
        "y = cvd_df[\"cardio\"]\n",
        "X = cvd_df.drop([\"cardio\"], axis=1).copy()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=2020)\n",
        "\n",
        "print(x_train.shape, x_test.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(58404, 11) (10307, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y-YoIaBK7jL"
      },
      "source": [
        "##Learning about Interpretation Method Types and Scopes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFyRfn92K8Zd"
      },
      "source": [
        "Now that we have prepared our data and split it into training/test datasets, we can fit the model using the training data and print a summary of the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--lhsjYvK2kO",
        "outputId": "f21a317f-c732-41c5-b949-ed9ddc481e83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "log_model = sm.Logit(y_train, sm.add_constant(x_train))\n",
        "log_result = log_model.fit()\n",
        "\n",
        "print(log_result.summary2())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.560389\n",
            "         Iterations 6\n",
            "                         Results: Logit\n",
            "=================================================================\n",
            "Model:              Logit            Pseudo R-squared: 0.191     \n",
            "Dependent Variable: cardio           AIC:              65481.9016\n",
            "Date:               2021-07-03 09:38 BIC:              65589.6032\n",
            "No. Observations:   58404            Log-Likelihood:   -32729.   \n",
            "Df Model:           11               LL-Null:          -40480.   \n",
            "Df Residuals:       58392            LLR p-value:      0.0000    \n",
            "Converged:          1.0000           Scale:            1.0000    \n",
            "No. Iterations:     6.0000                                       \n",
            "-----------------------------------------------------------------\n",
            "               Coef.   Std.Err.    z     P>|z|   [0.025   0.975] \n",
            "-----------------------------------------------------------------\n",
            "const         -11.2653   0.2508 -44.9155 0.0000 -11.7569 -10.7737\n",
            "age             0.0510   0.0015  34.7417 0.0000   0.0481   0.0539\n",
            "gender         -0.0000   0.0238  -0.0021 0.9983  -0.0466   0.0465\n",
            "height         -0.0033   0.0014  -2.3973 0.0165  -0.0061  -0.0006\n",
            "weight          0.0108   0.0007  14.5383 0.0000   0.0094   0.0123\n",
            "ap_hi           0.0564   0.0010  56.6108 0.0000   0.0544   0.0583\n",
            "ap_lo           0.0105   0.0015   6.7873 0.0000   0.0075   0.0135\n",
            "cholesterol     0.5022   0.0169  29.6309 0.0000   0.4689   0.5354\n",
            "gluc           -0.1209   0.0193  -6.2687 0.0000  -0.1587  -0.0831\n",
            "smoke          -0.1246   0.0377  -3.3060 0.0009  -0.1985  -0.0507\n",
            "alco           -0.2587   0.0461  -5.6157 0.0000  -0.3490  -0.1684\n",
            "active         -0.2259   0.0238  -9.4937 0.0000  -0.2726  -0.1793\n",
            "=================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7M5RGufvXwi"
      },
      "source": [
        "The preceding summary helps us to understand which 𝑋𝑋 features contributed the most to the 𝑦𝑦 CVD diagnosis using the model coefficients (labeled Coef. in the table). Much like with linear regression, they are like a weight applied to every predictor. However, the linear combination exponent is a logistic function. This makes the interpretation more difficult.\n",
        "\n",
        "You can only tell by looking at it that the features with the absolute highest values are `cholesterol` and `active`, but it's not very intuitive in terms of what this means.\n",
        "\n",
        "A more interpretable way of looking at these values is revealed once you calculate the exponential of these coefficients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQgbx0zvvCBU",
        "outputId": "365f8f95-2381-48dd-9329-05eab61f2ddf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.exp(log_result.params).sort_values(ascending=False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "cholesterol    1.652284\n",
              "ap_hi          1.057969\n",
              "age            1.052325\n",
              "weight         1.010902\n",
              "ap_lo          1.010566\n",
              "gender         0.999950\n",
              "height         0.996671\n",
              "gluc           0.886156\n",
              "smoke          0.882837\n",
              "active         0.797768\n",
              "alco           0.772062\n",
              "const          0.000013\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUK7DLXsv9Uj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}