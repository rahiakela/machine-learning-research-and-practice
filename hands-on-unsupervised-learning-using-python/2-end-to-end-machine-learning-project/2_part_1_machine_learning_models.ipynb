{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-part-1-machine-learning-models.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPk/zSqPszq9GgFEwBkDF98",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/hands-on-unsupervised-learning-using-python/blob/main/2-end-to-end-machine-learning-project/2_part_1_machine_learning_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxJpwC7RGYv2"
      },
      "source": [
        "## Part-1:Machine Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWQ5miqyHDuW"
      },
      "source": [
        "Before we begin exploring unsupervised learning algorithms in detail, we will\n",
        "review how to set up and manage machine learning projects, covering everything\n",
        "from acquiring data to building and evaluating a model and implementing a\n",
        "solution. We will work with supervised learning models—an area most readers should have some experience in—before jumping into unsupervised learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJhlRUr5HTwl"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78lieHGcHflH"
      },
      "source": [
        "'''Main'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "'''Data Viz'''\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "color = sns.color_palette()\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "'''Data Prep'''\n",
        "from sklearn import preprocessing as pp \n",
        "from scipy.stats import pearsonr \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold \n",
        "from sklearn.metrics import log_loss \n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score \n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report \n",
        "\n",
        "'''Algos'''\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# import xgboost as xgb\n",
        "import lightgbm as lgb"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u-WMIu1MYZd"
      },
      "source": [
        "Let's download creditcardfraud dataset from [Kaggle](https://www.kaggle.com/isaikumar/creditcardfraud)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4NrHCs1Mc8O"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload() # upload kaggle.json file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aB2cxu8AIKMr",
        "outputId": "3a079150-ec20-4da4-b358-48a08ceb6433"
      },
      "source": [
        "%%shell\n",
        "\n",
        "mkdir -p ~/.kaggle\n",
        "mv kaggle.json ~/.kaggle/\n",
        "ls ~/.kaggle\n",
        "chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# download word embeddings from kaggle\n",
        "kaggle datasets download -d isaikumar/creditcardfraud\n",
        "unzip -qq creditcardfraud.zip\n",
        "rm -rf creditcardfraud.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n",
            "Downloading creditcardfraud.zip to /content\n",
            " 93% 61.0M/65.9M [00:00<00:00, 59.5MB/s]\n",
            "100% 65.9M/65.9M [00:00<00:00, 122MB/s] \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIoyOSZBIVL9"
      },
      "source": [
        "## Loading dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnNM2eFGJADP"
      },
      "source": [
        "We will use a real dataset of anonymized credit card transactions\n",
        "made by [European cardholders from September 2013](https://www.kaggle.com/isaikumar/creditcardfraud). These transactions are labeled as fraudulent or genuine, and we will build a fraud detection solution using machine learning to predict the correct labels for never-before-seen instances.\n",
        "\n",
        "This dataset is highly imbalanced. Of the 284,807 transactions, only 492 are\n",
        "fraudulent (0.172%). This low percentage of fraud is pretty typical for credit\n",
        "card transactions.\n",
        "\n",
        "There are 28 features, all of which are numerical, and there are no categorical\n",
        "variables. These features are not the original features but rather the output of\n",
        "principal component analysis. The original features were distilled to 28 principal components using this form of dimensionality reduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96m6DJh0Kv2S"
      },
      "source": [
        "## Data Acquisition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfWeWTBDK00h"
      },
      "source": [
        "The first step in any machine learning project is data acquisition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "1bS2bQUkINb-",
        "outputId": "1c6284ea-f220-4ab0-89af-a2f63918029f"
      },
      "source": [
        "# Read the data\n",
        "data = pd.read_csv(\"creditcard.csv\")\n",
        "data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
              "0   0.0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62      0\n",
              "1   0.0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69      0\n",
              "2   1.0 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66      0\n",
              "3   1.0 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50      0\n",
              "4   2.0 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99      0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsAeO_MXRgVQ"
      },
      "source": [
        "## Generate Feature Matrix and Labels Array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOCeddV2RhM5"
      },
      "source": [
        "Let’s create and standardize the feature matrix X and isolate the labels array y\n",
        "(one for fraud, zero for not fraud). \n",
        "\n",
        "Later on we will feed these into the machine learning algorithms during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bj80D8abRI0u"
      },
      "source": [
        "# Create the feature matrix X and the labels array Y\n",
        "data_x = data.copy().drop([\"Class\"], axis=1)\n",
        "data_y = data[\"Class\"].copy()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrWpsW05ILj-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "55bfeeb8-4251-4eb5-d094-65d5349eed18"
      },
      "source": [
        "data_x.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Time        V1        V2        V3  ...       V26       V27       V28  Amount\n",
              "0   0.0 -1.359807 -0.072781  2.536347  ... -0.189115  0.133558 -0.021053  149.62\n",
              "1   0.0  1.191857  0.266151  0.166480  ...  0.125895 -0.008983  0.014724    2.69\n",
              "2   1.0 -1.358354 -1.340163  1.773209  ... -0.139097 -0.055353 -0.059752  378.66\n",
              "3   1.0 -0.966272 -0.185226  1.792993  ... -0.221929  0.062723  0.061458  123.50\n",
              "4   2.0 -1.158233  0.877737  1.548718  ...  0.502292  0.219422  0.215153   69.99\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzZ0m1dSH8w7"
      },
      "source": [
        "Let’s rescale the feature matrix so that each feature, except for time, has a mean of zero and standard deviation of one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ASo4XYFH6Jh"
      },
      "source": [
        "# Standardize the feature matrix X\n",
        "features_to_scale = data_x.drop([\"Time\"], axis=1).columns\n",
        "sx = pp.StandardScaler(copy=True)\n",
        "data_x.loc[:, features_to_scale] = sx.fit_transform(data_x[features_to_scale])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFtEPntHPn9e"
      },
      "source": [
        "##Data Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb6hMCXHPo-B"
      },
      "source": [
        "As a final step, let’s visualize the data to appreciate just how imbalanced the\n",
        "dataset is.Since there are so few cases of fraud to learn from, this is\n",
        "a difficult problem to solve; fortunately, we have labels for the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8-szK_YMqBt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "0a677113-fce0-4d16-bca3-317f184fb506"
      },
      "source": [
        "count_classes = pd.value_counts(data['Class'],sort=True).sort_index()\n",
        "ax = sns.barplot(x=count_classes.index, y=[tuple(count_classes/len(data))[0],tuple(count_classes/len(data))[1]])\n",
        "ax.set_title('Frequency Percentage by Class')\n",
        "ax.set_xlabel('Class')\n",
        "ax.set_ylabel('Frequency Percentage')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Frequency Percentage')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfouAwpuYNsJ"
      },
      "source": [
        "##Model Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAK8G1wYYOaT"
      },
      "source": [
        "Now that the data is ready, let’s prepare for the model. We need to split the data\n",
        "into a training and a test set, select a cost function, and prepare for k-fold crossvalidation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtIJnxYRYYdg"
      },
      "source": [
        "###Split into Training and Test Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AURug2DPYafu"
      },
      "source": [
        "We need to set up our machine learning project so that we have a training set\n",
        "from which the machine learning algorithm learns. We also need a test set (the\n",
        "never-before-seen cases) the machine learning algorithm can make predictions\n",
        "on. The performance on this test set will be the ultimate gauge of success.\n",
        "\n",
        "Let’s go ahead and split our credit card transactions dataset into a training set and a test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay61SWLyYlmS",
        "outputId": "2fd3ecdb-a4b5-4d5b-bca3-8e6876e44822",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.33, random_state=2020, stratify=data_y)\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(190820, 30) (190820,)\n",
            "(93987, 30) (93987,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OACQEWkPZKI_"
      },
      "source": [
        "To preserve the percentage of fraud (~0.17%) for both the training and the test set, we have set the stratify parameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX2gvApebZma"
      },
      "source": [
        "###Feature scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i66tHHLgbaqe",
        "outputId": "c3f73ae9-1cdd-4093-cb52-142f3c820d62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Feature scaling\n",
        "features_to_scale = data_x.drop(['Time'],axis=1).columns\n",
        "features_to_scale"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n",
              "       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n",
              "       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwAlrzRcbkiJ",
        "outputId": "594da556-acc4-4098-b67d-5f0fd4463728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "# Suppress warnings\n",
        "pd.set_option('mode.chained_assignment', None)\n",
        "\n",
        "sx = pp.StandardScaler(copy=True)\n",
        "x_train.loc[:, features_to_scale] = sx.fit_transform(x_train.loc[:, features_to_scale])\n",
        "scaling_factors = pd.DataFrame(data=[sx.mean_, sx.scale_],index=['Mean','StDev'], columns=features_to_scale)\n",
        "\n",
        "x_test.loc[:, features_to_scale] = sx.transform(x_test.loc[:, features_to_scale])\n",
        "\n",
        "scaling_factors"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Mean</th>\n",
              "      <td>0.000221</td>\n",
              "      <td>-0.001008</td>\n",
              "      <td>-0.002144</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>-0.000134</td>\n",
              "      <td>-0.000977</td>\n",
              "      <td>0.001089</td>\n",
              "      <td>-0.001113</td>\n",
              "      <td>-0.000303</td>\n",
              "      <td>-0.000981</td>\n",
              "      <td>-0.000372</td>\n",
              "      <td>0.000391</td>\n",
              "      <td>-0.001872</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.000298</td>\n",
              "      <td>0.000129</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.002538</td>\n",
              "      <td>-0.003456</td>\n",
              "      <td>-0.001382</td>\n",
              "      <td>-0.000735</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>0.000988</td>\n",
              "      <td>-0.001237</td>\n",
              "      <td>0.003105</td>\n",
              "      <td>-0.002765</td>\n",
              "      <td>0.001606</td>\n",
              "      <td>0.000832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>StDev</th>\n",
              "      <td>1.003017</td>\n",
              "      <td>1.005316</td>\n",
              "      <td>1.002874</td>\n",
              "      <td>1.001470</td>\n",
              "      <td>1.012178</td>\n",
              "      <td>1.005301</td>\n",
              "      <td>1.015145</td>\n",
              "      <td>1.001745</td>\n",
              "      <td>0.998685</td>\n",
              "      <td>0.994373</td>\n",
              "      <td>0.999072</td>\n",
              "      <td>0.995072</td>\n",
              "      <td>0.998671</td>\n",
              "      <td>0.996406</td>\n",
              "      <td>0.998960</td>\n",
              "      <td>0.996890</td>\n",
              "      <td>0.995264</td>\n",
              "      <td>1.001107</td>\n",
              "      <td>0.999722</td>\n",
              "      <td>1.012138</td>\n",
              "      <td>0.998972</td>\n",
              "      <td>0.999236</td>\n",
              "      <td>1.010673</td>\n",
              "      <td>0.998968</td>\n",
              "      <td>1.001821</td>\n",
              "      <td>1.001085</td>\n",
              "      <td>1.004932</td>\n",
              "      <td>1.030749</td>\n",
              "      <td>1.012571</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             V1        V2        V3  ...       V27       V28    Amount\n",
              "Mean   0.000221 -0.001008 -0.002144  ... -0.002765  0.001606  0.000832\n",
              "StDev  1.003017  1.005316  1.002874  ...  1.004932  1.030749  1.012571\n",
              "\n",
              "[2 rows x 29 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcTYdxrvZOYj"
      },
      "source": [
        "###Create k-Fold Cross-Validation Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqzWGeykZPCY"
      },
      "source": [
        "To help the machine learning algorithm estimate what its performance will be on\n",
        "the never-before-seen examples (the test set), it is best practice to further split the training set into a training set and a validation set.\n",
        "\n",
        "It is possible to train and evaluate like this five times—leaving aside a different\n",
        "fifth slice as the validation set each time. This is known as k-fold crossvalidation,\n",
        "where k in this case is five. With this approach, we will have not one\n",
        "estimate but five estimates for the generalization error.\n",
        "\n",
        "We will store the training score and the cross-validation score for each of the\n",
        "five runs, and we will store the cross-validation predictions each time. After all\n",
        "five runs are complete, we will have cross-validation predictions for the entire\n",
        "dataset. This will be the best all-in estimate of the performance the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5O8A_UxZB9C"
      },
      "source": [
        "k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8zbUGl3ce4q"
      },
      "source": [
        "##Model #1: Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVoo0Czxcyr_"
      },
      "source": [
        "Now we’re ready to build the machine learning models. For each machine\n",
        "algorithm we consider, we will set hyperparameters, train the model, and\n",
        "evaluate the results.\n",
        "\n",
        "Let’s start with the most basic classification algorithm, logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC-ZOutKc09h"
      },
      "source": [
        "penalty = 'l2'              # set the penalty to the default value L2 instead of L1\n",
        "C = 1.0                     # C is the regularization strength\n",
        "class_weight = 'balanced'   # This signals to the logistic regression algorithm that we have an imbalanced class problem\n",
        "random_state = 2018\n",
        "solver = 'liblinear'\n",
        "n_jobs = 1"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXffDh0VeOuh"
      },
      "source": [
        "###Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx1gMACyePpL"
      },
      "source": [
        "We will train the logistic regression model on each of the five k-fold cross-validation splits, training on four-fifths of the training set and evaulating the performance on the fifth slice that is held aside."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Sea7xv_c5P2"
      },
      "source": [
        "log_reg = LogisticRegression(penalty=penalty, C=C, class_weight=class_weight, random_state=random_state, solver=solver, n_jobs=n_jobs)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqXUHX0ydRCv",
        "outputId": "a5f9b807-ccd6-4389-9b00-b95141f343f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "training_scores = []\n",
        "cv_scores = []\n",
        "predictions_based_on_kfolds = pd.DataFrame(data=[], index=y_train.index, columns=[0, 1])\n",
        "\n",
        "for train_index, cv_index in k_fold.split(np.zeros(len(x_train)), y_train.ravel()):\n",
        "  x_train_fold, x_cv_fold = x_train.iloc[train_index, :], x_train.iloc[cv_index, :]\n",
        "  y_train_fold, y_cv_fold = y_train.iloc[train_index], y_train.iloc[cv_index]\n",
        "\n",
        "  log_reg.fit(x_train_fold, y_train_fold)\n",
        "\n",
        "  logloss_training = log_loss(y_train_fold, log_reg.predict_proba(x_train_fold)[:, 1])\n",
        "  training_scores.append(logloss_training) \n",
        "\n",
        "  predictions_based_on_kfolds.loc[x_cv_fold.index, :] = log_reg.predict_proba(x_cv_fold)\n",
        "  logloss_cv = log_loss(y_cv_fold, predictions_based_on_kfolds.loc[x_cv_fold.index, 1])\n",
        "  cv_scores.append(logloss_cv)\n",
        "\n",
        "  print(\"Training Log Loss:\", logloss_training)\n",
        "  print(\"CV Log Loss:\", logloss_cv)\n",
        "\n",
        "logloss_logistic_regression = log_loss(y_train, predictions_based_on_kfolds.loc[:, 1])\n",
        "print(\"Logistic Regression Log Loss: \", logloss_logistic_regression) "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Log Loss: 0.1115914084503823\n",
            "CV Log Loss: 0.11072278047852098\n",
            "Training Log Loss: 0.11582744695384373\n",
            "CV Log Loss: 0.1176725688814084\n",
            "Training Log Loss: 0.11568628386826778\n",
            "CV Log Loss: 0.11335074816942886\n",
            "Training Log Loss: 0.11962887546060656\n",
            "CV Log Loss: 0.12090553843535343\n",
            "Training Log Loss: 0.12076096303135772\n",
            "CV Log Loss: 0.12339754424049496\n",
            "Logistic Regression Log Loss:  0.11720983604104132\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn-vhL4lmIno"
      },
      "source": [
        "For each of the five runs, their training and cross-validation log losses are\n",
        "similar. The logistic regression model does not exhibit severe overfitting; if it did, we would have a low training log loss and comparably high cross-validation log loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWAX_BfomUTH"
      },
      "source": [
        "###Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_W0c1awlmTH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}