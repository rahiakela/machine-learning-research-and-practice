{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-generating-shakespearean-text-using-character-rnn.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMgXWS8o7lHkQlPwfWAGFEX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow/blob/16-natural-nanguage-processing-with-RNNs-and-Attention/1_generating_shakespearean_text_using_character_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHUB8psfrgPJ",
        "colab_type": "text"
      },
      "source": [
        "# Generating Shakespearean Text Using a Character RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3nxCza6rwcN",
        "colab_type": "text"
      },
      "source": [
        "A common approach for natural language tasks is to use recurrent neural networks.\n",
        "We will therefore continue to explore RNNs, starting with\n",
        "a character RNN, trained to predict the next character in a sentence. This will allow us to generate some original text, and in the process we will see how to build a TensorFlow Dataset on a very long sequence. \n",
        "\n",
        "We will first use a stateless RNN (which learns on random portions of text at each iteration, without any information on the rest of the text), then we will build a stateful RNN (which preserves the hidden state between training iterations and continues reading where it left off, allowing it to learn longer patterns).\n",
        "\n",
        "Let’s start with a simple and fun model that can write like Shakespeare (well, sort of)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLNaLqOquKI4",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wC8SV2kuM11",
        "colab_type": "code",
        "outputId": "e028234a-1768-4114-aba1-831160c28f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import sys\n",
        "assert sys.version_info >= (3, 5)  # Python ≥3.5 is required\n",
        "\n",
        "import sklearn \n",
        "assert sklearn.__version__ >= \"0.20\"  # Scikit-Learn ≥0.20 is required\n",
        "\n",
        "# %tensorflow_version only exists in Colab.\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "  IS_COLAB = True\n",
        "except Exception:\n",
        "  IS_COLAB = False\n",
        "  pass\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= '2.0'\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3Rly-8iuRJL",
        "colab_type": "text"
      },
      "source": [
        "## Generating Shakespearean Text Using a Character RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztlqRYlpuSAz",
        "colab_type": "text"
      },
      "source": [
        "In a famous 2015 [blog post](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) titled “The Unreasonable Effectiveness of Recurrent Neural Networks,” Andrej Karpathy showed how to train an RNN to predict the next character in a sentence. This Char-RNN can then be used to generate novel text, one character at a time. Here is a small sample of the text generated by a Char-RNN model after it was trained on all of Shakespeare’s work:\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "PANDARUS:\n",
        "\n",
        "Alas, I think he shall be come approached and the day\n",
        "\n",
        "When little srain would be attain’d into being never fed,\n",
        "\n",
        "And who is but a chain and subjects of his death,\n",
        "\n",
        "I should not sleep.\n",
        "\n",
        "---\n",
        "\n",
        "Not exactly a masterpiece, but it is still impressive that the model was able to learn words, grammar, proper punctuation, and more, just by learning to predict the next character in a sentence. \n",
        "\n",
        "Let’s look at how to build a Char-RNN, step by step, starting\n",
        "with the creation of the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuOZsaWcNHXK",
        "colab_type": "text"
      },
      "source": [
        "## Splitting a sequence into batches of shuffled windows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHZEUnE5NM8a",
        "colab_type": "text"
      },
      "source": [
        "For example, let's split the sequence 0 to 14 into windows of length 5, each shifted by 2 (e.g.,`[0, 1, 2, 3, 4]`, `[2, 3, 4, 5, 6]`, etc.), then shuffle them, and split them into inputs (the first 4 steps) and targets (the last 4 steps) (e.g., `[2, 3, 4, 5, 6]` would be split into `[[2, 3, 4, 5], [3, 4, 5, 6]]`), then create batches of 3 such input/target pairs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBMcHqcyNANH",
        "colab_type": "code",
        "outputId": "f5f0db9a-7285-4cb0-e4a9-ddd11035cedf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "n_steps = 5\n",
        "dataset = tf.data.Dataset.from_tensor_slices(tf.range(15))\n",
        "dataset = dataset.window(n_steps, shift=2, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(n_steps))\n",
        "dataset = dataset.shuffle(10).map(lambda window: (window[:-1], window[1:]))\n",
        "dataset = dataset.batch(3).prefetch(1)\n",
        "for index, (X_batch, Y_batch) in enumerate(dataset):\n",
        "    print(\"_\" * 20, \"Batch\", index, \"\\nX_batch\")\n",
        "    print(X_batch.numpy())\n",
        "    print(\"=\" * 5, \"\\nY_batch\")\n",
        "    print(Y_batch.numpy())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "____________________ Batch 0 \n",
            "X_batch\n",
            "[[6 7 8 9]\n",
            " [2 3 4 5]\n",
            " [4 5 6 7]]\n",
            "===== \n",
            "Y_batch\n",
            "[[ 7  8  9 10]\n",
            " [ 3  4  5  6]\n",
            " [ 5  6  7  8]]\n",
            "____________________ Batch 1 \n",
            "X_batch\n",
            "[[ 0  1  2  3]\n",
            " [ 8  9 10 11]\n",
            " [10 11 12 13]]\n",
            "===== \n",
            "Y_batch\n",
            "[[ 1  2  3  4]\n",
            " [ 9 10 11 12]\n",
            " [11 12 13 14]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odsh5-zAznJC",
        "colab_type": "text"
      },
      "source": [
        "## Creating the Training Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYDdX8lRzsHo",
        "colab_type": "text"
      },
      "source": [
        "First, let’s download all of Shakespeare’s work, using Keras’s handy get_file() function and downloading the data from Andrej Karpathy’s Char-RNN project:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wawoSl90SJO",
        "colab_type": "code",
        "outputId": "ace8c6d3-fbe2-431f-e9c2-1acc795105a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "shakespeare_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "filepath = keras.utils.get_file('shakespeare.txt', shakespeare_url)\n",
        "\n",
        "with open(filepath) as f:\n",
        "  shakespeare_text = f.read()\n",
        "\n",
        "print(shakespeare_text[:148])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVziKeqC0zLZ",
        "colab_type": "code",
        "outputId": "6ba6d2e8-e470-44c5-fc1f-1d387c599bc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\"\".join(sorted(set(shakespeare_text.lower())))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5RuSBA81T6s",
        "colab_type": "text"
      },
      "source": [
        "We must encode every character as an integer. One option is to create a custom preprocessing layer. \n",
        "\n",
        "But in this case, it will be simpler to use Keras’s Tokenizer class. First we need to fit a tokenizer to the text: it will find all the characters used in the text and map each of them to a different character ID, from 1 to the number of distinct characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICTlgY4v1MJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(shakespeare_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MACJFPKb163k",
        "colab_type": "text"
      },
      "source": [
        "We set char_level=True to get character-level encoding rather than the default word-level encoding. Note that this tokenizer converts the text to lowercase by default (but you can set lower=False if you do not want that). \n",
        "\n",
        "Now the tokenizer can encode a sentence (or a list of sentences) to a list of character IDs and back, and it tells us how many distinct characters there are and the total number of characters in the text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhrGxGIh1s8M",
        "colab_type": "code",
        "outputId": "3ed1dee5-19dd-46ca-a609-cfca37bdaa4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer.texts_to_sequences(['First'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[20, 6, 9, 8, 3]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koCa8Zoy2HXJ",
        "colab_type": "code",
        "outputId": "092ffd96-4158-4908-9381-ad20f11cbd46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer.sequences_to_texts([[20, 6, 8, 3]])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['f i s t']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EP6tQp0q2PNP",
        "colab_type": "code",
        "outputId": "dc9f04b0-fadf-4182-ff3a-075afbbb7ddc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "max_id = len(tokenizer.word_index) # number of distinct characters\n",
        "dataset_size = tokenizer.document_count  # total number of characters\n",
        "print(max_id)\n",
        "print(dataset_size)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39\n",
            "1115394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OicBIYd520AU",
        "colab_type": "text"
      },
      "source": [
        "Let’s encode the full text so each character is represented by its ID (we subtract 1 to get IDs from 0 to 38, rather than from 1 to 39):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYPjSFAb2bTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uboycVEm3N4f",
        "colab_type": "text"
      },
      "source": [
        "### How to Split a Sequential Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUyc0rds3RPt",
        "colab_type": "text"
      },
      "source": [
        "It is very important to avoid any overlap between the training set, the validation set, and the test set.\n",
        "\n",
        "When dealing with time series, you would in general split across time,: for example, you might take the years 2000 to 2012 for the training set, the years 2013 to 2015 for the validation set, and the years 2016 to 2018 for the test set. However, in some cases you may be able to split along other dimensions, which will give you a longer time period to train on.\n",
        "\n",
        "So, it is often safer to split across time—but this implicitly assumes that the patterns the RNN can learn in the past (in the training set) will still exist in the future. In other words, we assume that the time series is *stationary* (at least in a wide sense).\n",
        "\n",
        "In short, splitting a time series into a training set, a validation set, and a test set is not a trivial task, and how it’s done will depend strongly on the task at hand.\n",
        "\n",
        "Let’s take the first 90% of the text for the training set\n",
        "(keeping the rest for the validation set and the test set), and create a tf.data.Dataset that will return each character one by one from this set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9nipJwU3M8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_size = dataset_size * 90 // 100\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6HmYaO05AZE",
        "colab_type": "text"
      },
      "source": [
        "### Chopping the Sequential Dataset into Multiple Windows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-rp-6uE5Ban",
        "colab_type": "text"
      },
      "source": [
        "The training set now consists of a single sequence of over a million characters, so we can’t just train the neural network directly on it: the RNN would be equivalent to a deep net with over a million layers, and we would have a single (very long) instance to train it. Instead, we will use the dataset’s window() method to convert this long sequence of characters into many smaller windows of text. \n",
        "\n",
        "Every instance in the dataset will be a fairly short substring of the whole text, and the RNN will be unrolled only over the length of these substrings. This is called **truncated backpropagation through time**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ6j6a6X4v15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_steps = 100\n",
        "window_length = n_steps + 1  # target = input shifted 1 character ahead\n",
        "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61JYVWxK6TTv",
        "colab_type": "text"
      },
      "source": [
        "By default, the window() method creates nonoverlapping windows, but to get the largest possible training set we use shift=1 so that the first window contains characters 0 to 100, the second contains characters 1 to 101, and so on. To ensure that all windows are exactly 101 characters long, we set drop_remainder=True.\n",
        "\n",
        "The window() method creates a dataset that contains windows, each of which is also represented as a dataset. It’s a nested dataset, analogous to a list of lists. This is useful when you want to transform each window by calling its dataset methods (e.g., to shuffle them or batch them).\n",
        "\n",
        "However, we cannot use a nested dataset directly for training, as our model will expect tensors as input, not datasets. So, we must call the flat_map() method: it converts a nested dataset into a flat dataset (one that does not contain datasets)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqRLr87U6CnU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaSg8Lyd7OI2",
        "colab_type": "text"
      },
      "source": [
        "Notice that we call batch(window_length) on each window: since all windows have exactly that length, we will get a single tensor for each of them. \n",
        "\n",
        "Now the dataset contains consecutive windows of 101 characters each. Since Gradient Descent works best when the instances in the training set are independent and identically distributed.\n",
        "\n",
        "we need to shuffle these windows. Then we can batch the windows and separate the inputs (the first 100 characters) from the target (the last character):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhRdyJc77ndv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "batch_size = 32\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8Bf-zAk7-_2",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/shuffled-windows.png?raw=1' width='800'/>\n",
        "\n",
        "It summarizes the dataset preparation steps discussed so far (showing windows of length 11 rather than 101, and a batch size of 3 instead of 32).\n",
        "\n",
        "Categorical input features should generally be encoded,\n",
        "usually as one-hot vectors or as embeddings. Here, we will encode each character using a one-hot vector because there are fairly few distinct characters(only 39)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHgswkUB9CYg",
        "colab_type": "code",
        "outputId": "0f996fe8-d068-42b6-fa80-4c40592ee2d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset.take(1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((None, None, 39), (None, None)), types: (tf.float32, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i401O6iS9_WJ",
        "colab_type": "text"
      },
      "source": [
        "Finally, we just need to add prefetching."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzU8ccaE9-WJ",
        "colab_type": "code",
        "outputId": "c23b388d-3771-41c4-9289-07b8c657c999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "dataset = dataset.prefetch(1)\n",
        "dataset.take(1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((None, None, 39), (None, None)), types: (tf.float32, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwMFHFDpCLrS",
        "colab_type": "code",
        "outputId": "1ca69863-f9ed-478b-e5ea-dc83c1b2814f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "for X_batch, Y_batch in dataset.take(1):\n",
        "  print(X_batch.shape, Y_batch.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 100, 39) (32, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptwCwS78-Iga",
        "colab_type": "text"
      },
      "source": [
        "That’s it! Preparing the dataset was the hardest part. Now let’s create the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlSKhb6r-JOI",
        "colab_type": "text"
      },
      "source": [
        "## Building and Training the Char-RNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3VfK0wp-LuD",
        "colab_type": "text"
      },
      "source": [
        "To predict the next character based on the previous 100 characters, we can use an RNN with 2 GRU layers of 128 units each and 20% dropout on both the inputs (dropout) and the hidden states (recurrent_dropout). We can tweak these hyperparameters\n",
        "later, if needed. The output layer is a time-distributed Dense layer.\n",
        "\n",
        "This time this layer must have 39 units (max_id) because there are 39 distinct characters in the text, and we want to output a probability for each possible character (at each time step). The output probabilities should sum up to 1 at each time step, so we apply the softmax activation function to the outputs of the Dense layer. \n",
        "\n",
        "We can then compile this model, using the \"sparse_categorical_crossentropy\" loss and an Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5_bM7ns-Ef1",
        "colab_type": "code",
        "outputId": "bd1b9f47-bb8b-4f15-a1b5-8f93c68acdfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))                            \n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru (GRU)                    (None, None, 128)         64896     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, None, 128)         99072     \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, None, 39)          5031      \n",
            "=================================================================\n",
            "Total params: 168,999\n",
            "Trainable params: 168,999\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NofAusbtDkBI",
        "colab_type": "code",
        "outputId": "8c62d782-50d8-4ba2-fe36-ecd16008d9b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "history = model.fit(dataset, steps_per_epoch=train_size // batch_size, epochs=5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 31370 steps\n",
            "Epoch 1/5\n",
            "31370/31370 [==============================] - 478s 15ms/step - loss: 0.9566\n",
            "Epoch 2/5\n",
            "31370/31370 [==============================] - 467s 15ms/step - loss: 0.9603\n",
            "Epoch 3/5\n",
            "31370/31370 [==============================] - 466s 15ms/step - loss: 1.0236\n",
            "Epoch 4/5\n",
            "31370/31370 [==============================] - 462s 15ms/step - loss: 1.0744\n",
            "Epoch 5/5\n",
            "31370/31370 [==============================] - 484s 15ms/step - loss: 1.1023\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAzn3WV2Or79",
        "colab_type": "text"
      },
      "source": [
        "## Using the Char-RNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQYwTAGmOtL1",
        "colab_type": "text"
      },
      "source": [
        "Now we have a model that can predict the next character in text written by Shakespeare. To feed it some text, we first need to preprocess it like we did earlier, so let’s create a little function for this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHxJq8dkD0JX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(texts):\n",
        "  X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
        "  return tf.one_hot(X, max_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isDhjrcg8cFY",
        "colab_type": "text"
      },
      "source": [
        "Now let’s use the model to predict the next letter in some text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0i58Gcz8c2m",
        "colab_type": "code",
        "outputId": "d9b48c3e-ce88-4866-970f-8eb8e8b3fb2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_new = preprocess(['How are yo'])\n",
        "Y_pred = model.predict_classes(X_new)\n",
        "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]  # 1st sentence, last char"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'u'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfH1n6iM9Iqr",
        "colab_type": "code",
        "outputId": "0b6c564f-189b-4534-a7fa-f01bb36f0450",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer.sequences_to_texts(Y_pred + 1)[0]  # # 1st sentence, all chars"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'e u   s   e   t o u'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug86x-C889QC",
        "colab_type": "text"
      },
      "source": [
        "Success! The model guessed right. Now let’s use this model to generate new text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gnXqqo68-GA",
        "colab_type": "text"
      },
      "source": [
        "## Generating Fake Shakespearean Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhqjOaXI9gRy",
        "colab_type": "text"
      },
      "source": [
        "To generate new text using the Char-RNN model, we could feed it some text, make the model predict the most likely next letter, add it at the end of the text, then give the extended text to the model to guess the next letter, and so on. But in practice this often leads to the same words being repeated over and over again. \n",
        "\n",
        "Instead, we can pick the next character randomly, with a probability equal to the estimated probability, using TensorFlow’s tf.random.categorical() function. This will generate more diverse and interesting text. The categorical() function samples random class indices, given the class log probabilities (logits). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4diFK4K86F2",
        "colab_type": "code",
        "outputId": "2d727716-1a99-4d3a-ec63-ec0a0c3057c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
              "        2, 0, 0, 1, 1, 1, 0, 0, 1, 2, 0, 0, 1, 1, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i1gQztO_CHO",
        "colab_type": "text"
      },
      "source": [
        "To have more control over the diversity of the generated text, we can divide the logits by a number called the temperature, which we can tweak as we wish: a temperature close to 0 will favor the highprobability characters, while a very high temperature will give all characters an equal probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiSMh9Bw-zuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def next_char(text, temperature=1):\n",
        "  X_new = preprocess([text])\n",
        "  y_proba = model.predict(X_new)[0, -1:, :]\n",
        "  rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "  char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
        "\n",
        "  return tokenizer.sequences_to_texts(char_id.numpy())[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UL8cBT8_rkj",
        "colab_type": "code",
        "outputId": "40814dcd-60de-4eb0-dbf5-ce563e5b8034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "next_char('How are yo', temperature=1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'u'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdhNYGjo_50R",
        "colab_type": "code",
        "outputId": "a4e03e75-9772-4040-b77b-cc2f57041556",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "next_char('How is lif', temperature=1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'t'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be5O71gYAPJc",
        "colab_type": "text"
      },
      "source": [
        "We can write a small function that will repeatedly call next_char() to get the next character and append it to the given text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5KMC4tK__L1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def complete_text(text, n_chars=100, temperature=1):\n",
        "  for _ in range(n_chars):\n",
        "    text += next_char(text, temperature)\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_TuQYOSCVcx",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to generate some text! Let’s try with different temperatures:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKXJ5bUOAuIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "print(complete_text('t', temperature=0.2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QV0xKuEA2UZ",
        "colab_type": "code",
        "outputId": "09903a95-24d8-4edc-9897-bf548593e754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(complete_text('t', temperature=1))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tperves me from this faults revenged,\n",
            "i bray it! supking, my enemy to the commoner to please yourselv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQBVv41iBggA",
        "colab_type": "code",
        "outputId": "f3505f84-e058-4d03-b4a2-a9d567533c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(complete_text('t', temperature=2))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t as't so;,' rare' reigs as sir; broighews cipixict hid\n",
            "curmom's helves with\n",
            "yoke, are remialumiens; \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDYIjVxuBjR_",
        "colab_type": "code",
        "outputId": "964108ea-025c-4904-8aa7-4c6a031008d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(complete_text('p', temperature=0.2))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prayes.\n",
            "they her care have the rest was deliver the heart and the belly and with her to the belly and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5Lt3Iw-CvVJ",
        "colab_type": "text"
      },
      "source": [
        "Apparently our Shakespeare model works best at a temperature close to 1. To generate more convincing text, you could try using more GRU layers and more neurons per layer, train for longer, and add some regularization.\n",
        "\n",
        "Moreover, the model is currently incapable of learning patterns longer than n_steps, which is just 100 characters. You could try\n",
        "making this window larger, but it will also make training harder, and even LSTM and GRU cells cannot handle very long sequences. \n",
        "\n",
        "Alternatively, you could use a stateful RNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks5X0LBXDCn8",
        "colab_type": "text"
      },
      "source": [
        "## Stateful RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0bSQJhIGGi0",
        "colab_type": "text"
      },
      "source": [
        "Until now, we have used only stateless RNNs: at each training iteration the model starts with a hidden state full of zeros, then it updates this state at each time step, and after the last time step, it throws it away, as it is not needed anymore. \n",
        "\n",
        "What if we told the RNN to preserve this final state after processing one training batch and use it as the initial state for the next training batch? \n",
        "\n",
        "This way the model can learn long-term patterns despite only backpropagating through short sequences. This is called a stateful RNN.\n",
        "\n",
        "First, note that a stateful RNN only makes sense if each input sequence in a batch starts exactly where the corresponding sequence in the previous batch left off. So the first thing we need to do to build a stateful RNN is to use sequential and non-overlapping input sequences (rather than the shuffled and overlapping sequences we used to train stateless RNNs).\n",
        "\n",
        "When creating the Dataset, we must therefore use shift=n_steps (instead of shift=1), when calling the window() method. Moreover,\n",
        "we must obviously not call the shuffle() method.\n",
        "\n",
        "Unfortunately, batching is much harder when preparing a dataset for a stateful RNN than it is for a stateless RNN.\n",
        "\n",
        "Indeed, if we were to call batch(32), then 32 consecutive windows would be put in the same batch, and the following batch would not continue each of these window where it left off. \n",
        "\n",
        "The first batch would contain windows 1 to 32 and the second batch would contain windows 33 to 64, so if you consider, say, the first window of each batch (i.e., windows 1 and 33), you can see that they are not consecutive. The simplest solution to this problem is to just use “batches” containing a single window:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sjjp95VWIERC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_ZuX2QHIRTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "dataset = dataset.repeat().batch(1)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA5N4gRAJ-xa",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/sequence-fragments-for-stateful-rnn.png?raw=1' width='800'/>\n",
        "\n",
        "Batching is harder, but it is not impossible. For example, we could chop Shakespeare’s text into 32 texts of equal length, create one dataset of consecutive input sequences for each of them, and finally use tf.train.Dataset.zip(datasets).map(lambda\n",
        "*windows: tf.stack(windows)) to create proper consecutive batches, where the nth input sequence in a batch starts off exactly where the nth input sequence ended in the previous batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRMilRTpJYn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
        "datasets = []\n",
        "\n",
        "for encoded_part in encoded_parts:\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
        "  dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "  datasets.append(dataset)\n",
        "\n",
        "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
        "dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PJnlkoPMA19",
        "colab_type": "text"
      },
      "source": [
        "Now let’s create the stateful RNN. \n",
        "\n",
        "First, we need to set stateful=True when creating every recurrent layer. \n",
        "\n",
        "Second, the stateful RNN needs to know the batch size (since it\n",
        "will preserve a state for each input sequence in the batch), so we must set the batch_input_shape argument in the first layer.\n",
        "\n",
        "Note that we can leave the second dimension unspecified, since the inputs could have any length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSRmeZMeL1lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, recurrent_dropout=0.2, \n",
        "                     batch_input_shape=[batch_size, None, max_id]),\n",
        "    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, recurrent_dropout=0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))                             \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68CYyl4INbEa",
        "colab_type": "text"
      },
      "source": [
        "At the end of each epoch, we need to reset the states before we go back to the beginning of the text. For this, we can use a small callback:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrpKCmLqNVCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "  def on_epoch_begin(self, epoch, logs):\n",
        "    self.model.reset_states()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY4-qdKGN018",
        "colab_type": "text"
      },
      "source": [
        "And now we can compile and fit the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tIj15-fNwXm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6fc5d9bd-7bc8-45d5-e9c1-e9aec2b34963"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "steps_per_epoch = train_size // batch_size // n_steps\n",
        "\n",
        "model.fit(dataset, epochs=50, steps_per_epoch=steps_per_epoch, callbacks=[ResetStatesCallback()])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 313 steps\n",
            "Epoch 1/50\n",
            "313/313 [==============================] - 109s 348ms/step - loss: 2.6224\n",
            "Epoch 2/50\n",
            "313/313 [==============================] - 106s 339ms/step - loss: 2.2280\n",
            "Epoch 3/50\n",
            "313/313 [==============================] - 106s 339ms/step - loss: 2.1504\n",
            "Epoch 4/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 2.4703\n",
            "Epoch 5/50\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 2.3564\n",
            "Epoch 6/50\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 2.2239\n",
            "Epoch 7/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 2.0765\n",
            "Epoch 8/50\n",
            "313/313 [==============================] - 105s 337ms/step - loss: 2.0493\n",
            "Epoch 9/50\n",
            "313/313 [==============================] - 105s 335ms/step - loss: 2.0224\n",
            "Epoch 10/50\n",
            "313/313 [==============================] - 105s 335ms/step - loss: 1.9398\n",
            "Epoch 11/50\n",
            "313/313 [==============================] - 106s 337ms/step - loss: 1.9237\n",
            "Epoch 12/50\n",
            "313/313 [==============================] - 105s 335ms/step - loss: 1.8705\n",
            "Epoch 13/50\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 1.8721\n",
            "Epoch 14/50\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 1.7946\n",
            "Epoch 15/50\n",
            "313/313 [==============================] - 105s 334ms/step - loss: 1.7937\n",
            "Epoch 16/50\n",
            "313/313 [==============================] - 104s 334ms/step - loss: 1.7369\n",
            "Epoch 17/50\n",
            "313/313 [==============================] - 104s 333ms/step - loss: 1.7189\n",
            "Epoch 18/50\n",
            "313/313 [==============================] - 104s 334ms/step - loss: 1.7045\n",
            "Epoch 19/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 1.7021\n",
            "Epoch 20/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 1.6820\n",
            "Epoch 21/50\n",
            "313/313 [==============================] - 106s 337ms/step - loss: 1.6724\n",
            "Epoch 22/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 1.6631\n",
            "Epoch 23/50\n",
            "313/313 [==============================] - 104s 333ms/step - loss: 1.6555\n",
            "Epoch 24/50\n",
            "313/313 [==============================] - 103s 329ms/step - loss: 1.6485\n",
            "Epoch 25/50\n",
            "313/313 [==============================] - 105s 334ms/step - loss: 1.6410\n",
            "Epoch 26/50\n",
            "313/313 [==============================] - 105s 337ms/step - loss: 1.6365\n",
            "Epoch 27/50\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 1.6305\n",
            "Epoch 28/50\n",
            "313/313 [==============================] - 106s 337ms/step - loss: 1.6260\n",
            "Epoch 29/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 1.6215\n",
            "Epoch 30/50\n",
            "313/313 [==============================] - 105s 337ms/step - loss: 1.6181\n",
            "Epoch 31/50\n",
            "313/313 [==============================] - 106s 337ms/step - loss: 1.6126\n",
            "Epoch 32/50\n",
            "313/313 [==============================] - 105s 337ms/step - loss: 1.6084\n",
            "Epoch 33/50\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 1.6067\n",
            "Epoch 34/50\n",
            "313/313 [==============================] - 105s 337ms/step - loss: 1.6021\n",
            "Epoch 35/50\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 1.5995\n",
            "Epoch 36/50\n",
            "313/313 [==============================] - 105s 337ms/step - loss: 1.5965\n",
            "Epoch 37/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 1.5946\n",
            "Epoch 38/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 1.5907\n",
            "Epoch 39/50\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 1.5874\n",
            "Epoch 40/50\n",
            "313/313 [==============================] - 106s 339ms/step - loss: 1.5862\n",
            "Epoch 41/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 1.5845\n",
            "Epoch 42/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 1.5827\n",
            "Epoch 43/50\n",
            "313/313 [==============================] - 106s 340ms/step - loss: 1.5783\n",
            "Epoch 44/50\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 1.5781\n",
            "Epoch 45/50\n",
            "313/313 [==============================] - 105s 335ms/step - loss: 1.5743\n",
            "Epoch 46/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 1.5728\n",
            "Epoch 47/50\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 1.5718\n",
            "Epoch 48/50\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 1.5697\n",
            "Epoch 49/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 1.5680\n",
            "Epoch 50/50\n",
            "313/313 [==============================] - 105s 335ms/step - loss: 1.5671\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f951e5fafd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osuKwGg2P1xc",
        "colab_type": "text"
      },
      "source": [
        "After this model is trained, it will only be possible to use it to make predictions for batches of the same size as were used during training.\n",
        "\n",
        "To avoid this restriction, create an identical stateless model,\n",
        "and copy the stateful model’s weights to this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ejpRyHyP5Hf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stateless_model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))                                       \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7JnYH00QXpX",
        "colab_type": "text"
      },
      "source": [
        "To set the weights, we first need to build the model (so the weights get created):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aPB8sjdQYRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stateless_model.build(tf.TensorShape([None, None, max_id]))\n",
        "\n",
        "stateless_model.set_weights(model.get_weights())\n",
        "model = stateless_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwQNL81yQ3-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAchdyp1RTd1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b3532688-bb38-4923-b3c9-a6ef5c8a2371"
      },
      "source": [
        "print(complete_text('t'))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tant honour friends.\n",
            "'tis mave strented by\n",
            "do him of discoriolanus and voice as bloody the letter's g\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPigt2OuQ4mI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "7ec0d4bb-f151-406e-b18c-94ef1901d772"
      },
      "source": [
        "print(complete_text('t', temperature=0.2))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tish the death,\n",
            "and the stand the change of the seal the present\n",
            "to the poor son, the strange to the \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aMqeraFRNFN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "ed2ec6c8-6a7b-4eeb-f898-05332d24aac2"
      },
      "source": [
        "print(complete_text('t', temperature=1))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t as sost; for we his\n",
            "warwick,--and i death in point be cause\n",
            "the kind welcome that require: no, no w\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyuCGtGGRO53",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "0900336f-8d84-4d40-e8f1-3d88231fd856"
      },
      "source": [
        "print(complete_text('t', temperature=2))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t:\n",
            "aqhneed's me: bie o. caws-dafe?\n",
            "at,,--by him comb!ocjnar'd court'rt?'\n",
            "the voicoftiels faebe-clace.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uES1zAy3RQjl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "5b688082-1b6c-47c7-816f-385a7e28dca2"
      },
      "source": [
        "print(complete_text('p', temperature=0.2))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "part of the seeming the command\n",
            "to the country to the stand the stands of the words,\n",
            "and i will be so\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABtXeJg4PO8Z",
        "colab_type": "text"
      },
      "source": [
        "Now that we have built a character-level model, it’s time to look at word-level models\n",
        "and tackle a common natural language processing task: sentiment analysis."
      ]
    }
  ]
}