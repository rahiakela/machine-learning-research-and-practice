{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-encoder–decoder-network-for-neural-machine-translation.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMrFiv+IxhNtKd56GbH9/o6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow/blob/16-natural-nanguage-processing-with-RNNs-and-Attention/3_encoder%E2%80%93decoder_network_for_neural_machine_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8RaNVQswd4h",
        "colab_type": "text"
      },
      "source": [
        "# An Encoder–Decoder Network for Neural Machine Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd94gCIcxCV_",
        "colab_type": "text"
      },
      "source": [
        "Let’s take a look at a simple neural machine translation model that will translate English sentences to French.\n",
        "\n",
        "In short, the English sentences are fed to the encoder, and the decoder outputs the French translations. Note that the French translations are also used as inputs to the decoder, but shifted back by one step.\n",
        "\n",
        "In other words, the decoder is given as input the word that it should have output at the previous step (regardless of what it actually output). For the very first word, it is given the start-of-sequence (SOS) token. The decoder is expected to end the sentence with an end-of-sequence (EOS) token.\n",
        "\n",
        "Note that the English sentences are reversed before they are fed to the encoder. For example, “I drink milk” is reversed to “milk drink I.” \n",
        "\n",
        "This ensures that the beginning of the English sentence will be fed last to the encoder, which is useful because that’s generally the first thing that the decoder needs to translate.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/simple-machine-translation-model.png?raw=1' width='800'/>\n",
        "\n",
        "Each word is initially represented by its ID (e.g., 288 for the word “milk”). Next, an embedding layer returns the word embedding. These word embeddings are what is actually fed to the encoder and the decoder.\n",
        "\n",
        "At each step, the decoder outputs a score for each word in the output vocabulary (i.e., French), and then the softmax layer turns these scores into probabilities.\n",
        "\n",
        "For example, at the first step the word “Je” may have a probability of 20%, “Tu” may have a probability of 1%, and so on. The word with the highest probability is output. This is\n",
        "very much like a regular classification task, so you can train the model using the \"sparse_categorical_crossentropy\" loss, much like we did in the Char-RNN model.\n",
        "\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/inference-time.png?raw=1' width='800'/>\n",
        "\n",
        "Note that at inference time (after training), you will not have the target sentence to feed to the decoder. Instead, simply feed the decoder the word that it output at the previous step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lez5tkil20KR",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQc1zzuL22tJ",
        "colab_type": "code",
        "outputId": "419a1c7d-444d-443c-f947-dd964fd24e53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import sys\n",
        "assert sys.version_info >= (3, 5)  # Python ≥3.5 is required\n",
        "\n",
        "import sklearn \n",
        "assert sklearn.__version__ >= \"0.20\"  # Scikit-Learn ≥0.20 is required\n",
        "\n",
        "# %tensorflow_version only exists in Colab.\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "  IS_COLAB = True\n",
        "except Exception:\n",
        "  IS_COLAB = False\n",
        "  pass\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= '2.0'\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj7rMsjS33M9",
        "colab_type": "text"
      },
      "source": [
        "## Encoder–Decoder Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4m2DXq838sv",
        "colab_type": "text"
      },
      "source": [
        "OK, now you have the big picture. Still, there are a few more details to handle if you implement this model:\n",
        "\n",
        "* So far we have assumed that all input sequences (to the encoder and to the decoder) have a constant length. But obviously sentence lengths vary. Since regular tensors have fixed shapes, they can only contain sentences of the same length.\n",
        "You can use masking to handle this, as discussed earlier. However, if the sentences have very different lengths, you can’t just crop them like we did for sentiment analysis (because we want full translations, not cropped translations). Instead,\n",
        "group sentences into buckets of similar lengths (e.g., a bucket for the 1- to 6- word sentences, another for the 7- to 12-word sentences, and so on), using padding for the shorter sequences to ensure all sentences in a bucket have the same\n",
        "length (check out the tf.data.experimental.bucket_by_sequence_length()\n",
        "function for this). For example, “I drink milk” becomes “<pad> <pad> <pad> milk drink I.”\n",
        "\n",
        "* We want to ignore any output past the EOS token, so these tokens should not contribute to the loss (they must be masked out). For example, if the model outputs “Je bois du lait <eos> oui,” the loss for the last word should be ignored.\n",
        "\n",
        "* When the output vocabulary is large (which is the case here), outputting a probability for each and every possible word would be terribly slow. If the target vocabulary contains, say, 50,000 French words, then the decoder would output 50,000-dimensional vectors, and then computing the softmax function over such\n",
        "a large vector would be very computationally intensive. To avoid this, one solution is to look only at the logits output by the model for the correct word and for a random sample of incorrect words, then compute an approximation of the loss based only on these logits. This sampled softmax technique was introduced in 2015. In TensorFlow you can use the tf.nn.sampled_softmax_loss() function for this during training and use the normal softmax\n",
        "function at inference time (sampled softmax cannot be used at inference time because it requires knowing the target).\n",
        "\n",
        "The TensorFlow Addons project includes many sequence-to-sequence tools to let you easily build production-ready Encoder–Decoders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET9GLvf55p9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "vocab_size = 100\n",
        "embed_size = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejliVumo6IxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_addons as tfa\n",
        "\n",
        "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
        "\n",
        "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
        "encoder_embeddings = embeddings(encoder_inputs)\n",
        "decoder_embeddings = embeddings(decoder_inputs)\n",
        "\n",
        "encoder = keras.layers.LSTM(512, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
        "encoder_state = [state_h, state_c]\n",
        "\n",
        "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "\n",
        "decoder_cell = keras.layers.LSTMCell(512)\n",
        "output_layer = keras.layers.Dense(vocab_size)\n",
        "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, output_layer=output_layer)\n",
        "\n",
        "final_outputs, final_state, final_sequence_lengths = decoder(decoder_embeddings, \n",
        "                                                             initial_state=encoder_state, \n",
        "                                                             sequence_length=sequence_lengths)\n",
        "\n",
        "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
        "\n",
        "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs, sequence_lengths], outputs=[Y_proba])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLMOSKWLLk5O",
        "colab_type": "text"
      },
      "source": [
        "There are a few points to note:\n",
        "\n",
        "* First, we set return_state=True when creating the LSTM layer so that we can get its final hidden state and pass it to the decoder. Since we are using an LSTM cell, it actually returns two hidden states (short term and long term).\n",
        "\n",
        "* The TrainingSampler is one of several samplers available in TensorFlow Addons: their role is to tell the decoder at each step\n",
        "what it should pretend the previous output was. During inference, this should be the embedding of the token that was actually output. During training, it should be the embedding of the previous target token: this is why we used the TrainingSampler.\n",
        "\n",
        "In practice, it is often a good idea to start training with the embedding of the target of the previous time step and gradually transition to using the embedding of the actual token that was output at the previous step.\n",
        "\n",
        "The ScheduledEmbeddingTrainingSampler will randomly choose between the target or the actual output, with a probability that you can gradually change during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKtyYmqqKwp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H2XviFSK5dY",
        "colab_type": "code",
        "outputId": "b139a667-c185-42df-8b2c-7ea83e825656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "X = np.random.randint(100, size=10*1000).reshape(1000, 10)\n",
        "Y = np.random.randint(100, size=15*1000).reshape(1000, 15)\n",
        "X_decoder = np.c_[np.zeros((1000, 1)), Y[:, :-1]]\n",
        "seq_lengths = np.full([1000], 15)\n",
        "\n",
        "history = model.fit([X, X_decoder, seq_lengths], Y, epochs=2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1000 samples\n",
            "Epoch 1/2\n",
            "1000/1000 [==============================] - 9s 9ms/sample - loss: 4.6050\n",
            "Epoch 2/2\n",
            "1000/1000 [==============================] - 1s 1ms/sample - loss: 4.6038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS6hYjM-MXZi",
        "colab_type": "text"
      },
      "source": [
        "## Bidirectional RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F-Teu9-MYQv",
        "colab_type": "text"
      },
      "source": [
        "A each time step, a regular recurrent layer only looks at past and present inputs before generating its output. In other words, it is “causal,” meaning it cannot look into the future. \n",
        "\n",
        "This type of RNN makes sense when forecasting time series, but for many NLP tasks, such as Neural Machine Translation, it is often preferable to look ahead at the next words before encoding a given word.\n",
        "\n",
        "For example, consider the phrases “the Queen of the United Kingdom,” “the queen of hearts,” and “the queen bee”: to properly\n",
        "encode the word “queen,” you need to look ahead. \n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/bidirectional-recurrent-layer.png?raw=1' width='800'/>\n",
        "\n",
        "To implement this, run two recurrent layers on the same inputs, one reading the words from left to right and the other reading them from right to left. Then simply combine their outputs at each time step, typically by concatenating them. This is called a bidirectional recurrent layer.\n",
        "\n",
        "To implement a bidirectional recurrent layer in Keras, wrap a recurrent layer in a keras.layers.Bidirectional layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWGihy4yLYC8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "64c31801-6021-4725-a18c-24a3ce27ee30"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "   keras.layers.GRU(10, return_sequences=True, input_shape=[None, 10]),\n",
        "   keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True))                              \n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru (GRU)                    (None, None, 10)          660       \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 20)          1320      \n",
            "=================================================================\n",
            "Total params: 1,980\n",
            "Trainable params: 1,980\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1p4M9yVCxdQ",
        "colab_type": "text"
      },
      "source": [
        "The Bidirectional layer will create a clone of the GRU layer (but in the reverse direction), and it will run both and concatenate their outputs. So although the GRU layer has 10 units, the Bidirectional layer will output 20 values per time step."
      ]
    }
  ]
}