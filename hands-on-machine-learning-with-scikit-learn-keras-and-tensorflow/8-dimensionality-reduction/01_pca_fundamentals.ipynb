{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-pca-fundamentals.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOruT6Drk09i34BdIVGKYTo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/machine-learning-research-and-practice/blob/main/hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow/8-dimensionality-reduction/01_pca_fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Principal Component Analysis Fundamentals"
      ],
      "metadata": {
        "id": "bg1B9IbnnPYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Principal Component Analysis** (PCA) is by far the most popular dimensionality reduction\n",
        "algorithm. First it identifies the hyperplane that lies closest to the data, and then\n",
        "it projects the data onto it.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-research-and-practice/blob/main/hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow/8-dimensionality-reduction/images/0.png?raw=1' width='600'/>\n",
        "\n",
        "**Preserving the Variance**\n",
        "\n",
        "Before you can project the training set onto a lower-dimensional hyperplane, you\n",
        "first need to choose the right hyperplane.\n",
        "\n",
        "For example, a simple 2D dataset is represented\n",
        "on the left in figure, along with three different axes (i.e., 1D hyperplanes).\n",
        "On the right is the result of the projection of the dataset onto each of these axes.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-research-and-practice/blob/main/hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow/8-dimensionality-reduction/images/1.png?raw=1' width='600'/>\n",
        "\n",
        "As you can see, the projection onto the solid line preserves the maximum variance, while\n",
        "the projection onto the dotted line preserves very little variance and the projection\n",
        "onto the dashed line preserves an intermediate amount of variance.\n",
        "\n",
        "It seems reasonable to select the axis that preserves the maximum amount of variance,\n",
        "as it will most likely lose less information than the other projections. Another\n",
        "way to justify this choice is that it is the axis that minimizes the mean squared distance\n",
        "between the original dataset and its projection onto that axis. This is the rather\n",
        "simple idea behind [**PCA**](https://www.tandfonline.com/doi/pdf/10.1080/14786440109462720).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SDMdO_INpby-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "Aq3w08Crud4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.datasets import make_swiss_roll\n",
        "\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import FancyArrowPatch\n",
        "from mpl_toolkits.mplot3d import proj3d\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ],
      "metadata": {
        "id": "B6iRs6x3ufPm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build 3D dataset."
      ],
      "metadata": {
        "id": "zYpTazWr4zR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(4)\n",
        "m = 60\n",
        "w1, w2 = 0.1, 0.3\n",
        "noise = 0.1\n",
        "\n",
        "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
        "X = np.empty((m, 3))\n",
        "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
        "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
        "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
      ],
      "metadata": {
        "id": "NNAqTbBx42Ov"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Principal Components"
      ],
      "metadata": {
        "id": "e3tO7AM5qIgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA identifies the axis that accounts for the largest amount of variance in the training set.\n",
        "\n",
        "It also finds a second axis, orthogonal to the\n",
        "first one, that accounts for the largest amount of remaining variance. \n",
        "\n",
        "\n",
        "In this 2D\n",
        "example there is no choice: it is the dotted line. If it were a higher-dimensional dataset,\n",
        "PCA would also find a third axis, orthogonal to both previous axes, and a fourth,\n",
        "a fifth, and so on—as many axes as the number of dimensions in the dataset.\n",
        "\n",
        "The $i^{th}$ axis is called the $i^{th}$ principal component (PC) of the data.\n",
        "\n",
        "- The first PC is the axis on which vector $c_1$ lies\n",
        "- The second PC is the axis on which vector $c_2$ lies\n",
        "\n",
        "The first two PCs are the orthogonal axes on which the\n",
        "two arrows lie, on the plane, and the third PC is the axis orthogonal to that plane.\n",
        "\n",
        "So how can you find the principal components of a training set?\n",
        "\n",
        "Luckily, there is a standard matrix factorization technique called **Singular Value Decomposition** (SVD)\n",
        "that can decompose the training set matrix $X$ into the matrix multiplication of three\n",
        "matrices $U Σ V^⊺$, where $V$ contains the unit vectors that define all the principal components\n",
        "that we are looking for.\n",
        "\n",
        "$\n",
        "\\mathbf{V}^T =\n",
        "\\begin{pmatrix}\n",
        "  \\mid & \\mid & & \\mid \\\\\n",
        "  \\mathbf{c_1} & \\mathbf{c_2} & \\cdots & \\mathbf{c_n} \\\\\n",
        "  \\mid & \\mid & & \\mid\n",
        "\\end{pmatrix}\n",
        "$\n",
        "\n",
        "The following Python code uses NumPy’s `svd()` function to obtain all the principal\n",
        "components of the training set, then extracts the two unit vectors that define the first\n",
        "two PCs:"
      ],
      "metadata": {
        "id": "S8fXu5hx4dH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# don’t forget to center the data first\n",
        "X_centered = X - X.mean(axis=0)\n",
        "\n",
        "U, s , Vt = np.linalg.svd(X_centered)\n",
        "c1 = Vt.T[:, 0]\n",
        "c2 = Vt.T[:, 1]"
      ],
      "metadata": {
        "id": "FWJrETqn4qx1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA assumes that the dataset is centered around the origin.\n",
        "\n",
        "If you implement PCA yourself, or if you use other libraries, don’t forget to center the data first."
      ],
      "metadata": {
        "id": "vO9S5SPr5lyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c1"
      ],
      "metadata": {
        "id": "i9dxzcBn5uMs",
        "outputId": "b67e0f52-ccee-4d5c-91bc-87c422a8caec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.93636116, 0.29854881, 0.18465208])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c2"
      ],
      "metadata": {
        "id": "O0epvrQM5wHe",
        "outputId": "2042f8bf-73bf-48f3-c8ca-23b542edf597",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.34027485,  0.90119108,  0.2684542 ])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c3 = Vt.T[:, 2]\n",
        "c3"
      ],
      "metadata": {
        "id": "X2mgQRMO58CM",
        "outputId": "477ad241-c0b9-4c93-fe62-356a6a61733f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.08626012, -0.31420255,  0.94542898])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Projecting Down to d Dimensions"
      ],
      "metadata": {
        "id": "zrLiT5M56Niy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "U5-ggtyN6OP2"
      }
    }
  ]
}