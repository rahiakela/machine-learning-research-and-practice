{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-pca-fundamentals.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO1AoE323vENHxgjsiL+vlL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/machine-learning-research-and-practice/blob/main/hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow/8-dimensionality-reduction/01_pca_fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Principal Component Analysis Fundamentals"
      ],
      "metadata": {
        "id": "bg1B9IbnnPYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Principal Component Analysis** (PCA) is by far the most popular dimensionality reduction\n",
        "algorithm. First it identifies the hyperplane that lies closest to the data, and then\n",
        "it projects the data onto it.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-research-and-practice/blob/main/hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow/8-dimensionality-reduction/images/0.png?raw=1' width='600'/>\n",
        "\n",
        "**Preserving the Variance**\n",
        "\n",
        "Before you can project the training set onto a lower-dimensional hyperplane, you\n",
        "first need to choose the right hyperplane.\n",
        "\n",
        "For example, a simple 2D dataset is represented\n",
        "on the left in figure, along with three different axes (i.e., 1D hyperplanes).\n",
        "On the right is the result of the projection of the dataset onto each of these axes.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-research-and-practice/blob/main/hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow/8-dimensionality-reduction/images/1.png?raw=1' width='600'/>\n",
        "\n",
        "As you can see, the projection onto the solid line preserves the maximum variance, while\n",
        "the projection onto the dotted line preserves very little variance and the projection\n",
        "onto the dashed line preserves an intermediate amount of variance.\n",
        "\n",
        "It seems reasonable to select the axis that preserves the maximum amount of variance,\n",
        "as it will most likely lose less information than the other projections. Another\n",
        "way to justify this choice is that it is the axis that minimizes the mean squared distance\n",
        "between the original dataset and its projection onto that axis. This is the rather\n",
        "simple idea behind [**PCA**](https://www.tandfonline.com/doi/pdf/10.1080/14786440109462720).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SDMdO_INpby-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "Aq3w08Crud4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.datasets import make_swiss_roll\n",
        "\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import FancyArrowPatch\n",
        "from mpl_toolkits.mplot3d import proj3d\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ],
      "metadata": {
        "id": "B6iRs6x3ufPm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build 3D dataset."
      ],
      "metadata": {
        "id": "zYpTazWr4zR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(4)\n",
        "m = 60\n",
        "w1, w2 = 0.1, 0.3\n",
        "noise = 0.1\n",
        "\n",
        "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
        "X = np.empty((m, 3))\n",
        "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
        "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
        "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
      ],
      "metadata": {
        "id": "NNAqTbBx42Ov"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Principal Components"
      ],
      "metadata": {
        "id": "e3tO7AM5qIgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA identifies the axis that accounts for the largest amount of variance in the training set.\n",
        "\n",
        "It also finds a second axis, orthogonal to the\n",
        "first one, that accounts for the largest amount of remaining variance. \n",
        "\n",
        "\n",
        "In this 2D\n",
        "example there is no choice: it is the dotted line. If it were a higher-dimensional dataset,\n",
        "PCA would also find a third axis, orthogonal to both previous axes, and a fourth,\n",
        "a fifth, and so on—as many axes as the number of dimensions in the dataset.\n",
        "\n",
        "The $i^{th}$ axis is called the $i^{th}$ principal component (PC) of the data.\n",
        "\n",
        "- The first PC is the axis on which vector $c_1$ lies\n",
        "- The second PC is the axis on which vector $c_2$ lies\n",
        "\n",
        "The first two PCs are the orthogonal axes on which the\n",
        "two arrows lie, on the plane, and the third PC is the axis orthogonal to that plane.\n",
        "\n",
        "So how can you find the principal components of a training set?\n",
        "\n",
        "Luckily, there is a standard matrix factorization technique called **Singular Value Decomposition** (SVD)\n",
        "that can decompose the training set matrix $X$ into the matrix multiplication of three\n",
        "matrices $U Σ V^⊺$, where $V$ contains the unit vectors that define all the principal components\n",
        "that we are looking for.\n",
        "\n",
        "$\n",
        "\\mathbf{V}^T =\n",
        "\\begin{pmatrix}\n",
        "  \\mid & \\mid & & \\mid \\\\\n",
        "  \\mathbf{c_1} & \\mathbf{c_2} & \\cdots & \\mathbf{c_n} \\\\\n",
        "  \\mid & \\mid & & \\mid\n",
        "\\end{pmatrix}\n",
        "$\n",
        "\n",
        "The following Python code uses NumPy’s `svd()` function to obtain all the principal\n",
        "components of the training set, then extracts the two unit vectors that define the first\n",
        "two PCs:"
      ],
      "metadata": {
        "id": "S8fXu5hx4dH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# don’t forget to center the data first\n",
        "X_centered = X - X.mean(axis=0)\n",
        "\n",
        "U, s , Vt = np.linalg.svd(X_centered)\n",
        "c1 = Vt.T[:, 0]\n",
        "c2 = Vt.T[:, 1]"
      ],
      "metadata": {
        "id": "FWJrETqn4qx1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA assumes that the dataset is centered around the origin.\n",
        "\n",
        "If you implement PCA yourself, or if you use other libraries, don’t forget to center the data first."
      ],
      "metadata": {
        "id": "vO9S5SPr5lyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9dxzcBn5uMs",
        "outputId": "c78d0dcb-8508-4d61-b004-5caf2f61fcb6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.93636116, 0.29854881, 0.18465208])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0epvrQM5wHe",
        "outputId": "43783c0f-03fd-4967-9c91-f57d3ac33d82"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.34027485,  0.90119108,  0.2684542 ])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c3 = Vt.T[:, 2]\n",
        "c3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2mgQRMO58CM",
        "outputId": "6b945cf9-2084-41e7-a164-40ab354293a6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.08626012, -0.31420255,  0.94542898])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Projecting Down to d Dimensions"
      ],
      "metadata": {
        "id": "zrLiT5M56Niy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have identified all the principal components, you can reduce the dimensionality\n",
        "of the dataset down to `d` dimensions by projecting it onto the hyperplane\n",
        "defined by the first `d` principal components. Selecting this hyperplane ensures that the\n",
        "projection will preserve as much variance as possible.\n",
        "\n",
        "To project the training set onto the hyperplane and obtain a reduced dataset $X_{d-proj}$ of\n",
        "dimensionality `d`, compute the matrix multiplication of the training set matrix $X$ by\n",
        "the matrix $W_d$, defined as the matrix containing the first `d` columns of $V$.\n",
        "\n",
        "$\n",
        "\\mathbf{X}_{d\\text{-proj}} = \\mathbf{X} \\mathbf{W}_d\n",
        "$\n",
        "\n",
        "The following Python code projects the training set onto the plane defined by the first\n",
        "two principal components:"
      ],
      "metadata": {
        "id": "U5-ggtyN6OP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the matrix containing the first 2 columns of  V\n",
        "W2 = Vt.T[:, :2]\n",
        "# now project it onto the hyperplane and obtain a reduced dataset of dimensionality 2\n",
        "X2D = X_centered.dot(W2)"
      ],
      "metadata": {
        "id": "DJRg2Q7qlA1M"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X2D.shape  # reduced "
      ],
      "metadata": {
        "id": "WUftu5GxmLq5",
        "outputId": "65310365-e8bb-4cb8-ecc6-b67b85ab5e55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape  # original"
      ],
      "metadata": {
        "id": "gjwzhSjEmTUL",
        "outputId": "de4dd970-eb21-47a1-cf74-eeb15c0efdcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You now know how to reduce the dimensionality of any dataset\n",
        "down to any number of dimensions, while preserving as much variance as possible."
      ],
      "metadata": {
        "id": "zq69GUXxlaCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA using Scikit-Learn"
      ],
      "metadata": {
        "id": "xC_TZ35AmfxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-Learn’s PCA class uses SVD decomposition to implement PCA.\n",
        "\n",
        "Let's applies PCA to reduce the dimensionality\n",
        "of the dataset down to two dimensions (note that it automatically takes care of centering\n",
        "the data):"
      ],
      "metadata": {
        "id": "jSEYWcUtmhyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "X2D = pca.fit_transform(X)\n",
        "X2D.shape"
      ],
      "metadata": {
        "id": "UrL-pIzrmyqi",
        "outputId": "5e05ce7e-e98b-45da-926c-52b29ca8eabf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After fitting the PCA transformer to the dataset, its `components_` attribute holds the\n",
        "transpose of $W_d$ (e.g., the unit vector that defines the first principal component is\n",
        "equal to `pca.components_.T[:, 0])`."
      ],
      "metadata": {
        "id": "urpnfT6LnFnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca.components_"
      ],
      "metadata": {
        "id": "vLxHJpp3nJ2R",
        "outputId": "ca0a5185-0764-4352-c01f-b1f0e48b1eb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.93636116, -0.29854881, -0.18465208],\n",
              "       [ 0.34027485, -0.90119108, -0.2684542 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca.components_.T[:, 0]  # c1 = first principal component"
      ],
      "metadata": {
        "id": "VEelTCaRnNE_",
        "outputId": "c6ceb1d4-22ad-42c4-8490-6eb4e693b386",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.93636116, -0.29854881, -0.18465208])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca.components_.T[:, 1]  # c2 = second principal component"
      ],
      "metadata": {
        "id": "VHYbFQ-tnTQJ",
        "outputId": "fc0a466a-1a72-4042-b1f4-91e38e8e6d9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.34027485, -0.90119108, -0.2684542 ])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Explained Variance Ratio"
      ],
      "metadata": {
        "id": "xtMrmn7lnwHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another useful piece of information is the explained variance ratio of each principal\n",
        "component, available via the `explained_variance_ratio_` variable. The ratio indicates\n",
        "the proportion of the dataset’s variance that lies along each principal component.\n",
        "\n",
        "For example, let’s look at the explained variance ratios of the first two\n",
        "components of the 3D dataset."
      ],
      "metadata": {
        "id": "kVwFYHmDnxFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "0h4Qbd_An7lK",
        "outputId": "ad8e44ca-fa2c-4bd8-da1f-65e99dc25a78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.84248607, 0.14631839])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output tells you that 84.2% of the dataset’s variance lies along the first PC, and\n",
        "14.6% lies along the second PC. This leaves less than 1.2% for the third PC, so it is\n",
        "reasonable to assume that the third PC probably carries little information."
      ],
      "metadata": {
        "id": "3lrGjXBnoKqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca.explained_variance_"
      ],
      "metadata": {
        "id": "TUf5PjOMoAFE",
        "outputId": "a4e731a3-2184-4e54-ade6-d307a2e47fae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.77830975, 0.1351726 ])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Choosing the Right Number of Dimensions"
      ],
      "metadata": {
        "id": "tSshr7ZaoZ4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of arbitrarily choosing the number of dimensions to reduce down to, it is\n",
        "simpler to choose the number of dimensions that add up to a sufficiently large portion\n",
        "of the variance (e.g., 95%). \n",
        "\n",
        "Unless, of course, you are reducing dimensionality for\n",
        "data visualization—in that case you will want to reduce the dimensionality down to 2\n",
        "or 3.\n",
        "\n",
        "Let's performs PCA without reducing dimensionality, then computes\n",
        "the minimum number of dimensions required to preserve 95% of the training set’s\n",
        "variance:"
      ],
      "metadata": {
        "id": "ig0v-Ty-ob-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
        "d = np.argmax(cumsum >= 0.95) + 1\n",
        "d"
      ],
      "metadata": {
        "id": "upa_rzQjoqGX",
        "outputId": "daca33ea-0ea6-4cef-af14-9ea9fbed7cb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You could then set `n_components=d` and run PCA again. But there is a much better\n",
        "option: instead of specifying the number of principal components you want to preserve,\n",
        "you can set `n_components` to be a float between 0.0 and 1.0, indicating the ratio\n",
        "of variance you wish to preserve:"
      ],
      "metadata": {
        "id": "OjIJgCF0rAlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=0.95)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "X_reduced.shape"
      ],
      "metadata": {
        "id": "PBOFi1zKrF3P",
        "outputId": "5303825a-e088-4bfc-df05-e6660de6d747",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yet another option is to plot the explained variance as a function of the number of\n",
        "dimensions.\n",
        "\n",
        "<img src='https://github.com/rahiakela/machine-learning-research-and-practice/blob/main/hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow/8-dimensionality-reduction/images/2.png?raw=1' width='600'/>\n",
        "\n",
        "There will usually be an elbow in the\n",
        "curve, where the explained variance stops growing fast. In this case, you can see that\n",
        "reducing the dimensionality down to about 100 dimensions wouldn’t lose too much\n",
        "explained variance."
      ],
      "metadata": {
        "id": "zC8kkgGKrrSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PCA for Compression"
      ],
      "metadata": {
        "id": "92duNzK6sBdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "41AP2uQssCun"
      }
    }
  ]
}