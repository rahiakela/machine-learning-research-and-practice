{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_pca_linear_autoencoder.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNvpA37I0/nyJV5RqAiwUZB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow/blob/17-representation-learning-and-generative-learning-using-autoencoders-and-gans/1_pca_linear_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnckN6thNSsA",
        "colab_type": "text"
      },
      "source": [
        "# Performing PCA with an Undercomplete Linear Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mwMyTsCNjz1",
        "colab_type": "text"
      },
      "source": [
        "Autoencoders are artificial neural networks capable of learning dense representations of the input data, called latent representations or codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction, especially for visualization purposes. \n",
        "\n",
        "Autoencoders also act as feature detectors, and they can be used for unsupervised pretraining of deep neural networks. Lastly, some autoencoders are generative models: they are capable of randomly generating new data that looks very similar to the training data.\n",
        "\n",
        "For example, you could train an autoencoder on pictures of faces, and it would then be able to generate new faces. However, the generated images are usually fuzzy and not entirely realistic.\n",
        "\n",
        "In contrast, faces generated by generative adversarial networks (GANs) are now so convincing that it is hard to believe that the people they represent do not exist. You can judge so for yourself by visiting https://thispersondoesnotexist.com/, a website that shows faces generated by a recent GAN architecture called StyleGAN (you can also check out https://thisrentaldoesnotexist.com/ to see some generated Airbnb bedrooms).\n",
        "\n",
        "GANs are now widely used for super resolution (increasing the resolution of\n",
        "an image), colorization, powerful image editing (e.g., replacing photo bombers with realistic background), turning a simple sketch into a photorealistic image, predicting the next frames in a video, augmenting a dataset (to train other models), generating other types of data (such as text, audio, and time series), identifying the weaknesses in other models and strengthening them, and more.\n",
        "\n",
        "Autoencoders and GANs are both unsupervised, they both learn dense representations, they can both be used as generative models, and they have many similar applications.\n",
        "\n",
        "However, they work very differently:\n",
        "\n",
        "* Autoencoders simply learn to copy their inputs to their outputs. This may sound like a trivial task, but we will see that constraining the network in various ways can make it rather difficult. For example, you can limit the size of the latent representations, or you can add noise to the inputs and train the network to recover the original inputs. These constraints prevent the autoencoder from trivially copying the inputs directly to the outputs, which forces it to learn efficient ways of representing the data. In short, the codings are byproducts of the autoencoder learning the identity function under some constraints.\n",
        "\n",
        "* GANs are composed of two neural networks: a generator that tries to generate data that looks similar to the training data, and a discriminator that tries to tell real data from fake data. This architecture is very original in Deep Learning in that the generator and the discriminator compete against each other during training: the generator is often compared to a criminal trying to make realistic counterfeit money, while the discriminator is like the police investigator trying to tell real money from fake.\n",
        "\n",
        "We will start by exploring in more depth how autoencoders work and how to use them for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. This will naturally lead us to GANs. We will start by building a simple GAN to generate fake images, but we will see that training is often quite difficult.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHl__URfSt52",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wurCrWzuSva6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRUcip4qXPwx",
        "colab_type": "text"
      },
      "source": [
        "A couple utility functions to plot grayscale 28x28 image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vuc3bGF4XQWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_image(image):\n",
        "    plt.imshow(image, cmap=\"binary\")\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRyAYUz3SvwR",
        "colab_type": "text"
      },
      "source": [
        "## Efficient Data Representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpbJYI6GVQWS",
        "colab_type": "text"
      },
      "source": [
        "Which of the following number sequences do you find the easiest to memorize?\n",
        "\n",
        "* 40, 27, 25, 36, 81, 57, 10, 73, 19, 68\n",
        "* 50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28, 26, 24, 22, 20, 18, 16, 14\n",
        "\n",
        "At first glance, it would seem that the first sequence should be easier, since it is much shorter. However, if you look carefully at the second sequence, you will notice that it is just the list of even numbers from 50 down to 14. Once you notice this pattern, the second sequence becomes much easier to memorize than the first because you only need to remember the pattern (i.e., decreasing even numbers) and the starting and ending numbers (i.e., 50 and 14). \n",
        "\n",
        "Note that if you could quickly and easily memorize very long sequences, you would not care much about the existence of a pattern in the second sequence. You would just learn every number by heart, and that would be\n",
        "that. \n",
        "\n",
        "The fact that it is hard to memorize long sequences is what makes it useful to recognize patterns, and hopefully this clarifies why constraining an autoencoder during training pushes it to discover and exploit patterns in the data.\n",
        "\n",
        "An autoencoder looks at the inputs, converts them to an efficient latent representation, and then spits out something that (hopefully) looks very close to the inputs. An autoencoder is always composed of two parts: an encoder (or recognition network) that converts the inputs to a latent representation, followed by a decoder (or generative network) that converts the internal representation to the outputs.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/simple-autoencoder.png?raw=1' width='800'/>\n",
        "\n",
        "As you can see, an autoencoder typically has the same architecture as a Multi-Layer Perceptron, except that the number of neurons in the output\n",
        "layer must be equal to the number of inputs. In this example, there is just one hidden layer composed of two neurons (the encoder), and one output layer composed of three neurons (the decoder). \n",
        "\n",
        "The outputs are often called the reconstructions because the autoencoder tries to reconstruct the inputs, and the cost function contains a reconstruction loss that penalizes the model when the reconstructions are different from the inputs.\n",
        "\n",
        "Because the internal representation has a lower dimensionality than the input data (it is 2D instead of 3D), the autoencoder is said to be undercomplete. An undercomplete autoencoder cannot trivially copy its inputs to the codings, yet it must find a way to output a copy of its inputs. It is forced to learn the most important features in the input data (and drop the unimportant ones).\n",
        "\n",
        "\n",
        "Let’s see how to implement a very simple undercomplete autoencoder for dimensionality reduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHqLmF-zW-qu",
        "colab_type": "text"
      },
      "source": [
        "## PCA with an Undercomplete Linear Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiWsENewXAeD",
        "colab_type": "text"
      },
      "source": [
        "If the autoencoder uses only linear activations and the cost function is the mean squared error (MSE), then it ends up performing Principal Component Analysis.\n",
        "\n",
        "Let's generate 3D dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhQh71D8Xx8R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "578fc258-cb68-4cef-8a6d-7911636bacbe"
      },
      "source": [
        "np.random.seed(4)\n",
        "\n",
        "def generate_3d_data(m, w1=0.1, w2=0.3, noise=0.1):\n",
        "  angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
        "  data = np.empty((m, 3))\n",
        "  data[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
        "  data[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
        "  data[:, 2] = data[:, 0] * w1 + data[:, 1] * w2 + noise * np.random.randn(m)\n",
        "  return data\n",
        "\n",
        "X_train = generate_3d_data(60)\n",
        "X_train = X_train - X_train.mean(axis=0, keepdims=0)\n",
        "print(X_train.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7vF9KN3YeA3",
        "colab_type": "text"
      },
      "source": [
        "Now let's build the Autoencoder..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDb3JXGxYZNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "encoder = keras.models.Sequential([keras.layers.Dense(2, input_shape=[3])])\n",
        "decoder = keras.models.Sequential([keras.layers.Dense(3, input_shape=[2])])\n",
        "\n",
        "autoencoder = keras.models.Sequential([encoder, decoder])\n",
        "autoencoder.compile(loss='mse', optimizer=keras.optimizers.SGD(learning_rate=1.5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR-SMWWDZZem",
        "colab_type": "text"
      },
      "source": [
        "* We organized the autoencoder into two subcomponents: the encoder and the\n",
        "decoder. Both are regular Sequential models with a single Dense layer each, and the autoencoder is a Sequential model containing the encoder followed by the decoder (remember that a model can be used as a layer in another model).\n",
        "\n",
        "* The autoencoder’s number of outputs is equal to the number of inputs (i.e., 3).\n",
        "\n",
        "* To perform simple PCA, we do not use any activation function (i.e., all neurons are linear), and the cost function is the MSE.\n",
        "\n",
        "Now let’s train the model on a simple generated 3D dataset and use it to encode that same dataset (i.e., project it to 2D):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkHVg37yZR4f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "158da1b4-60d9-4cc6-ffd5-152a1f6a0432"
      },
      "source": [
        "history = autoencoder.fit(X_train, X_train, epochs=20)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60 samples\n",
            "Epoch 1/20\n",
            "60/60 [==============================] - 2s 32ms/sample - loss: 0.3064\n",
            "Epoch 2/20\n",
            "60/60 [==============================] - 0s 134us/sample - loss: 0.3797\n",
            "Epoch 3/20\n",
            "60/60 [==============================] - 0s 112us/sample - loss: 0.1454\n",
            "Epoch 4/20\n",
            "60/60 [==============================] - 0s 147us/sample - loss: 0.0872\n",
            "Epoch 5/20\n",
            "60/60 [==============================] - 0s 162us/sample - loss: 0.0658\n",
            "Epoch 6/20\n",
            "60/60 [==============================] - 0s 144us/sample - loss: 0.0464\n",
            "Epoch 7/20\n",
            "60/60 [==============================] - 0s 167us/sample - loss: 0.0356\n",
            "Epoch 8/20\n",
            "60/60 [==============================] - 0s 153us/sample - loss: 0.0238\n",
            "Epoch 9/20\n",
            "60/60 [==============================] - 0s 150us/sample - loss: 0.0225\n",
            "Epoch 10/20\n",
            "60/60 [==============================] - 0s 137us/sample - loss: 0.0093\n",
            "Epoch 11/20\n",
            "60/60 [==============================] - 0s 132us/sample - loss: 0.0068\n",
            "Epoch 12/20\n",
            "60/60 [==============================] - 0s 144us/sample - loss: 0.0067\n",
            "Epoch 13/20\n",
            "60/60 [==============================] - 0s 130us/sample - loss: 0.0073\n",
            "Epoch 14/20\n",
            "60/60 [==============================] - 0s 159us/sample - loss: 0.0054\n",
            "Epoch 15/20\n",
            "60/60 [==============================] - 0s 134us/sample - loss: 0.0051\n",
            "Epoch 16/20\n",
            "60/60 [==============================] - 0s 161us/sample - loss: 0.0055\n",
            "Epoch 17/20\n",
            "60/60 [==============================] - 0s 139us/sample - loss: 0.0050\n",
            "Epoch 18/20\n",
            "60/60 [==============================] - 0s 185us/sample - loss: 0.0049\n",
            "Epoch 19/20\n",
            "60/60 [==============================] - 0s 161us/sample - loss: 0.0051\n",
            "Epoch 20/20\n",
            "60/60 [==============================] - 0s 146us/sample - loss: 0.0054\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idVNAuvQZyOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "codings = encoder.predict(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tACIK7DHaYL5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "0683c9c9-c872-4ec6-b7d5-f44eb6dddd94"
      },
      "source": [
        "codings[:10]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.5274061 ,  0.00450174],\n",
              "       [ 0.29497597,  0.30931056],\n",
              "       [-1.4017817 ,  0.03030768],\n",
              "       [-0.96917593,  0.6367279 ],\n",
              "       [-0.8871722 ,  0.59361136],\n",
              "       [ 1.2372295 , -0.4703033 ],\n",
              "       [-1.6398767 ,  0.01756443],\n",
              "       [ 0.42794442, -0.78036684],\n",
              "       [ 1.215723  , -0.1822743 ],\n",
              "       [ 0.8950341 ,  0.10711112]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_4qhUq_ad0c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "caad1e3f-bb08-4e11-c555-931d2ae11658"
      },
      "source": [
        "fig = plt.figure(figsize=(4,3))\n",
        "plt.plot(codings[:,0], codings[:, 1], \"b.\")\n",
        "plt.xlabel(\"$z_1$\", fontsize=18)\n",
        "plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR4AAADbCAYAAAC7m/5lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAASYElEQVR4nO3db4xc1XnH8e+zf1hcm1WCiTbKC+Ig\nQaQ6qR3bUbU0iCVEqRKFGoW2BGrcxiGWsJsodNsKRbEUkcqIqI5cMKF1I8Nu6kKixCAjGlVR5FFw\nvWplJKxq+8JEBKOoSQimYHaT7B/m6Ys74x3PzuzO7tx7zr0zv480Wubund2zl/FvznnOufeauyMi\nElJP7AaISPdR8IhIcAoeEQlOwSMiwSl4RCQ4BY+IBNcXuwFZu+qqq3zDhg2xm5G66elp1q5dG7sZ\nuaJjsljMY/L888+/5u7vavS9jg+eDRs2cPr06djNSF2pVGJkZCR2M3JFx2SxmMfEzM41+56GWiIS\nnIJHRIJT8IhIcAoeWdbEBDzwQPJVJA0dX1yW9kxMwM03w+wsXHYZ/OhHMDwcu1VSdOrxyJJKpSR0\n3n47+VoqxW6RdAIFjyxpZCTp6fT2Jl81Wy1p0FBLljQ8nAyvSqUkdDTMkjQoeGRZw8MKHEmXhloi\nEpyCR0SCU/CISHAKHhEJTsFTcFpVLEWkWa0C64RVxRMTmqrvRgqeAmu0qrhI/3g7IThldTTUKrCi\nryrW6RjdSz2eyNoZajRbVVyU4Us1OKs9nqIFp6yegieiNIYa9auKizR80ekY3UvBE1EWNZqi1X10\nOkZ3Uo0noixqNEWv+0h3UI8notUONSYm4OjRqxkYWPwaDV+aK0rtqxsoeCJb6VCjWsOZmXkfR482\nruFo+LJYkWpf3UBDrYKp1nDKZdMU9Apo6j5fogePmV1pZk+Z2bSZnTOzO5vs91UzmzOzqZrHNaHb\nG1u1htPTUw5ew8nT6RkrbYtqX/mSh6HWI8AsMARsBp41szPuPtlg3++4+46grVtCjJpBtYZz5MjL\n7Np1TbDf285QJe3jtJq2qPaVL1GDx8zWArcBH3D3KeCkmR0H7gLui9m25cSsGQwPw8zMKwwPh+vw\nrXaaPovjVLQlA7JY7B7PdcC8u5+t2XYGuLHJ/reY2evAz4FD7v5oo53MbDewG2BoaIhSBgP6o0ev\nZmbmfZTLxsxMmSNHXmZm5pXUf08zU1NTmfxdzQwODtLXtwl3o6/PGRw8Q6l0YdnXZXGcmrVlqWMy\nOTnI6Ogm5uZ66O8vc+DAGTZuXL79RRf6fdIyd4/2AG4AflG37fNAqcG+vwu8B+gFricJnzuW+x1b\nt271LJw65b5mjXtvb/L11KlMfk1TJ06cCPsLPfkb9+9f2d+a1XFq1Jaljsn+/UkbIPm6f3867ci7\nGO+TKuC0N/l3GbvHMwUM1m0bBN6q39Hd/6fm6Skz+wfgj4Ensmtec1nUDPK+zmQ10/RZ1VZW2had\nF5YvsYPnLNBnZte6+4uVbZuARoXleg5YZi1rQZrrZTp5nclqjlPaIazicr5EDR53nzazY8D9ZnY3\nyazWdpKh1CXMbDvwY+AN4MPAF4EvB2xuplQwXZBVCGthZX5EX8cD7AHWAK+SDJvucfdJM7vBzKZq\n9vsM8BOSYdg48KC7jwVvbUa0zmSBFvt1vthDLdz9deDWBtufA9bVPL8jZLtC01BggeoxnS968MiC\n2qFA3gvNaWn0dyqEO5+CJ4c6udBca6m/My/1mG75AAgtDzUeqdMtNY68/53VYNy3L/mah3PUOoWC\nJ4e6pdCc978z78FYZBpq5VC31Diqf+f4eOyWNKYid3YUPDmVlxpHCGNjyT/usbF81bPa/QBQfag5\nBY9E1akLJ7tlgmC1FDwSVZ6HM+2ER6cGaloUPBJVnutZ7YRHngM1DxQ8El1e61nr10NPD7ivPDzy\nHKh5oOARaWBiAr70paS309MDBw+ufFV5XgM1DxQ8Ig0s3M0DzOD8+WS7isbp0ALCGq3cuSBPd1qQ\n7DRb3KhFhelQj6eilU8yfdp1j2Y1GhWN06HgqWhlBkNTpN2lUY1GReN0KHgqWvkkW8mnnVatdi4V\njdun4Klo5ZOs1U87DclElqbgqdHKJ1kr+2hIJrI0zWplIO+Xe5Bi6cSZVPV4MqACpKSlOmyfmUk+\nyA4dgt27Y7eqfQqejKgAKWkolZLQKZeTx9698MEPFv+9paGWSI6NjCQ9napyuTMWLSp4RHJseDgZ\nXvX1JeeMDQx0Rs1QQy2RjLW7pmv37mR41Uk1QwWPSIbSWtO12prh5OQgExP5CywFj0iGYq7pmpiA\n0dFNzM/nbyGrajwiGYq5pqtUgrm5nlyeSa8ej0iGYq7pGhmB/v4y8/O9uVvIquARyViWa7qWKlwP\nD8OBA2e4cGGLajwiko5WCtcbN17IVU+nSjUekYIq8tUQWwoeM7vMzGbNzJs8jmXdUBG5VJFPRm51\nqNUP7Gqw/V5gC/BMai0qCF3oS2Ir8snILQWPu08D/1K7zcy+ThI6o+7+WAZtyy1d6Es6SYwP0RUX\nl83MgIeAvcBed/9m6q3KOV3oS/JgpR+AjQIm1ofoiorLZtYDHAb2AJ+rho6ZDZjZP5vZS2b2lpmd\nNbMvZNDeXKgdW/f2wiuvwOHDnXexJsm3lRSXqwGzb1/ytfo+jVWgbrnHY2a9wBhwO7DD3Z+o+zm/\nAD4OvAT8HvDvZvZLd/9uiu3NxEq7mtWx9fg4PPZYEjrl8sLZwxp6SQgruflAs156rNv1tDqr1Q88\nCfwJcHtd6ODu0+6+z91/4u5ld38BOA58pIWffaWZPWVm02Z2zszubLKfmdmDZna+8niwMuxrS7NP\ngqX2f+CB5L+vvhrm55PQgeRr0aY1pbiqH4Bf+9ryH3bNZsBW8jPStGyPx8wGgO8BHwM+7e7PtvCa\nfuAG4O9baMMjwCwwBGwGnjWzM+4+WbffbuBWYBPgwA+BnwL/2MLvaKqVek21R7R+fXI/7eqnw8GD\nydfqFeJ6eoo3rSnF1uqq6KVmwGJcLbOVodY48CngceCdZraj7vvH3f1C3bZDwFuV1zZlZmuB24AP\nuPsUcNLMjgN3AffV7f7nwAF3/1nltQeAz9Nm8CzV1ZyYSIZTR44kwWS2cAnK2dnkftrV/5nr1yfP\nizatKd0hb8s/zN2bfzMZyrwJXNFklzJwhbv/uuY13yDpHX3U3V9b8pebfQj4D3f/nZptfw3c6O63\n1O37JvBxd//PyvNtwAl3X9Q2M9tN0kNiaGho65NPPrlUM5icHOSFF97B5s1vsHHjhYvbRkc3MTvb\nQ3KIDLMyPT3gDv39zoEDZy7uH9rU1BTr1q2L8rvzSsdksampKc6dew+jo5uYm+uhv78c7H170003\nPe/u2xp9b8kejyepNNjqLzKzg8DNtBA6FeuA+iPQLOjWVb5Xu986MzOvS093P0wy+8a2bdt8ZJmx\nT6NvT0wk9ZvqTzaDyy/v4eDB2p7NliV/bpZKpRLL/V3dRsdksVKpxIULWy7WIufney+eNBpTaieJ\nmtlDwEeBm9z9Vy2+bIrFwTZIMkxbbt9BYKo+dNJSOwTr64PPfhZ27sxHN1VkJWLNXC0lleAxs/cC\nXwBmgJ/WTDY95+6fWOKlZ4E+M7vW3V+sbNsE1BeWqWzbBPzXMvulosjL0UVq5fG9nErwuPs5YMVT\n2+4+XTnB9H4zu5tkVms7cH2D3ceBvzKzfyOZ1RoFHl59q5ene2NJp2j2Xo5VdM7D9Xj2AEeAV4Hz\nwD3uPmlmNwA/cPdqtfCfgGuA/648/1Zlm4isQqPTJSBMEEUPHnd/nWR9Tv3250gKytXnDvxt5SEi\nbapfwzY+DmNjYc7b0oXARLpU/WpmCHfeVvQej4jEUV90hkt7PFnOfil4RLpYfdE51OyXgkdELgo1\nk6saj4gEp+ARkeAUPCISnIJHRIJT8IjIItUrbWZ1DXHNaonIJULceUI9HhG5RIg7Tyh42pR1l1Qk\ntBC3RtZQqw26o6h0ohDX71HwtEF3FJVOlfUKZg212hCiSyrSidTjaUMeLykpUgQKnjbp8qgiK6eh\nlogEp+ARkeAUPCISnIJHRIJT8IhIcAoeEQlOwSMiwSl4RCQ4BY+IBKfgEZHgFDwiEpyCR0SCU/CI\nSMvSuuKmzk4XkZakecVN9XhEpCVpXgRewSMiLUnzipsaaolIS9K84qaCR0RaltYVNzXUEpGmsrpv\nXNTgMbMrzewpM5s2s3NmducS+37VzObMbKrmcU3I9op0k+os1r59ydc0wyd2j+cRYBYYAv4MeNTM\nNi6x/3fcfV3N46UgrRTpQlneyjha8JjZWuA2YJ+7T7n7SeA4cFesNonIgizvGxezuHwdMO/uZ2u2\nnQFuXOI1t5jZ68DPgUPu/miWDRTpZlneNy5m8KwDLtRtexO4osn+3wUOA78Efh/4vpm94e5P1O9o\nZruB3QBDQ0OU0uwj5sTU1FRH/l3t0DFZMDk5yAsvvIP3v78PKLX1s4aHYWYm3aEW7p7Jg+Sv9SaP\nk8CHgF/XvWYUeKbFn38f8P3l9tu6dat3ohMnTsRuQu7omCROnXJfs8a9t9d9YGDeT52K0w7gtDf5\nd5lZjcfdR9zdmjw+ApwF+szs2pqXbQImW/0VgKXdbpGiqy0Kz81Zqj2Vwp8k6u7TZnYMuN/M7gY2\nA9uB6xvtb2bbgR8DbwAfBr4IfDlQc0UKo1oUnp2Fvj5PrSic5kmisVcu7wGOAK8C54F73H0SwMxu\nAH7g7usq+36msu8A8DPgQXcfC99kkXyrLQoPDp5heHhLKj+30fR6IYPH3V8Hbm3yvedICtDV53eE\napdI0VVPbSiV6udvVq+2J6WTREUkCJ0kKiJR6CRRESksBY+IXJTV2ej1NNQSESDd6fLlKHhEBLh0\nuvy3v4Xx8YXtnXSulojkyMhIcib622+DO3zrW3DkSPK82gMCzWqJSIqGh+GTn4Snn06ez8+DWRJC\ns7NJD2hsTLe3EZGUvfvdlz7v6Vm4Hg+kd2Ew9XhE5KKdO5Ph1dwc9PfDww/D+fMLq5RrezxauSwi\nqdm1K/m6c+fioZRWLotIquqn03fuXLyPVi6LSKqyvLh7PQWPiADZXty9noZaIgJke3H3egoeEbko\nrRrOcjTUEpHgFDwiBRbqbPK0aagl0oaJiTA1kWa/O9TZ5GlT8IisUux/+GlefD00DbVEVinkupdG\nQk5/p009HpFVSvOuC6sRcvo7bQoekVWq/Ye/fv1CjydkAISa/k6bgkekDdV/9EUt8sai4BFpU9pF\n3upM2fr1C5ek6LQgU/CItCnNWk91pmxmBsrl5EJcAwOd14tS8Ii0Kc0ib7X3VC4nz8vl4k2Vt0LB\nI5KCtIq81d5TbY+naFPlrVDwiORI/UyZajwiEkRRp8hXQiuXRSQ4BY+IBKfgEZHgFDwiEpy5e+w2\nZMrMfgWci92ODFwFvBa7ETmjY7JYzGPyXnd/V6NvdHzwdCozO+3u22K3I090TBbL6zHRUEtEglPw\niEhwCp7iOhy7ATmkY7JYLo+JajwiEpx6PCISnIJHRIJT8BScmf2lmZ02sxkzezx2e2IwsyvN7Ckz\nmzazc2Z2Z+w2xZb394XOTi++/wX+DvhDYE3ktsTyCDALDAGbgWfN7Iy7T8ZtVlS5fl+ox1Nw7n7M\n3Z8GzsduSwxmtha4Ddjn7lPufhI4DtwVt2Vx5f19oeCRorsOmHf3szXbzgAbI7VHWqDgkaJbB1yo\n2/YmcEWEtkiLFDw5ZmYlM/Mmj5Ox25cTU8Bg3bZB4K0IbZEWqbicY+4+ErsNBXAW6DOza939xcq2\nTUA3F5ZzTz2egjOzPjO7HOgFes3scjPrmg8Ud58GjgH3m9laM/sDYDvw7bgtiyvv7wsFT/F9BfgN\ncB+wo/LfX4naovD2kEwZvwo8AdzT5VPpkPP3hc7VEpHg1OMRkeAUPCISnIJHRIJT8IhIcAoeEQlO\nwSMiwSl4RCQ4BY+IBKfgEZHgFDwSlZldZmazS5yFfyx2GyV9uTlpTLpWP7CrwfZ7gS3AM2GbIyHo\nXC3JHTP7OvA3wKi7fyN2eyR96vFIbpiZAQ8Be4G97v7NyE2SjKjGI7lgZj0kt9vdA3yuNnTM7E/N\n7KSZTZnZy7HaKOlRj0eiM7NeYAy4Hdjh7k/U7fJ/wCGS29fcG7h5kgEFj0RlZv3AvwJ/BNzu7otm\nsdz9h5V9bw3cPMmIgkeiMbMB4HvAx4BPu/uzkZskgSh4JKZx4FPA48A7zWxH3fePu3v9rWukAyh4\nJIrKDNYnKk//ovKoVUb3xupYCh6JwpMFZPX3w5IuoeCR3KvMevVXHla5bYu7+0zclslqKXikCO4C\nHqt5/hvgHLAhSmukbTplQkSC08plEQlOwSMiwSl4RCQ4BY+IBKfgEZHgFDwiEpyCR0SCU/CISHD/\nD0S+bTwjuYibAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 288x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga2v6F2fa39r",
        "colab_type": "text"
      },
      "source": [
        "Note that the same dataset, X_train, is used as both the inputs and the targets.This shows the original 3D dataset (on the left) and the output of the autoencoder’s hidden layer (i.e., the coding layer, on the right). As you can see, the autoencoder found the best 2D plane to project the data onto, preserving as much variance in the data as it could (just like PCA).\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/linear-autoencoder.png?raw=1' width='800'/>\n",
        "\n",
        "You can think of autoencoders as a form of self-supervised learning\n",
        "(i.e., using a supervised learning technique with automatically generated\n",
        "labels, in this case simply equal to the inputs)."
      ]
    }
  ]
}