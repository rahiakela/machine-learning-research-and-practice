{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-tensorflow-data-api.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMQGXRkUXkgbYsnDTFTo0v/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow/blob/13-loading-and-preprocessing-data-with-tensorflow/1_tensorflow_data_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gffXA1BL54_S"
      },
      "source": [
        "# Loading and Preprocessing Data with TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWhm5Orq6O_p"
      },
      "source": [
        "Ingesting a large dataset and preprocessing it efficiently can be tricky to implement with other **Deep Learning** libraries, but **TensorFlow** makes it easy thanks to the **Data API**: you just create a dataset object, tell it where to get the data, then transform it in any way you want, and **TensorFlow** takes care of all the implementation details, such as multithreading, queuing, batching, prefetching, and so on.\n",
        "\n",
        "Off the shelf, the **Data API** can read from text files (such as CSV files), binary files with fixed-size records, and binary files that use **TensorFlow’s TFRecord format**, which supports records of varying sizes. TFRecord is a flexible and efficient binary format based on Protocol Buffers (an open source binary format). \n",
        "\n",
        "The **Data API** also has support for reading from SQL databases. Moreover, many Open Source extensions are available to read from all sorts of data sources, such as **Google’s BigQuery** service.\n",
        "\n",
        "However, reading huge datasets efficiently is not the only difficulty: the data also needs to be preprocessed. Indeed, it is not always composed strictly of convenient numerical fields: sometimes there will be text features, categorical features, and so on.\n",
        "\n",
        "To handle this, TensorFlow provides the **Features API**: it lets you easily convert these\n",
        "features to numerical features that can be consumed by your neural network. \n",
        "\n",
        "For example, categorical features with a large number of categories (such as cities, or words) can be encoded using embeddings (an embedding is a trainable dense vector that represents a category).\n",
        "\n",
        "**Data API** cover the **TFRecord format** and the **Features\n",
        "API** that is related to  these projects.\n",
        "\n",
        "* **TF Transform (tf.Transform)** makes it possible to write a single preprocessing function that can be run both in batch mode on your full training set, before training (to speed it up), and then exported to a TF Function and incorporated into your trained model, so that once it is deployed in production, it can take care of preprocessing new instances on the fly.\n",
        "* **TF Datasets (TFDS)** provides a convenient function to download many common datasets of all kinds, including large ones like **ImageNet**, and it provides convenient dataset objects to manipulate them using the **Data API**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRnXTCg_8nRM"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbcckhe58s5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4fdf25e-73a4-4994-8f9f-0b4c019c0bf8"
      },
      "source": [
        "import sys\n",
        "assert sys.version_info >= (3, 5)  # Python ≥3.5 is required\n",
        "\n",
        "import sklearn \n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "assert sklearn.__version__ >= \"0.20\"  # Scikit-Learn ≥0.20 is required\n",
        "\n",
        "# %tensorflow_version only exists in Colab.\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "  IS_COLAB = True\n",
        "except Exception:\n",
        "  IS_COLAB = False\n",
        "  pass\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= '2.0'\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\n",
            "Go to Runtime > Change runtime and select a GPU hardware accelerator.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkAn-27n9TcX"
      },
      "source": [
        "## The Data API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ddjsd2PS9UfV"
      },
      "source": [
        "The whole **Data API** revolves around the concept of a dataset: as you might suspect, this represents a sequence of data items. Usually you will use datasets that gradually read data from disk, but for simplicity let’s just create a dataset entirely in RAM using tf.data.Dataset.from_tensor_slices():"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXdlDh1P81as",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ece4f8f0-1dd1-4f6c-d51e-6b95c16e3649"
      },
      "source": [
        "X = tf.range(10)  # any data tensor\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "dataset"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: (), types: tf.int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYxStsF7-LeT"
      },
      "source": [
        "The from_tensor_slices() function takes a tensor and creates a tf.data.Dataset whose elements are all the slices of X (along the first dimension), so this dataset contains 10 items: tensors 0, 1, 2, …, 9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftUX4SMx-A7h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62bfe04f-7b3a-414d-8f41-c9dbd7a06aaf"
      },
      "source": [
        "for item in dataset:\n",
        "  print(item)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(3, shape=(), dtype=int32)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "tf.Tensor(5, shape=(), dtype=int32)\n",
            "tf.Tensor(6, shape=(), dtype=int32)\n",
            "tf.Tensor(7, shape=(), dtype=int32)\n",
            "tf.Tensor(8, shape=(), dtype=int32)\n",
            "tf.Tensor(9, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9589xSI-Y5L"
      },
      "source": [
        "Equivalently"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBLrXYbO-Suk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd70e198-98c5-4b84-ee42-b9eafd4ffa34"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "for item in dataset:\n",
        "  print(item)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(3, shape=(), dtype=int64)\n",
            "tf.Tensor(4, shape=(), dtype=int64)\n",
            "tf.Tensor(5, shape=(), dtype=int64)\n",
            "tf.Tensor(6, shape=(), dtype=int64)\n",
            "tf.Tensor(7, shape=(), dtype=int64)\n",
            "tf.Tensor(8, shape=(), dtype=int64)\n",
            "tf.Tensor(9, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf5evy05-wLi"
      },
      "source": [
        "### Chaining Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO-G1Hzt-zaT"
      },
      "source": [
        "Once you have a dataset, you can apply all sorts of transformations to it by calling its transformation methods. Each method returns a new dataset, so you can chain transformations like this.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/chaining-dataset-transformations.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_chc0Ee-huU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c4bf38-ee65-450e-e165-3af478a0d086"
      },
      "source": [
        "dataset = dataset.repeat(3).batch(7)\n",
        "for item in dataset:\n",
        "  print(item)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
            "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
            "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n",
            "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n",
            "tf.Tensor([8 9], shape=(2,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSEHyhg_AArS"
      },
      "source": [
        "We first call the repeat() method on the original dataset, and it returns a new dataset that will repeat the items of the original dataset 3 times. Of course, this will not copy the whole data in memory 3 times! In fact, if you call this method with no arguments, the new dataset will repeat the source dataset forever. Then we call the batch() method on this new dataset, and again this creates a new dataset. This one will group the items of the previous dataset in batches of 7 items.\n",
        "\n",
        "Finally, we iterate over the items of this final dataset. As you can see, the batch() method had to output a final batch of size 2 instead of 7, but you can call it with drop_remainder=True if you want it to drop this final batch so that all batches have the exact same size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOFkgio3_b_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c584ff39-ea70-429e-aad3-efe34c692e55"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset = dataset.repeat(5).batch(9)\n",
        "for item in dataset:\n",
        "  print(item)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 1 2 3 4 5 6 7 8], shape=(9,), dtype=int64)\n",
            "tf.Tensor([9 0 1 2 3 4 5 6 7], shape=(9,), dtype=int64)\n",
            "tf.Tensor([8 9 0 1 2 3 4 5 6], shape=(9,), dtype=int64)\n",
            "tf.Tensor([7 8 9 0 1 2 3 4 5], shape=(9,), dtype=int64)\n",
            "tf.Tensor([6 7 8 9 0 1 2 3 4], shape=(9,), dtype=int64)\n",
            "tf.Tensor([5 6 7 8 9], shape=(5,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPvEIWcR_l-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a289c769-e258-4e9c-f131-d24b674a1d09"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset = dataset.repeat(5).batch(9, drop_remainder=True)  # discard the remainder\n",
        "for item in dataset:\n",
        "  print(item)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 1 2 3 4 5 6 7 8], shape=(9,), dtype=int64)\n",
            "tf.Tensor([9 0 1 2 3 4 5 6 7], shape=(9,), dtype=int64)\n",
            "tf.Tensor([8 9 0 1 2 3 4 5 6], shape=(9,), dtype=int64)\n",
            "tf.Tensor([7 8 9 0 1 2 3 4 5], shape=(9,), dtype=int64)\n",
            "tf.Tensor([6 7 8 9 0 1 2 3 4], shape=(9,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdo6O7HrBttP"
      },
      "source": [
        "You can also apply any transformation you want to the items by calling the map() method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDL2p_E9A11n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b872962-9f3c-4a65-fe18-2f4657f3e909"
      },
      "source": [
        "dataset = dataset.map(lambda x: x * 2)\n",
        "for item in dataset:\n",
        "  print(item)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([ 0  2  4  6  8 10 12 14 16], shape=(9,), dtype=int64)\n",
            "tf.Tensor([18  0  2  4  6  8 10 12 14], shape=(9,), dtype=int64)\n",
            "tf.Tensor([16 18  0  2  4  6  8 10 12], shape=(9,), dtype=int64)\n",
            "tf.Tensor([14 16 18  0  2  4  6  8 10], shape=(9,), dtype=int64)\n",
            "tf.Tensor([12 14 16 18  0  2  4  6  8], shape=(9,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x3nEvdwCKYD"
      },
      "source": [
        "This function is the one you will call to apply any preprocessing you want to your data. Sometimes, this will include computations that can be quite intensive, such as reshaping or rotating an image, so you will usually want to spawn multiple threads to speed things up: it’s as simple as setting the num_parallel_calls argument.\n",
        "\n",
        "While the map() applies a transformation to each item, the apply() method applies a transformation to the dataset as a whole.\n",
        "\n",
        "For example, the following code “unbatches” the dataset, by applying the unbatch() function to the dataset.Each item in the new dataset will be a single integer tensor instead of a batch of 7 integers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSS5cp1NB783",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a6bd36d-4595-496e-b84c-acdc637c39ca"
      },
      "source": [
        "dataset1 = tf.data.Dataset.range(10)\n",
        "dataset1 = dataset1.repeat(5).batch(9, drop_remainder=True)  # discard the remainder\n",
        "dataset1 = dataset1.apply(tf.data.experimental.unbatch())\n",
        "for item in dataset1:\n",
        "  print(item)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-19387b78fa5e>:3: unbatch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.unbatch()`.\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(3, shape=(), dtype=int64)\n",
            "tf.Tensor(4, shape=(), dtype=int64)\n",
            "tf.Tensor(5, shape=(), dtype=int64)\n",
            "tf.Tensor(6, shape=(), dtype=int64)\n",
            "tf.Tensor(7, shape=(), dtype=int64)\n",
            "tf.Tensor(8, shape=(), dtype=int64)\n",
            "tf.Tensor(9, shape=(), dtype=int64)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(3, shape=(), dtype=int64)\n",
            "tf.Tensor(4, shape=(), dtype=int64)\n",
            "tf.Tensor(5, shape=(), dtype=int64)\n",
            "tf.Tensor(6, shape=(), dtype=int64)\n",
            "tf.Tensor(7, shape=(), dtype=int64)\n",
            "tf.Tensor(8, shape=(), dtype=int64)\n",
            "tf.Tensor(9, shape=(), dtype=int64)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(3, shape=(), dtype=int64)\n",
            "tf.Tensor(4, shape=(), dtype=int64)\n",
            "tf.Tensor(5, shape=(), dtype=int64)\n",
            "tf.Tensor(6, shape=(), dtype=int64)\n",
            "tf.Tensor(7, shape=(), dtype=int64)\n",
            "tf.Tensor(8, shape=(), dtype=int64)\n",
            "tf.Tensor(9, shape=(), dtype=int64)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(3, shape=(), dtype=int64)\n",
            "tf.Tensor(4, shape=(), dtype=int64)\n",
            "tf.Tensor(5, shape=(), dtype=int64)\n",
            "tf.Tensor(6, shape=(), dtype=int64)\n",
            "tf.Tensor(7, shape=(), dtype=int64)\n",
            "tf.Tensor(8, shape=(), dtype=int64)\n",
            "tf.Tensor(9, shape=(), dtype=int64)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(3, shape=(), dtype=int64)\n",
            "tf.Tensor(4, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfoLVL__D6eQ"
      },
      "source": [
        "It is also possible to simply filter the dataset using the filter() method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK5_GaJID7VQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bfcb0d7-527a-458a-ae13-81e3546cfa70"
      },
      "source": [
        "dataset1 = dataset1.filter(lambda x: x < 5)\n",
        "for item in dataset1:\n",
        "  print(item)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(3, shape=(), dtype=int64)\n",
            "tf.Tensor(4, shape=(), dtype=int64)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(3, shape=(), dtype=int64)\n",
            "tf.Tensor(4, shape=(), dtype=int64)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(3, shape=(), dtype=int64)\n",
            "tf.Tensor(4, shape=(), dtype=int64)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(3, shape=(), dtype=int64)\n",
            "tf.Tensor(4, shape=(), dtype=int64)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(3, shape=(), dtype=int64)\n",
            "tf.Tensor(4, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poeMGLINEfI8"
      },
      "source": [
        "You will often want to look at just a few items from a dataset. You can use the take() method for that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCYKZLAqCrPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57ed2501-66a4-4adc-d071-a6a31093f100"
      },
      "source": [
        "for item in dataset1.take(3):\n",
        "  print(item)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7schtlrd8lAn",
        "outputId": "87a62372-c26e-46cb-b96f-3226f75011d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for item in dataset1.take(5):\r\n",
        "  print(item)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(3, shape=(), dtype=int64)\n",
            "tf.Tensor(4, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmX4ZKD5EtS9"
      },
      "source": [
        "### Shuffling the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5YVRd26EuTL"
      },
      "source": [
        "As you know, Gradient Descent works best when the instances in the training set are independent and identically distributed.A simple way to ensure this\r\n",
        "is to shuffle the instances, using the shuffle() method. It will create a new dataset that will start by filling up a buffer with the first items of the source dataset. Then, whenever it is asked for an item, it will pull one out randomly from the buffer and replace it with a fresh one from the source dataset, until it has iterated entirely through the source dataset.\r\n",
        "\r\n",
        "At this point it continues to pull out items randomly from the buffer until it is empty. You must specify the buffer size, and it is important to\r\n",
        "make it large enough, or else shuffling will not be very effective.\r\n",
        "\r\n",
        "For example, the following code creates and displays a dataset containing the integers 0 to 9, repeated 3 times, shuffled using a buffer of size 5 and a random seed of 42, and batched with a batch size of 7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNwbGz3sEmsy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a4b2b2f-dcd7-463e-b43f-6d83490e3bee"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10).repeat(3)\r\n",
        "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\r\n",
        "for item in dataset:\r\n",
        "  print(item)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
            "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
            "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
            "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
            "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMr5C3G9-dcl"
      },
      "source": [
        "> If you call repeat() on a shuffled dataset, by default it will generate\r\n",
        "a new order at every iteration. This is generally a good idea, but if\r\n",
        "you prefer to reuse the same order at each iteration (e.g., for tests\r\n",
        "or debugging), you can set reshuffle_each_iteration=False.\r\n",
        "\r\n",
        "For a large dataset that does not fit in memory, this simple shuffling-buffer approach may not be sufficient, since the buffer will be small compared to the dataset. One solution is to shuffle the source data itself.\r\n",
        "\r\n",
        "To shuffle the instances some more, a common approach is to split the source data into multiple files, then read them in a random order during training. However, instances located in the same file will still end up close to each other. To avoid this you can pick multiple files randomly and read them simultaneously, interleaving their records. Then on top of that you can add a shuffling buffer using the shuffle() method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x37Hgdkm_n-K"
      },
      "source": [
        "#### Interleaving lines from multiple files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlDeQVjO_pQ8"
      },
      "source": [
        "First, let’s suppose that you’ve loaded the California housing dataset, shuffled it (unless it was already shuffled), and split it into a training set, a validation set, and a test set. Then you split each set into many CSV files that each look like this (each row contains eight input features plus the target median house value):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8tG4Uhx92Tu",
        "outputId": "1985afd2-4356-4260-94f7-38c7b09be9fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "housing = fetch_california_housing()\r\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target.reshape(-1, 1), random_state=42)\r\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\r\n",
        "\r\n",
        "scaler = StandardScaler()\r\n",
        "scaler.fit(X_train)\r\n",
        "X_mean = scaler.mean_\r\n",
        "X_std = scaler.scale_"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l907ATyAjqi"
      },
      "source": [
        "For a very large dataset that does not fit in memory, you will typically want to split it into many files first, then have TensorFlow read these files in parallel. \r\n",
        "\r\n",
        "To demonstrate this, let's start by splitting the housing dataset and save it to 20 CSV files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6p1b1pEAgr1"
      },
      "source": [
        "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\r\n",
        "  housing_dir = os.path.join(\"datasets\", \"housing\")\r\n",
        "  os.makedirs(housing_dir, exist_ok=True)\r\n",
        "  path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\r\n",
        "\r\n",
        "  filepaths = []\r\n",
        "  m = len(data)\r\n",
        "  for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\r\n",
        "    part_csv = path_format.format(name_prefix, file_idx)\r\n",
        "    filepaths.append(part_csv)\r\n",
        "    with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\r\n",
        "      if header is not None:\r\n",
        "        f.write(header)\r\n",
        "        f.write(\"\\n\")\r\n",
        "      for row_idx in row_indices:\r\n",
        "        f.write(\",\".join([repr(col) for col in data[row_idx]]))\r\n",
        "        f.write(\"\\n\")\r\n",
        "  return filepaths"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccK5OWz9EAiN"
      },
      "source": [
        "train_data = np.c_[X_train, y_train]\r\n",
        "valid_data = np.c_[X_valid, y_valid]\r\n",
        "test_data = np.c_[X_test, y_test]\r\n",
        "\r\n",
        "header_cols = housing.feature_names + [\"MedianHouseValue\"]\r\n",
        "header = \",\".join(header_cols)\r\n",
        "\r\n",
        "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\r\n",
        "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\r\n",
        "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up-ILqlyE7eM"
      },
      "source": [
        "Okay, now let's take a peek at the first few lines of one of these CSV files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv-wk4pXE3Nz",
        "outputId": "8d3c4650-628f-4e61-e0d6-2a273179783d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "pd.read_csv(train_filepaths[0]).head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MedInc</th>\n",
              "      <th>HouseAge</th>\n",
              "      <th>AveRooms</th>\n",
              "      <th>AveBedrms</th>\n",
              "      <th>Population</th>\n",
              "      <th>AveOccup</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>MedianHouseValue</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.5214</td>\n",
              "      <td>15.0</td>\n",
              "      <td>3.049945</td>\n",
              "      <td>1.106548</td>\n",
              "      <td>1447.0</td>\n",
              "      <td>1.605993</td>\n",
              "      <td>37.63</td>\n",
              "      <td>-122.43</td>\n",
              "      <td>1.442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.3275</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.490060</td>\n",
              "      <td>0.991054</td>\n",
              "      <td>3464.0</td>\n",
              "      <td>3.443340</td>\n",
              "      <td>33.69</td>\n",
              "      <td>-117.39</td>\n",
              "      <td>1.687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.1000</td>\n",
              "      <td>29.0</td>\n",
              "      <td>7.542373</td>\n",
              "      <td>1.591525</td>\n",
              "      <td>1328.0</td>\n",
              "      <td>2.250847</td>\n",
              "      <td>38.44</td>\n",
              "      <td>-122.98</td>\n",
              "      <td>1.621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.1736</td>\n",
              "      <td>12.0</td>\n",
              "      <td>6.289003</td>\n",
              "      <td>0.997442</td>\n",
              "      <td>1054.0</td>\n",
              "      <td>2.695652</td>\n",
              "      <td>33.55</td>\n",
              "      <td>-117.70</td>\n",
              "      <td>2.621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0549</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.312457</td>\n",
              "      <td>1.085092</td>\n",
              "      <td>3297.0</td>\n",
              "      <td>2.244384</td>\n",
              "      <td>33.93</td>\n",
              "      <td>-116.93</td>\n",
              "      <td>0.956</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   MedInc  HouseAge  AveRooms  ...  Latitude  Longitude  MedianHouseValue\n",
              "0  3.5214      15.0  3.049945  ...     37.63    -122.43             1.442\n",
              "1  5.3275       5.0  6.490060  ...     33.69    -117.39             1.687\n",
              "2  3.1000      29.0  7.542373  ...     38.44    -122.98             1.621\n",
              "3  7.1736      12.0  6.289003  ...     33.55    -117.70             2.621\n",
              "4  2.0549      13.0  5.312457  ...     33.93    -116.93             0.956\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z92sqyDwFNyi"
      },
      "source": [
        "Or in text mode:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "938INEvXE-k2",
        "outputId": "cca6d581-cf19-47f8-d218-dafd786495b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "with open(train_filepaths[0]) as f:\r\n",
        "  for i in range(5):\r\n",
        "    print(f.readline(), end=\"\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
            "3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n",
            "5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n",
            "3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n",
            "7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDHmu7QQFdeK",
        "outputId": "29680cd4-e552-421d-fb1d-ae13e2d0e86d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_filepaths"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['datasets/housing/my_train_00.csv',\n",
              " 'datasets/housing/my_train_01.csv',\n",
              " 'datasets/housing/my_train_02.csv',\n",
              " 'datasets/housing/my_train_03.csv',\n",
              " 'datasets/housing/my_train_04.csv',\n",
              " 'datasets/housing/my_train_05.csv',\n",
              " 'datasets/housing/my_train_06.csv',\n",
              " 'datasets/housing/my_train_07.csv',\n",
              " 'datasets/housing/my_train_08.csv',\n",
              " 'datasets/housing/my_train_09.csv',\n",
              " 'datasets/housing/my_train_10.csv',\n",
              " 'datasets/housing/my_train_11.csv',\n",
              " 'datasets/housing/my_train_12.csv',\n",
              " 'datasets/housing/my_train_13.csv',\n",
              " 'datasets/housing/my_train_14.csv',\n",
              " 'datasets/housing/my_train_15.csv',\n",
              " 'datasets/housing/my_train_16.csv',\n",
              " 'datasets/housing/my_train_17.csv',\n",
              " 'datasets/housing/my_train_18.csv',\n",
              " 'datasets/housing/my_train_19.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jZfbhvIFvyU"
      },
      "source": [
        "#### Building an Input Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK6PbzfKFw0O"
      },
      "source": [
        "By default, the list_files() function returns a dataset that shuffles the file paths. In general this is a good thing, but you can set shuffle=False if you do not want that for some reason."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ukEXKFdFiIp"
      },
      "source": [
        "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM6L9XEgGU1t",
        "outputId": "83fa8986-07d4-4509-ce85-1b5747d6d3b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for file_path in filepath_dataset:\r\n",
        "  print(file_path)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'datasets/housing/my_train_05.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_16.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_01.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_17.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_00.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_14.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_10.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_02.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_12.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_19.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_07.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_09.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_13.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_15.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_11.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_18.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_04.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_06.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_03.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_08.csv', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH6E5_AAGyXa"
      },
      "source": [
        "Next, you can call the interleave() method to read from five files at a time and\r\n",
        "interleave their lines (skipping the first line of each file, which is the header row, using the skip() method):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oWcVKb1GaEn"
      },
      "source": [
        "n_readers = 5\r\n",
        "dataset = filepath_dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbW4Iak0HhGi"
      },
      "source": [
        "The interleave() method will create a dataset that will pull five file paths from the filepath_dataset, and for each one it will call the function you gave it to create a new dataset (in this case a TextLineDataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r9OIUzzHMyS",
        "outputId": "5ebd1a10-71ec-408b-aa7d-9693787698c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for line in dataset.take(5):\r\n",
        "  print(line.numpy())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'4.5909,16.0,5.475877192982456,1.0964912280701755,1357.0,2.9758771929824563,33.63,-117.71,2.418'\n",
            "b'2.4792,24.0,3.4547038327526134,1.1341463414634145,2251.0,3.921602787456446,34.18,-118.38,2.0'\n",
            "b'4.2708,45.0,5.121387283236994,0.953757225433526,492.0,2.8439306358381504,37.48,-122.19,2.67'\n",
            "b'2.1856,41.0,3.7189873417721517,1.0658227848101265,803.0,2.0329113924050635,32.76,-117.12,1.205'\n",
            "b'4.1812,52.0,5.701388888888889,0.9965277777777778,692.0,2.4027777777777777,33.73,-118.31,3.215'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKn2pt55H8vH"
      },
      "source": [
        "By default, interleave() does not use parallelism; it just reads one line at a time from each file, sequentially. If you want it to actually read files in parallel, you can set the num_parallel_calls argument to the number of threads you want (note that the map() method also has this argument). You can even set it to tf.data.experimental.AUTOTUNE to make TensorFlow choose the right number of threads dynamically based on the available CPU (however, this is an experimental feature for now).\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VjP6dA5IQ1e"
      },
      "source": [
        "### Preprocessing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jozLP__kIUKM"
      },
      "source": [
        ""
      ]
    }
  ]
}