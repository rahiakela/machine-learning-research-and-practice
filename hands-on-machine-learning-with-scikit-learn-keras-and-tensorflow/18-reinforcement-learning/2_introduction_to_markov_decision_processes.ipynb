{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-introduction-to-markov-decision-processes.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNIGqMnAlZ4SguZVzi4TnFT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow/blob/18-reinforcement-learning/2_introduction_to_markov_decision_processes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFgQ8q2ao6gf",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Markov Decision Processes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHP2IpiMo7V7",
        "colab_type": "text"
      },
      "source": [
        "In the early 20th century, **the mathematician Andrey Markov studied stochastic processes with no memory, called Markov chains. Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from a state s to a state s′ is fixed, and it depends only on the pair (s, s′), not on past states (this is why we say that the system has no memory)**.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/markov-chain.png?raw=1' width='800'/>\n",
        "\n",
        "Suppose that the process starts in state $s_0$, and there is a 70% chance that it will remain in that state at the next step. Eventually it is bound to leave that state and never come back because no other state points back to $s_0$. If it goes to state $s_1$, it will then most likely go to state $s_2$ (90% probability), then immediately back to state $s_1$ (with 100% probability). It may alternate a number of times between these two states, but eventually it will fall into state $s_3$ and remain there forever (this is a terminal\n",
        "state). Markov chains can have very different dynamics, and they are heavily used in thermodynamics, chemistry, statistics, and much more.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_PLJy4GsaXM",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r1JJGSCsbg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWCrAC2D_H-z",
        "colab_type": "text"
      },
      "source": [
        "## Markov Chains"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVpHkbCUBNTf",
        "colab_type": "code",
        "outputId": "f0180a21-b5f7-4bce-c14e-4e3ccfe1015d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# shape=[s, s']\n",
        "transition_probabilities = [\n",
        "    [0.7, 0.2, 0.0, 0.1],  # from s0 to s0, s1, s2, s3\n",
        "    [0.0, 0.0, 0.9, 0.1],  # from s1 to ...\n",
        "    [0.0, 1.0, 0.0, 0.0],  # from s2 to ...\n",
        "    [0.0, 0.0, 0.0, 1.0]   # from s3 to ...                       \n",
        "]\n",
        "\n",
        "n_max_steps = 50\n",
        "\n",
        "def print_sequence():\n",
        "  current_state = 0\n",
        "  print('States:', end=' ')\n",
        "  for step in range(n_max_steps):\n",
        "    print(current_state, end=' ')\n",
        "    if current_state == 3:\n",
        "      break\n",
        "    current_state = np.random.choice(range(4), p=transition_probabilities[current_state])\n",
        "  else:\n",
        "    print('...', end='')\n",
        "  print()\n",
        "\n",
        "for _ in range(10):\n",
        "  print_sequence()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "States: 0 0 3 \n",
            "States: 0 1 2 1 2 1 2 1 2 1 3 \n",
            "States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
            "States: 0 3 \n",
            "States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
            "States: 0 1 3 \n",
            "States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 ...\n",
            "States: 0 0 3 \n",
            "States: 0 0 0 1 2 1 2 1 3 \n",
            "States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ-nBGsVFOUT",
        "colab_type": "text"
      },
      "source": [
        "## Markov Decision Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLy-Elk2FPMy",
        "colab_type": "text"
      },
      "source": [
        "Markov decision processes were first [described in the 1950s by Richard Bellman](https://apps.dtic.mil/dtic/tr/fulltext/u2/606367.pdf).\n",
        "**They resemble Markov chains but with a twist: at each step, an agent can choose one of several possible actions, and the transition probabilities depend on the chosen action.** Moreover, some state transitions return some reward (positive or negative), and the agent’s goal is to find a policy that will maximize reward over time.\n",
        "\n",
        "For example, the MDP has three states (represented by circles) and up to three possible discrete actions at each step (represented by diamonds).\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/markov-decision-process.png?raw=1' width='800'/>\n",
        "\n",
        "If it starts in state $s_0$, the agent can choose between actions $a_0, a_1$, or $a_2$. If it chooses action $a_1$, it just remains in state $s_0$ with certainty, and without any reward. It can thus decide to stay there forever if it wants to. But if it chooses action $a_0$, it has a 70% probability\n",
        "of gaining a reward of +10 and remaining in state $s_0$. \n",
        "\n",
        "It can then try again and again to gain as much reward as possible, but at one point it is going to end up instead in state $s_1$. In state $s_1$ it has only two possible actions: $a_0$ or $a_2$. It can choose to stay put by repeatedly choosing action $a_0$, or it can choose to move on to state $s_2$ and get a negative reward of –50 (ouch). In state $s_2$ it has no other choice than to take action $a_1$, which will most likely lead it back to state $s_0$, gaining a reward of +40 on the way. \n",
        "\n",
        "You get the picture. By looking at this MDP, can you guess which strategy will\n",
        "gain the most reward over time? In state $s_0$ it is clear that action $a_0$ is the best option, and in state $s_2$ the agent has no choice but to take action $a_1$, but in state $s_1$ it is not obvious whether the agent should stay put ($a_0$) or go through the fire ($a_2$).\n",
        "\n",
        "Bellman found a way to estimate the optimal state value of any state $s$, noted $V*(s)$, which is the sum of all discounted future rewards the agent can expect on average after it reaches a state s, assuming it acts optimally.\n",
        "\n",
        "He showed that if the agent acts optimally, then the Bellman Optimality Equation applies.This recursive equation says that if the agent acts optimally, then the optimal value of the current state is equal to the reward it will get on average after taking one optimal action, plus the expected optimal value of all possible next states that this action can lead to.\n",
        "\n",
        "$$V^*(s) = max_a Σ_sT(s,a,s′)[R(s,a,s′) + γ · V*(s′)]$$\n",
        "\n",
        "In this equation:\n",
        "\n",
        "* $T(s, a, s′)$ is the transition probability from state $s$ to state $s′$, given that the agent chose action $a$. For example,$T(s_2, a_1, s_0) = 0.8$.\n",
        "\n",
        "* $R(s, a, s′)$ is the reward that the agent gets when it goes from state $s$ to state $s′$, given that the agent chose action $a$. For example, $R(s2, a1,\n",
        "s0) = +40.$\n",
        "\n",
        "* $γ$ is the discount factor.\n",
        "\n",
        "This equation leads directly to an algorithm that can precisely estimate the optimal state value of every possible state: **you first initialize all the state value estimates to zero, and then you iteratively update them using the Value Iteration algorithm.**A remarkable result is that, given enough time, these estimates are guaranteed to converge to the optimal state values, corresponding to the optimal policy.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/value-iteration-algorithm.png?raw=1' width='800'/>\n",
        "\n",
        "In this equation, $V_k(s)$ is the estimated value of state $s$ at the $k^{th}$ iteration of the algorithm.\n",
        "\n",
        "> This algorithm is an example of Dynamic Programming, which\n",
        "breaks down a complex problem into tractable subproblems that\n",
        "can be tackled iteratively.\n",
        "\n",
        "Knowing the optimal state values can be useful, in particular to evaluate a policy, but it does not give us the optimal policy for the agent. Luckily, Bellman found a very similar algorithm to estimate the optimal state-action values, generally called QValues (Quality Values). The optimal Q-Value of the state-action pair $(s, a)$, noted $Q*(s, a)$, is the sum of discounted future rewards the agent can expect on average after it reaches the state s and chooses action $a$, but before it sees the outcome of this action, assuming it acts optimally after that action.\n",
        "\n",
        "Here is how it works: **once again, you start by initializing all the Q-Value estimates to zero, then you update them using the Q-Value Iteration algorithm.**\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/q-value-iteration-algorithm.png?raw=1' width='800'/>\n",
        "\n",
        "Once you have the optimal Q-Values, defining the optimal policy, noted $π*(s)$, is trivial: when the agent is in state $s$, it should choose the action with the highest Q-Value for that state: $π^*(s) = argmax Q^*(s, a)$.\n",
        "\n",
        "Let’s apply this algorithm to the MDP. First, we need to\n",
        "define the MDP:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXgXt3YME9_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shape=[s, a, s']\n",
        "transition_probabilities = [\n",
        "   [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
        "   [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
        "   [None, [0.8, 0.1, 0.1], None]                         \n",
        "]\n",
        "\n",
        "# shape=[s, a, s']\n",
        "rewards = [\n",
        "   [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
        "   [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
        "   [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]        \n",
        "]\n",
        "\n",
        "possible_actions = [\n",
        "    [0, 1, 2], [0, 2], [1]                \n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTEddQe5I_Eh",
        "colab_type": "text"
      },
      "source": [
        "For example, to know the transition probability from $s_2$ to $s_0$ after playing action a1, we will look up transition_probabilities[2][1][0] (which is 0.8). Similarly, to get the corresponding reward, we will look up rewards[2][1][0] (which is +40). And to get the list of possible actions in $s_2$, we will look up possible_actions[2] (in this case, only action $a_1$ is possible).\n",
        "\n",
        "Next, we must initialize all the Q-Values to 0 (except for the the impossible actions, for which we set the Q-Values to –∞):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gw-rT9mI-jn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Q_values = np.full((3, 3), -np.inf)  # -np.inf for impossible actions\n",
        "for state, actions in enumerate(possible_actions):\n",
        "  Q_values[state, actions] = 0.0    # for all possible actions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD63YPQjJ476",
        "colab_type": "text"
      },
      "source": [
        "Now let’s run the Q-Value Iteration algorithm. It applies repeatedly, to all Q-Values, for every state and every possible action:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymHTvDwNJ4Xo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the discount factor\n",
        "gamma = 0.90\n",
        "\n",
        "history1 = []\n",
        "for iteration in range(50):\n",
        "  Q_prev = Q_values.copy()\n",
        "  history1.append(Q_prev)\n",
        "  for s in range(3):\n",
        "    for a in possible_actions[s]:\n",
        "      Q_values[s, a] = np.sum([transition_probabilities[s][a][sp] * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp])) for sp in range(3)])\n",
        "history1 = np.array(history1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M84tDzeILIma",
        "colab_type": "text"
      },
      "source": [
        "That’s it! The resulting Q-Values look like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RaJZp0NLJM6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "89bc2416-00b6-481a-99c7-5d6a2d0b3e99"
      },
      "source": [
        "Q_values"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[18.91891892, 17.02702702, 13.62162162],\n",
              "       [ 0.        ,        -inf, -4.87971488],\n",
              "       [       -inf, 50.13365013,        -inf]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIttisCjLR6o",
        "colab_type": "text"
      },
      "source": [
        "For example, when the agent is in state $s_0$ and it chooses action $a_1$, the expected sum of discounted future rewards is approximately 17.0.\n",
        "\n",
        "For each state, let’s look at the action that has the highest Q-Value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cutCi6FSLLge",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "33ceb7ae-d1db-442d-fa34-6520f10ecf78"
      },
      "source": [
        "np.argmax(Q_values, axis=1)  # optimal action for each state"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7N2nMDBLv1s",
        "colab_type": "text"
      },
      "source": [
        "This gives us the optimal policy for this MDP, when using a discount factor of 0.90: in state $s_0$ choose action $a_0$; in state $s_1$ choose action $a_0$ (i.e., stay put); and in state $s_2$ choose action $a_1$ (the only possible action).\n",
        "\n",
        "Interestingly, if we increase the discount factor to 0.95, the optimal policy changes: in state $s_1$ the best action becomes $a_2$ (go through the fire!). This makes sense because the more you value future rewards, the more you are willing to put up with some pain now for the promise of future bliss.\n",
        "\n",
        "Let's try again with a discount factor of 0.95:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQniN29-LhlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Q_values = np.full((3, 3), -np.inf)  # -np.inf for impossible actions\n",
        "for state, actions in enumerate(possible_actions):\n",
        "  Q_values[state, actions] = 0.0    # for all possible actions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukjob6qmMi4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the discount factor\n",
        "gamma = 0.95\n",
        "\n",
        "for iteration in range(50):\n",
        "  Q_prev = Q_values.copy()\n",
        "  for s in range(3):\n",
        "    for a in possible_actions[s]:\n",
        "      Q_values[s, a] = np.sum([transition_probabilities[s][a][sp] * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp])) for sp in range(3)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qLPo6oVMsyP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0fad4f9a-e0f2-491d-b89c-fda7afb64b31"
      },
      "source": [
        "Q_values"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[21.73304188, 20.63807938, 16.70138772],\n",
              "       [ 0.95462106,        -inf,  1.01361207],\n",
              "       [       -inf, 53.70728682,        -inf]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OU1Nb0wMy-h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9507f4b3-d035-4751-eae2-d2e107cd55cb"
      },
      "source": [
        "np.argmax(Q_values, axis=1)  # optimal action for each state"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzAXY4PpM3yZ",
        "colab_type": "text"
      },
      "source": [
        "Now the policy has changed! In state $s_1$, we now prefer to go through the fire (choose action $a_2$). This is because the discount factor is larger so the agent values the future more, and it is therefore ready to pay an immediate penalty in order to get more future rewards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2D4bJvfMRyo",
        "colab_type": "text"
      },
      "source": [
        "## Temporal Difference Learning"
      ]
    }
  ]
}