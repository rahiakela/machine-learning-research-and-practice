{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM4AQLX60tAmURQ2q1xQWnb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/machine-learning-research-and-practice/blob/main/hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow/16-NLP-with-RNNs-and-Attention/v2_generating_text_using_character_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHUB8psfrgPJ"
      },
      "source": [
        "## Generating Text using Character RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3nxCza6rwcN"
      },
      "source": [
        "A common approach for natural language tasks is to use recurrent neural networks.\n",
        "We will therefore continue to explore RNNs, starting with\n",
        "a character RNN, trained to predict the next character in a sentence. This will allow us to generate some original text, and in the process we will see how to build a TensorFlow Dataset on a very long sequence. \n",
        "\n",
        "We will first use a stateless RNN (which learns on random portions of text at each iteration, without any information on the rest of the text), then we will build a stateful RNN (which preserves the hidden state between training iterations and continues reading where it left off, allowing it to learn longer patterns).\n",
        "\n",
        "Let’s start with a simple and fun model that can write like Shakespeare (well, sort of)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLNaLqOquKI4"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wC8SV2kuM11"
      },
      "source": [
        "import sys\n",
        "assert sys.version_info >= (3, 7)  # Python ≥3.5 is required\n",
        "\n",
        "import sklearn \n",
        "assert sklearn.__version__ >= \"0.20\"  # Scikit-Learn ≥0.20 is required\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= '2.0'\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3Rly-8iuRJL"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download the Shakespeare data from Andrej Karpathy's char-rnn project."
      ],
      "metadata": {
        "id": "Ze9tNuE2lTdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_url = \"https://homl.info/shakespeare\"\n",
        "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "  shakespeare_text = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_WqqSZelUFW",
        "outputId": "db3b7aec-589a-4a87-ee50-80354e2c451b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://homl.info/shakespeare\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code – shows a short text sample\n",
        "print(shakespeare_text[:80])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T7KPUtilsRo",
        "outputId": "cbf6b200-e2a0-46ae-f46c-1cd51559e2d7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code - shows all 39 distinct characters (after converting to lower case)\n",
        "\"\".join(sorted(set(shakespeare_text.lower())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9uveR0YmUFwh",
        "outputId": "0308df3a-93c4-4cbe-c262-6c8eebd6351c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's encode the text."
      ],
      "metadata": {
        "id": "y6oicn93UbdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\", standardize=\"lower\")\n",
        "text_vec_layer.adapt([shakespeare_text])\n",
        "encoded = text_vec_layer([shakespeare_text])[0]"
      ],
      "metadata": {
        "id": "dxqbhUeAUfWK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s subtract 2 from the\n",
        "character IDs and compute the number of distinct characters and the total number of\n",
        "characters."
      ],
      "metadata": {
        "id": "ze5g7d4TVcmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded -= 2                                     # drop tokens 0 (pad) and 1 (unknown), which we will not use\n",
        "n_tokens = text_vec_layer.vocabulary_size() - 2  # number of distinct chars = 39\n",
        "dataset_size = len(encoded)                      # total number of chars = 1,115,394\n",
        "\n",
        "print(n_tokens)\n",
        "print(dataset_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP85WH2wVS4z",
        "outputId": "136b520d-efa6-4db5-ea04-63f3a693c92c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39\n",
            "1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s convert a long sequence of character IDs into a dataset of input/target window pairs."
      ],
      "metadata": {
        "id": "3NTbAiT3XWiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
        "  ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "  ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
        "  ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(100_000, seed=seed)\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
      ],
      "metadata": {
        "id": "PGHTiZrvXZmv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code – a simple example using to_dataset()\n",
        "# There's just one sample in this dataset: the input represents \"to b\" and the output represents \"o be\"\n",
        "list(to_dataset(text_vec_layer([\"To be\"])[0], length=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GasjhdJaZgDe",
        "outputId": "157df028-12cb-42a8-8038-94fef75b8e88"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[ 4,  5,  2, 23]])>,\n",
              "  <tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[ 5,  2, 23,  3]])>)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(to_dataset(text_vec_layer([\"To be or not to be\"])[0], length=8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dnFoBZ6Z8_8",
        "outputId": "4077910e-f1aa-4fa9-8c1c-d02471de2ae3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(<tf.Tensor: shape=(10, 8), dtype=int64, numpy=\n",
              "  array([[ 4,  5,  2, 23,  3,  2,  5, 10],\n",
              "         [ 5,  2, 23,  3,  2,  5, 10,  2],\n",
              "         [ 2, 23,  3,  2,  5, 10,  2, 11],\n",
              "         [23,  3,  2,  5, 10,  2, 11,  5],\n",
              "         [ 3,  2,  5, 10,  2, 11,  5,  4],\n",
              "         [ 2,  5, 10,  2, 11,  5,  4,  2],\n",
              "         [ 5, 10,  2, 11,  5,  4,  2,  4],\n",
              "         [10,  2, 11,  5,  4,  2,  4,  5],\n",
              "         [ 2, 11,  5,  4,  2,  4,  5,  2],\n",
              "         [11,  5,  4,  2,  4,  5,  2, 23]])>,\n",
              "  <tf.Tensor: shape=(10, 8), dtype=int64, numpy=\n",
              "  array([[ 5,  2, 23,  3,  2,  5, 10,  2],\n",
              "         [ 2, 23,  3,  2,  5, 10,  2, 11],\n",
              "         [23,  3,  2,  5, 10,  2, 11,  5],\n",
              "         [ 3,  2,  5, 10,  2, 11,  5,  4],\n",
              "         [ 2,  5, 10,  2, 11,  5,  4,  2],\n",
              "         [ 5, 10,  2, 11,  5,  4,  2,  4],\n",
              "         [10,  2, 11,  5,  4,  2,  4,  5],\n",
              "         [ 2, 11,  5,  4,  2,  4,  5,  2],\n",
              "         [11,  5,  4,  2,  4,  5,  2, 23],\n",
              "         [ 5,  4,  2,  4,  5,  2, 23,  3]])>)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we’re ready to create the training set, the validation set, and the test set."
      ],
      "metadata": {
        "id": "8sPvSjJwZV4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "length = 100\n",
        "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True, seed=42)\n",
        "valid_set = to_dataset(encoded[1_000_000: 1_060_000], length=length)\n",
        "test_set = to_dataset(encoded[1_060_000:], length=length)"
      ],
      "metadata": {
        "id": "nILzO6EMZYbb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuOZsaWcNHXK"
      },
      "source": [
        "## Building and Training Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3VfK0wp-LuD"
      },
      "source": [
        "Let’s build and train\n",
        "a model with one GRU layer composed of 128 units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5_bM7ns-Ef1",
        "outputId": "f0f47767-cfc2-47f6-d254-53a61234c219",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.Dense(n_tokens, activation='softmax')                            \n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 16)          624       \n",
            "                                                                 \n",
            " gru (GRU)                   (None, None, 128)         56064     \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 39)          5031      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61,719\n",
            "Trainable params: 61,719\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NofAusbtDkBI"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=[\"accuracy\"])\n",
        "\n",
        "model_ckpt = keras.callbacks.ModelCheckpoint(\"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True)\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=10, callbacks=[model_ckpt])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQYwTAGmOtL1"
      },
      "source": [
        "Let's handle text preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_model = keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    keras.layers.Lambda(lambda x: x - 2),  # no <PAD> or <UNK> tokens\n",
        "    model\n",
        "])"
      ],
      "metadata": {
        "id": "XwLd181kIQ4B"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now let’s use it to predict the next character in a sentence."
      ],
      "metadata": {
        "id": "acbjr3jIIrO4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0i58Gcz8c2m",
        "outputId": "f89f06b4-c8c4-4ba6-ea72-9d5e0ba6b027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "y_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1]\n",
        "# choose the most probable character ID\n",
        "y_pred = tf.argmax(y_proba)\n",
        "text_vec_layer.get_vocabulary()[y_pred + 2]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 407ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'e'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba = shakespeare_model.predict([\"I love yo\"])[0, -1]\n",
        "# choose the most probable character ID\n",
        "y_pred = tf.argmax(y_proba)\n",
        "text_vec_layer.get_vocabulary()[y_pred + 2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "F4VcDW-8Zav9",
        "outputId": "04094579-0b8a-4bf6-b238-44a9033e5380"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 33ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'u'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba = shakespeare_model.predict([\"Where are you goin\"])[0, -1]\n",
        "# choose the most probable character ID\n",
        "y_pred = tf.argmax(y_proba)\n",
        "text_vec_layer.get_vocabulary()[y_pred + 2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "kl-YgVJoZmqN",
        "outputId": "cc16cf86-eb0e-4c63-d712-82cf7cebca57"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 28ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'g'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gnXqqo68-GA"
      },
      "source": [
        "## Generating Fake Shakespearean Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhqjOaXI9gRy"
      },
      "source": [
        "Let's generate more diverse and interesting text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4diFK4K86F2",
        "outputId": "bc7d8839-ea2c-4508-ad65-ba551a486a16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "# probas = 50%, 40%, and 10%\n",
        "log_probas = tf.math.log([[0.5, 0.4, 0.1]])\n",
        "tf.random.categorical(log_probas, num_samples=8) # draw 8 samples"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[0, 0, 1, 1, 1, 0, 0, 0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i1gQztO_CHO"
      },
      "source": [
        "Let's control over the diversity of the generated text using temperature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiSMh9Bw-zuI"
      },
      "source": [
        "def next_char(text, temperature=1):\n",
        "  y_proba = shakespeare_model.predict([text])[0, -1:]\n",
        "  rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "  char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
        "\n",
        "  return text_vec_layer.get_vocabulary()[char_id + 2]\n",
        "\n",
        "def extend_text(text, n_chars=50, temperature=1):\n",
        "  for _ in range(n_chars):\n",
        "    text += next_char(text, temperature)\n",
        "  return text"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to generate some text! Let’s try with different temperature values."
      ],
      "metadata": {
        "id": "Ggz3N8JI4zdK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UL8cBT8_rkj"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "t_text = extend_text('To be or not to be', temperature=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdhNYGjo_50R",
        "outputId": "e1307642-732a-4393-cc1d-594d5da584db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(t_text)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be or not to be a man i shall be a man i shall be a man i shall b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "t_text = extend_text('To be or not to be', temperature=1)"
      ],
      "metadata": {
        "id": "rB1n0msv60fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(t_text)"
      ],
      "metadata": {
        "id": "u5STLlZx61C9",
        "outputId": "919a78a6-2aa7-4bc2-eeb5-e6f678d706a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be or not to be hence\n",
            "sinnertys, as it was noth.\n",
            "\n",
            "page:\n",
            "thou shal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "t_text = extend_text('To be or not to be', temperature=100)"
      ],
      "metadata": {
        "id": "PEFqCmSK672w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(t_text)"
      ],
      "metadata": {
        "id": "XpDL7nlq7EJe",
        "outputId": "b7731c1c-c6bf-40d5-d209-b5d0a6a5a213",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be or not to be!:q?\n",
            "\n",
            "ddidn:;&yoe-3\n",
            "j.&lvj,s-pxh. b:kx:o? woystj3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks5X0LBXDCn8"
      },
      "source": [
        "## Stateful RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0bSQJhIGGi0"
      },
      "source": [
        "Until now, we have used only stateless RNNs: at each training iteration the model starts with a hidden state full of zeros, then it updates this state at each time step, and after the last time step, it throws it away, as it is not needed anymore. \n",
        "\n",
        "What if we told the RNN to preserve this final state after processing one training batch and use it as the initial state for the next training batch? \n",
        "\n",
        "This way the model can learn long-term patterns despite only backpropagating through short sequences. This is called a stateful RNN.\n",
        "\n",
        "First, note that a stateful RNN only makes sense if each input sequence in a batch starts exactly where the corresponding sequence in the previous batch left off. So the first thing we need to do to build a stateful RNN is to use sequential and non-overlapping input sequences (rather than the shuffled and overlapping sequences we used to train stateless RNNs).\n",
        "\n",
        "When creating the Dataset, we must therefore use shift=n_steps (instead of shift=1), when calling the window() method. Moreover,\n",
        "we must obviously not call the shuffle() method.\n",
        "\n",
        "Unfortunately, batching is much harder when preparing a dataset for a stateful RNN than it is for a stateless RNN.\n",
        "\n",
        "Indeed, if we were to call batch(32), then 32 consecutive windows would be put in the same batch, and the following batch would not continue each of these window where it left off. \n",
        "\n",
        "The first batch would contain windows 1 to 32 and the second batch would contain windows 33 to 64, so if you consider, say, the first window of each batch (i.e., windows 1 and 33), you can see that they are not consecutive. The simplest solution to this problem is to just use “batches” containing a single window:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sjjp95VWIERC"
      },
      "source": [
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_ZuX2QHIRTS"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "dataset = dataset.repeat().batch(1)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA5N4gRAJ-xa"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/sequence-fragments-for-stateful-rnn.png?raw=1' width='800'/>\n",
        "\n",
        "Batching is harder, but it is not impossible. For example, we could chop Shakespeare’s text into 32 texts of equal length, create one dataset of consecutive input sequences for each of them, and finally use tf.train.Dataset.zip(datasets).map(lambda\n",
        "*windows: tf.stack(windows)) to create proper consecutive batches, where the nth input sequence in a batch starts off exactly where the nth input sequence ended in the previous batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRMilRTpJYn1"
      },
      "source": [
        "batch_size = 32\n",
        "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
        "datasets = []\n",
        "\n",
        "for encoded_part in encoded_parts:\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
        "  dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "  datasets.append(dataset)\n",
        "\n",
        "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
        "dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PJnlkoPMA19"
      },
      "source": [
        "Now let’s create the stateful RNN. \n",
        "\n",
        "First, we need to set stateful=True when creating every recurrent layer. \n",
        "\n",
        "Second, the stateful RNN needs to know the batch size (since it\n",
        "will preserve a state for each input sequence in the batch), so we must set the batch_input_shape argument in the first layer.\n",
        "\n",
        "Note that we can leave the second dimension unspecified, since the inputs could have any length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSRmeZMeL1lc"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, recurrent_dropout=0.2, \n",
        "                     batch_input_shape=[batch_size, None, max_id]),\n",
        "    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, recurrent_dropout=0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))                             \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68CYyl4INbEa"
      },
      "source": [
        "At the end of each epoch, we need to reset the states before we go back to the beginning of the text. For this, we can use a small callback:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrpKCmLqNVCw"
      },
      "source": [
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "  def on_epoch_begin(self, epoch, logs):\n",
        "    self.model.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY4-qdKGN018"
      },
      "source": [
        "And now we can compile and fit the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tIj15-fNwXm"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "steps_per_epoch = train_size // batch_size // n_steps\n",
        "\n",
        "model.fit(dataset, epochs=50, steps_per_epoch=steps_per_epoch, callbacks=[ResetStatesCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osuKwGg2P1xc"
      },
      "source": [
        "After this model is trained, it will only be possible to use it to make predictions for batches of the same size as were used during training.\n",
        "\n",
        "To avoid this restriction, create an identical stateless model,\n",
        "and copy the stateful model’s weights to this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ejpRyHyP5Hf"
      },
      "source": [
        "stateless_model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))                                       \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7JnYH00QXpX"
      },
      "source": [
        "To set the weights, we first need to build the model (so the weights get created):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aPB8sjdQYRE"
      },
      "source": [
        "stateless_model.build(tf.TensorShape([None, None, max_id]))\n",
        "\n",
        "stateless_model.set_weights(model.get_weights())\n",
        "model = stateless_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwQNL81yQ3-o"
      },
      "source": [
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAchdyp1RTd1"
      },
      "source": [
        "print(complete_text('t'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPigt2OuQ4mI"
      },
      "source": [
        "print(complete_text('t', temperature=0.2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aMqeraFRNFN"
      },
      "source": [
        "print(complete_text('t', temperature=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyuCGtGGRO53"
      },
      "source": [
        "print(complete_text('t', temperature=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uES1zAy3RQjl"
      },
      "source": [
        "print(complete_text('p', temperature=0.2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABtXeJg4PO8Z"
      },
      "source": [
        "Now that we have built a character-level model, it’s time to look at word-level models\n",
        "and tackle a common natural language processing task: sentiment analysis."
      ]
    }
  ]
}