{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyME0BVqo/CODVfRb7hlVZ5A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/machine-learning-research-and-practice/blob/main/hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow/16-NLP-with-RNNs-and-Attention/v2_generating_text_using_character_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHUB8psfrgPJ"
      },
      "source": [
        "## Generating Text using Character RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3nxCza6rwcN"
      },
      "source": [
        "A common approach for natural language tasks is to use recurrent neural networks.\n",
        "We will therefore continue to explore RNNs, starting with\n",
        "a character RNN, trained to predict the next character in a sentence. This will allow us to generate some original text, and in the process we will see how to build a TensorFlow Dataset on a very long sequence. \n",
        "\n",
        "We will first use a stateless RNN (which learns on random portions of text at each iteration, without any information on the rest of the text), then we will build a stateful RNN (which preserves the hidden state between training iterations and continues reading where it left off, allowing it to learn longer patterns).\n",
        "\n",
        "Let’s start with a simple and fun model that can write like Shakespeare (well, sort of)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLNaLqOquKI4"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wC8SV2kuM11"
      },
      "source": [
        "import sys\n",
        "assert sys.version_info >= (3, 7)  # Python ≥3.5 is required\n",
        "\n",
        "import sklearn \n",
        "assert sklearn.__version__ >= \"0.20\"  # Scikit-Learn ≥0.20 is required\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= '2.0'\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3Rly-8iuRJL"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download the Shakespeare data from Andrej Karpathy's char-rnn project."
      ],
      "metadata": {
        "id": "Ze9tNuE2lTdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_url = \"https://homl.info/shakespeare\"\n",
        "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "  shakespeare_text = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_WqqSZelUFW",
        "outputId": "06fecdf9-5ca6-4b66-8265-c629a7b5d8ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://homl.info/shakespeare\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code – shows a short text sample\n",
        "print(shakespeare_text[:80])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T7KPUtilsRo",
        "outputId": "184ed8ab-a5ff-4bb0-8a1f-466e282d8179"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code - shows all 39 distinct characters (after converting to lower case)\n",
        "\"\".join(sorted(set(shakespeare_text.lower())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9uveR0YmUFwh",
        "outputId": "8f11b022-073e-44a6-d63c-e6c75c8e5991"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's encode the text."
      ],
      "metadata": {
        "id": "y6oicn93UbdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\", standardize=\"lower\")\n",
        "text_vec_layer.adapt([shakespeare_text])\n",
        "encoded = text_vec_layer([shakespeare_text])[0]"
      ],
      "metadata": {
        "id": "dxqbhUeAUfWK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s subtract 2 from the\n",
        "character IDs and compute the number of distinct characters and the total number of\n",
        "characters."
      ],
      "metadata": {
        "id": "ze5g7d4TVcmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded -= 2                                     # drop tokens 0 (pad) and 1 (unknown), which we will not use\n",
        "n_tokens = text_vec_layer.vocabulary_size() - 2  # number of distinct chars = 39\n",
        "dataset_size = len(encoded)                      # total number of chars = 1,115,394\n",
        "\n",
        "print(n_tokens)\n",
        "print(dataset_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP85WH2wVS4z",
        "outputId": "a3506315-77ad-4fa6-b8f1-38ea52434c36"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39\n",
            "1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s convert a long sequence of character IDs into a dataset of input/target window pairs."
      ],
      "metadata": {
        "id": "3NTbAiT3XWiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
        "  ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "  ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
        "  ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(100_000, seed=seed)\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
      ],
      "metadata": {
        "id": "PGHTiZrvXZmv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code – a simple example using to_dataset()\n",
        "# There's just one sample in this dataset: the input represents \"to b\" and the output represents \"o be\"\n",
        "list(to_dataset(text_vec_layer([\"To be\"])[0], length=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GasjhdJaZgDe",
        "outputId": "d037a2e2-48c0-4d5d-eb57-0d1c06a8f1a1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[ 4,  5,  2, 23]])>,\n",
              "  <tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[ 5,  2, 23,  3]])>)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(to_dataset(text_vec_layer([\"To be or not to be\"])[0], length=8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dnFoBZ6Z8_8",
        "outputId": "59b1bc1f-b80f-4a00-eaff-64a140c9b3d6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(<tf.Tensor: shape=(10, 8), dtype=int64, numpy=\n",
              "  array([[ 4,  5,  2, 23,  3,  2,  5, 10],\n",
              "         [ 5,  2, 23,  3,  2,  5, 10,  2],\n",
              "         [ 2, 23,  3,  2,  5, 10,  2, 11],\n",
              "         [23,  3,  2,  5, 10,  2, 11,  5],\n",
              "         [ 3,  2,  5, 10,  2, 11,  5,  4],\n",
              "         [ 2,  5, 10,  2, 11,  5,  4,  2],\n",
              "         [ 5, 10,  2, 11,  5,  4,  2,  4],\n",
              "         [10,  2, 11,  5,  4,  2,  4,  5],\n",
              "         [ 2, 11,  5,  4,  2,  4,  5,  2],\n",
              "         [11,  5,  4,  2,  4,  5,  2, 23]])>,\n",
              "  <tf.Tensor: shape=(10, 8), dtype=int64, numpy=\n",
              "  array([[ 5,  2, 23,  3,  2,  5, 10,  2],\n",
              "         [ 2, 23,  3,  2,  5, 10,  2, 11],\n",
              "         [23,  3,  2,  5, 10,  2, 11,  5],\n",
              "         [ 3,  2,  5, 10,  2, 11,  5,  4],\n",
              "         [ 2,  5, 10,  2, 11,  5,  4,  2],\n",
              "         [ 5, 10,  2, 11,  5,  4,  2,  4],\n",
              "         [10,  2, 11,  5,  4,  2,  4,  5],\n",
              "         [ 2, 11,  5,  4,  2,  4,  5,  2],\n",
              "         [11,  5,  4,  2,  4,  5,  2, 23],\n",
              "         [ 5,  4,  2,  4,  5,  2, 23,  3]])>)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we’re ready to create the training set, the validation set, and the test set."
      ],
      "metadata": {
        "id": "8sPvSjJwZV4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "length = 100\n",
        "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True, seed=42)\n",
        "valid_set = to_dataset(encoded[1_000_000: 1_060_000], length=length)\n",
        "test_set = to_dataset(encoded[1_060_000:], length=length)"
      ],
      "metadata": {
        "id": "nILzO6EMZYbb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuOZsaWcNHXK"
      },
      "source": [
        "## Building and Training Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3VfK0wp-LuD"
      },
      "source": [
        "Let’s build and train\n",
        "a model with one GRU layer composed of 128 units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5_bM7ns-Ef1",
        "outputId": "4b184fcd-bbd4-4196-eb1b-63fc234c6747",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.Dense(n_tokens, activation='softmax')                            \n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 16)          624       \n",
            "                                                                 \n",
            " gru (GRU)                   (None, None, 128)         56064     \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 39)          5031      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61,719\n",
            "Trainable params: 61,719\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NofAusbtDkBI"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=[\"accuracy\"])\n",
        "\n",
        "model_ckpt = keras.callbacks.ModelCheckpoint(\"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True)\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=10, callbacks=[model_ckpt])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQYwTAGmOtL1"
      },
      "source": [
        "Let's handle text preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_model = keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    keras.layers.Lambda(lambda x: x - 2),  # no <PAD> or <UNK> tokens\n",
        "    model\n",
        "])"
      ],
      "metadata": {
        "id": "XwLd181kIQ4B"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now let’s use it to predict the next character in a sentence."
      ],
      "metadata": {
        "id": "acbjr3jIIrO4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0i58Gcz8c2m",
        "outputId": "efaf5bb0-29bd-4a05-c12f-b6bef39b2e04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "y_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1]\n",
        "# choose the most probable character ID\n",
        "y_pred = tf.argmax(y_proba)\n",
        "text_vec_layer.get_vocabulary()[y_pred + 2]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 392ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'e'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba = shakespeare_model.predict([\"I love yo\"])[0, -1]\n",
        "# choose the most probable character ID\n",
        "y_pred = tf.argmax(y_proba)\n",
        "text_vec_layer.get_vocabulary()[y_pred + 2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "F4VcDW-8Zav9",
        "outputId": "ac8d09f5-95c1-45e5-f7c1-d6cb5d21bed3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 29ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'u'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba = shakespeare_model.predict([\"Where are you goin\"])[0, -1]\n",
        "# choose the most probable character ID\n",
        "y_pred = tf.argmax(y_proba)\n",
        "text_vec_layer.get_vocabulary()[y_pred + 2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "kl-YgVJoZmqN",
        "outputId": "8bb61661-fdf4-4f46-a378-8d696027db11"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 27ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'g'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gnXqqo68-GA"
      },
      "source": [
        "## Generating Fake Shakespearean Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhqjOaXI9gRy"
      },
      "source": [
        "To generate new text using the Char-RNN model, we could feed it some text, make the model predict the most likely next letter, add it at the end of the text, then give the extended text to the model to guess the next letter, and so on. But in practice this often leads to the same words being repeated over and over again. \n",
        "\n",
        "Instead, we can pick the next character randomly, with a probability equal to the estimated probability, using TensorFlow’s tf.random.categorical() function. This will generate more diverse and interesting text. The categorical() function samples random class indices, given the class log probabilities (logits). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4diFK4K86F2",
        "outputId": "2d727716-1a99-4d3a-ec63-ec0a0c3057c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
              "        2, 0, 0, 1, 1, 1, 0, 0, 1, 2, 0, 0, 1, 1, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i1gQztO_CHO"
      },
      "source": [
        "To have more control over the diversity of the generated text, we can divide the logits by a number called the temperature, which we can tweak as we wish: a temperature close to 0 will favor the highprobability characters, while a very high temperature will give all characters an equal probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiSMh9Bw-zuI"
      },
      "source": [
        "def next_char(text, temperature=1):\n",
        "  X_new = preprocess([text])\n",
        "  y_proba = model.predict(X_new)[0, -1:, :]\n",
        "  rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "  char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
        "\n",
        "  return tokenizer.sequences_to_texts(char_id.numpy())[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UL8cBT8_rkj",
        "outputId": "40814dcd-60de-4eb0-dbf5-ce563e5b8034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "next_char('How are yo', temperature=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'u'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdhNYGjo_50R",
        "outputId": "a4e03e75-9772-4040-b77b-cc2f57041556",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "next_char('How is lif', temperature=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'t'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be5O71gYAPJc"
      },
      "source": [
        "We can write a small function that will repeatedly call next_char() to get the next character and append it to the given text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5KMC4tK__L1"
      },
      "source": [
        "def complete_text(text, n_chars=100, temperature=1):\n",
        "  for _ in range(n_chars):\n",
        "    text += next_char(text, temperature)\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_TuQYOSCVcx"
      },
      "source": [
        "We are now ready to generate some text! Let’s try with different temperatures:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKXJ5bUOAuIv"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "print(complete_text('t', temperature=0.2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QV0xKuEA2UZ",
        "outputId": "09903a95-24d8-4edc-9897-bf548593e754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(complete_text('t', temperature=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tperves me from this faults revenged,\n",
            "i bray it! supking, my enemy to the commoner to please yourselv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQBVv41iBggA",
        "outputId": "f3505f84-e058-4d03-b4a2-a9d567533c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(complete_text('t', temperature=2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t as't so;,' rare' reigs as sir; broighews cipixict hid\n",
            "curmom's helves with\n",
            "yoke, are remialumiens; \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDYIjVxuBjR_",
        "outputId": "964108ea-025c-4904-8aa7-4c6a031008d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(complete_text('p', temperature=0.2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prayes.\n",
            "they her care have the rest was deliver the heart and the belly and with her to the belly and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5Lt3Iw-CvVJ"
      },
      "source": [
        "Apparently our Shakespeare model works best at a temperature close to 1. To generate more convincing text, you could try using more GRU layers and more neurons per layer, train for longer, and add some regularization.\n",
        "\n",
        "Moreover, the model is currently incapable of learning patterns longer than n_steps, which is just 100 characters. You could try\n",
        "making this window larger, but it will also make training harder, and even LSTM and GRU cells cannot handle very long sequences. \n",
        "\n",
        "Alternatively, you could use a stateful RNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks5X0LBXDCn8"
      },
      "source": [
        "## Stateful RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0bSQJhIGGi0"
      },
      "source": [
        "Until now, we have used only stateless RNNs: at each training iteration the model starts with a hidden state full of zeros, then it updates this state at each time step, and after the last time step, it throws it away, as it is not needed anymore. \n",
        "\n",
        "What if we told the RNN to preserve this final state after processing one training batch and use it as the initial state for the next training batch? \n",
        "\n",
        "This way the model can learn long-term patterns despite only backpropagating through short sequences. This is called a stateful RNN.\n",
        "\n",
        "First, note that a stateful RNN only makes sense if each input sequence in a batch starts exactly where the corresponding sequence in the previous batch left off. So the first thing we need to do to build a stateful RNN is to use sequential and non-overlapping input sequences (rather than the shuffled and overlapping sequences we used to train stateless RNNs).\n",
        "\n",
        "When creating the Dataset, we must therefore use shift=n_steps (instead of shift=1), when calling the window() method. Moreover,\n",
        "we must obviously not call the shuffle() method.\n",
        "\n",
        "Unfortunately, batching is much harder when preparing a dataset for a stateful RNN than it is for a stateless RNN.\n",
        "\n",
        "Indeed, if we were to call batch(32), then 32 consecutive windows would be put in the same batch, and the following batch would not continue each of these window where it left off. \n",
        "\n",
        "The first batch would contain windows 1 to 32 and the second batch would contain windows 33 to 64, so if you consider, say, the first window of each batch (i.e., windows 1 and 33), you can see that they are not consecutive. The simplest solution to this problem is to just use “batches” containing a single window:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sjjp95VWIERC"
      },
      "source": [
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_ZuX2QHIRTS"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "dataset = dataset.repeat().batch(1)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA5N4gRAJ-xa"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/sequence-fragments-for-stateful-rnn.png?raw=1' width='800'/>\n",
        "\n",
        "Batching is harder, but it is not impossible. For example, we could chop Shakespeare’s text into 32 texts of equal length, create one dataset of consecutive input sequences for each of them, and finally use tf.train.Dataset.zip(datasets).map(lambda\n",
        "*windows: tf.stack(windows)) to create proper consecutive batches, where the nth input sequence in a batch starts off exactly where the nth input sequence ended in the previous batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRMilRTpJYn1"
      },
      "source": [
        "batch_size = 32\n",
        "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
        "datasets = []\n",
        "\n",
        "for encoded_part in encoded_parts:\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
        "  dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "  datasets.append(dataset)\n",
        "\n",
        "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
        "dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PJnlkoPMA19"
      },
      "source": [
        "Now let’s create the stateful RNN. \n",
        "\n",
        "First, we need to set stateful=True when creating every recurrent layer. \n",
        "\n",
        "Second, the stateful RNN needs to know the batch size (since it\n",
        "will preserve a state for each input sequence in the batch), so we must set the batch_input_shape argument in the first layer.\n",
        "\n",
        "Note that we can leave the second dimension unspecified, since the inputs could have any length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSRmeZMeL1lc"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, recurrent_dropout=0.2, \n",
        "                     batch_input_shape=[batch_size, None, max_id]),\n",
        "    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, recurrent_dropout=0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))                             \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68CYyl4INbEa"
      },
      "source": [
        "At the end of each epoch, we need to reset the states before we go back to the beginning of the text. For this, we can use a small callback:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrpKCmLqNVCw"
      },
      "source": [
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "  def on_epoch_begin(self, epoch, logs):\n",
        "    self.model.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY4-qdKGN018"
      },
      "source": [
        "And now we can compile and fit the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tIj15-fNwXm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6fc5d9bd-7bc8-45d5-e9c1-e9aec2b34963"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "steps_per_epoch = train_size // batch_size // n_steps\n",
        "\n",
        "model.fit(dataset, epochs=50, steps_per_epoch=steps_per_epoch, callbacks=[ResetStatesCallback()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 313 steps\n",
            "Epoch 1/50\n",
            "313/313 [==============================] - 109s 348ms/step - loss: 2.6224\n",
            "Epoch 2/50\n",
            "313/313 [==============================] - 106s 339ms/step - loss: 2.2280\n",
            "Epoch 3/50\n",
            "313/313 [==============================] - 106s 339ms/step - loss: 2.1504\n",
            "Epoch 4/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 2.4703\n",
            "Epoch 5/50\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 2.3564\n",
            "Epoch 6/50\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 2.2239\n",
            "Epoch 7/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 2.0765\n",
            "Epoch 8/50\n",
            "313/313 [==============================] - 105s 337ms/step - loss: 2.0493\n",
            "Epoch 9/50\n",
            "313/313 [==============================] - 105s 335ms/step - loss: 2.0224\n",
            "Epoch 10/50\n",
            "313/313 [==============================] - 105s 335ms/step - loss: 1.9398\n",
            "Epoch 11/50\n",
            "313/313 [==============================] - 106s 337ms/step - loss: 1.9237\n",
            "Epoch 12/50\n",
            "313/313 [==============================] - 105s 335ms/step - loss: 1.8705\n",
            "Epoch 13/50\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 1.8721\n",
            "Epoch 14/50\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 1.7946\n",
            "Epoch 15/50\n",
            "313/313 [==============================] - 105s 334ms/step - loss: 1.7937\n",
            "Epoch 16/50\n",
            "313/313 [==============================] - 104s 334ms/step - loss: 1.7369\n",
            "Epoch 17/50\n",
            "313/313 [==============================] - 104s 333ms/step - loss: 1.7189\n",
            "Epoch 18/50\n",
            "313/313 [==============================] - 104s 334ms/step - loss: 1.7045\n",
            "Epoch 19/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 1.7021\n",
            "Epoch 20/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 1.6820\n",
            "Epoch 21/50\n",
            "313/313 [==============================] - 106s 337ms/step - loss: 1.6724\n",
            "Epoch 22/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 1.6631\n",
            "Epoch 23/50\n",
            "313/313 [==============================] - 104s 333ms/step - loss: 1.6555\n",
            "Epoch 24/50\n",
            "313/313 [==============================] - 103s 329ms/step - loss: 1.6485\n",
            "Epoch 25/50\n",
            "313/313 [==============================] - 105s 334ms/step - loss: 1.6410\n",
            "Epoch 26/50\n",
            "313/313 [==============================] - 105s 337ms/step - loss: 1.6365\n",
            "Epoch 27/50\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 1.6305\n",
            "Epoch 28/50\n",
            "313/313 [==============================] - 106s 337ms/step - loss: 1.6260\n",
            "Epoch 29/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 1.6215\n",
            "Epoch 30/50\n",
            "313/313 [==============================] - 105s 337ms/step - loss: 1.6181\n",
            "Epoch 31/50\n",
            "313/313 [==============================] - 106s 337ms/step - loss: 1.6126\n",
            "Epoch 32/50\n",
            "313/313 [==============================] - 105s 337ms/step - loss: 1.6084\n",
            "Epoch 33/50\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 1.6067\n",
            "Epoch 34/50\n",
            "313/313 [==============================] - 105s 337ms/step - loss: 1.6021\n",
            "Epoch 35/50\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 1.5995\n",
            "Epoch 36/50\n",
            "313/313 [==============================] - 105s 337ms/step - loss: 1.5965\n",
            "Epoch 37/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 1.5946\n",
            "Epoch 38/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 1.5907\n",
            "Epoch 39/50\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 1.5874\n",
            "Epoch 40/50\n",
            "313/313 [==============================] - 106s 339ms/step - loss: 1.5862\n",
            "Epoch 41/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 1.5845\n",
            "Epoch 42/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 1.5827\n",
            "Epoch 43/50\n",
            "313/313 [==============================] - 106s 340ms/step - loss: 1.5783\n",
            "Epoch 44/50\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 1.5781\n",
            "Epoch 45/50\n",
            "313/313 [==============================] - 105s 335ms/step - loss: 1.5743\n",
            "Epoch 46/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 1.5728\n",
            "Epoch 47/50\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 1.5718\n",
            "Epoch 48/50\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 1.5697\n",
            "Epoch 49/50\n",
            "313/313 [==============================] - 106s 338ms/step - loss: 1.5680\n",
            "Epoch 50/50\n",
            "313/313 [==============================] - 105s 335ms/step - loss: 1.5671\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f951e5fafd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osuKwGg2P1xc"
      },
      "source": [
        "After this model is trained, it will only be possible to use it to make predictions for batches of the same size as were used during training.\n",
        "\n",
        "To avoid this restriction, create an identical stateless model,\n",
        "and copy the stateful model’s weights to this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ejpRyHyP5Hf"
      },
      "source": [
        "stateless_model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))                                       \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7JnYH00QXpX"
      },
      "source": [
        "To set the weights, we first need to build the model (so the weights get created):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aPB8sjdQYRE"
      },
      "source": [
        "stateless_model.build(tf.TensorShape([None, None, max_id]))\n",
        "\n",
        "stateless_model.set_weights(model.get_weights())\n",
        "model = stateless_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwQNL81yQ3-o"
      },
      "source": [
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAchdyp1RTd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b3532688-bb38-4923-b3c9-a6ef5c8a2371"
      },
      "source": [
        "print(complete_text('t'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tant honour friends.\n",
            "'tis mave strented by\n",
            "do him of discoriolanus and voice as bloody the letter's g\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPigt2OuQ4mI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "7ec0d4bb-f151-406e-b18c-94ef1901d772"
      },
      "source": [
        "print(complete_text('t', temperature=0.2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tish the death,\n",
            "and the stand the change of the seal the present\n",
            "to the poor son, the strange to the \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aMqeraFRNFN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "ed2ec6c8-6a7b-4eeb-f898-05332d24aac2"
      },
      "source": [
        "print(complete_text('t', temperature=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t as sost; for we his\n",
            "warwick,--and i death in point be cause\n",
            "the kind welcome that require: no, no w\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyuCGtGGRO53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "0900336f-8d84-4d40-e8f1-3d88231fd856"
      },
      "source": [
        "print(complete_text('t', temperature=2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t:\n",
            "aqhneed's me: bie o. caws-dafe?\n",
            "at,,--by him comb!ocjnar'd court'rt?'\n",
            "the voicoftiels faebe-clace.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uES1zAy3RQjl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "5b688082-1b6c-47c7-816f-385a7e28dca2"
      },
      "source": [
        "print(complete_text('p', temperature=0.2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "part of the seeming the command\n",
            "to the country to the stand the stands of the words,\n",
            "and i will be so\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABtXeJg4PO8Z"
      },
      "source": [
        "Now that we have built a character-level model, it’s time to look at word-level models\n",
        "and tackle a common natural language processing task: sentiment analysis."
      ]
    }
  ]
}