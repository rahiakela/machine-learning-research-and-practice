{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06-layer-normalization.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOwIiIdyROXkjFjklCCHKdR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/machine-learning-research-and-practice/blob/main/hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow/12-custom-models-and-training-with-tensorflow/06_layer_normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer Normalization"
      ],
      "metadata": {
        "id": "3_-bfWpuJ6Bl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will implement a custom layer that performs Layer Normalization.\n",
        "\n",
        "And it will do the following things.\n",
        "\n",
        "1. The `build()` method should define two trainable weights *α* and *β*.\n",
        "2. The `call()` method should compute the mean_ μ _and standard deviation_ σ _of each instance's features.\n",
        "3. Ensure that your custom layer produces the same (or very nearly the same) output as the `keras.layers.LayerNormalization` layer.\n",
        "\n"
      ],
      "metadata": {
        "id": "JWnEp7wGJ7QY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "jGotmr4LK43k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import sklearn\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ],
      "metadata": {
        "id": "G0vsm-w4K5z-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "ty8i8qgFMpjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by loading and preparing the California housing dataset. "
      ],
      "metadata": {
        "id": "6CGg7E0PHCP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing = fetch_california_housing()\n",
        "\n",
        "x_train_full, x_test, y_train_full, y_test = train_test_split(housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train_full, y_train_full, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_valid_scaled = scaler.transform(x_valid)\n",
        "x_test_scaled = scaler.transform(x_test)"
      ],
      "metadata": {
        "id": "YJx_TeqwMqer"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1"
      ],
      "metadata": {
        "id": "weXOA8tjZHwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Exercise: The `build()` method should define two trainable weights *α* and *β*, both of shape `input_shape[-1:]` and data type `tf.float32`. *α* should be initialized with 1s, and *β* with 0s._"
      ],
      "metadata": {
        "id": "-Rj1dnaeZCYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(keras.layers.Layer):\n",
        "  def __init__(self, eps=0.001, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.eps = eps\n",
        "\n",
        "  def build(self, batch_input_shape):\n",
        "    self.alpha = self.add_weight(name=\"alpha\", shape=batch_input_shape[-1:], initializer=\"ones\")\n",
        "    self.beta = self.add_weight(name=\"beta\", shape=batch_input_shape[-1:], initializer=\"zeros\")\n",
        "    super().build(batch_input_shape)  # must be at the end"
      ],
      "metadata": {
        "id": "8qeIH9nEaScU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2"
      ],
      "metadata": {
        "id": "_PNkoPQrDkfU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Exercise: The `call()` method should compute the mean_ μ _and standard deviation_ σ _of each instance's features. \n",
        "\n",
        "For this, you can use `tf.nn.moments(inputs, axes=-1, keepdims=True)`, which returns the mean μ and the variance σ<sup>2</sup> of all instances (compute the square root of the variance to get the standard deviation). \n",
        "\n",
        "Then the function should compute and return *α*⊗(*X* - μ)/(σ + ε) + *β*, where ⊗ represents itemwise multiplication (`*`) and ε is a smoothing term (small constant to avoid division by zero, e.g., 0.001)._"
      ],
      "metadata": {
        "id": "zqfSKb1mDp0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(keras.layers.Layer):\n",
        "  def __init__(self, eps=0.001, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.eps = eps\n",
        "\n",
        "  def build(self, batch_input_shape):\n",
        "    self.alpha = self.add_weight(name=\"alpha\", shape=batch_input_shape[-1:], initializer=\"ones\")\n",
        "    self.beta = self.add_weight(name=\"beta\", shape=batch_input_shape[-1:], initializer=\"zeros\")\n",
        "    super().build(batch_input_shape)  # must be at the end\n",
        "  \n",
        "  def call(self, X):\n",
        "    mean, variance = tf.nn.moments(X, axes=-1, keepdims=True)\n",
        "    return self.alpha * (X - mean) / (tf.sqrt(variance + self.eps)) + self.beta\n",
        "\n",
        "  def compute_output_shape(self, batch_input_shape):\n",
        "    return batch_input_shape\n",
        "\n",
        "  def get_config(self):\n",
        "    base_config = super().get_config()\n",
        "    return {**base_config, \"eps\": self.eps}"
      ],
      "metadata": {
        "id": "N9hH1awVc_3e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that making _ε_ a hyperparameter (`eps`) was not compulsory. Also note that it's preferable to compute `tf.sqrt(variance + self.eps)` rather than `tf.sqrt(variance) + self.eps`. \n",
        "\n",
        "Indeed, the derivative of `sqrt(z)` is undefined when `z=0`, so training will bomb whenever the variance vector has at least one component equal to 0. Adding _ε_ within the square root guarantees that this will never happen."
      ],
      "metadata": {
        "id": "Xv4IGWJxFHZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 3"
      ],
      "metadata": {
        "id": "usoHqhPNFZwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Exercise: Ensure that your custom layer produces the same (or very nearly the same) output as the `keras.layers.LayerNormalization` layer._\n",
        "\n",
        "Let's create one instance of each class, apply them to some data (e.g., the training set), and ensure that the difference is negligeable."
      ],
      "metadata": {
        "id": "Gf4QVZlZFbC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = x_train.astype(np.float32)\n",
        "\n",
        "custom_layer_norm = LayerNormalization()\n",
        "keras_layer_norm = keras.layers.LayerNormalization()\n",
        "\n",
        "tf.reduce_mean(keras.losses.mean_absolute_error(keras_layer_norm(X), custom_layer_norm(X)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbvJjopNdRcw",
        "outputId": "70b1e74f-38af-4718-9ea3-d5cac924f8d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=5.496049e-08>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yep, that's close enough. \n",
        "\n",
        "To be extra sure, let's make alpha and beta completely random and compare again:"
      ],
      "metadata": {
        "id": "dYZC-k_KdqTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_alpha = np.random.rand(X.shape[-1])\n",
        "random_beta = np.random.rand(X.shape[-1])\n",
        "\n",
        "custom_layer_norm.set_weights([random_alpha, random_beta])\n",
        "keras_layer_norm.set_weights([random_alpha, random_beta])\n",
        "\n",
        "tf.reduce_mean(keras.losses.mean_absolute_error(keras_layer_norm(X), custom_layer_norm(X)))"
      ],
      "metadata": {
        "id": "puBdBJ4ndrs4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c419fa7d-9dbe-428c-f55c-304a13ff1ff4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=2.2581645e-08>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Still a negligeable difference! Our custom layer works fine."
      ],
      "metadata": {
        "id": "UM7a-sDlIoS8"
      }
    }
  ]
}