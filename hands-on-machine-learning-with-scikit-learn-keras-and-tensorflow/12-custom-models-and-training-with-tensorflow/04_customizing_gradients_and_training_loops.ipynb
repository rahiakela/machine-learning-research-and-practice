{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04-customizing-gradients-and-training-loops.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNTphK1UbqXUoyYujvuaxdi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/machine-learning-research-and-practice/blob/main/hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow/12-custom-models-and-training-with-tensorflow/04_customizing_gradients_and_training_loops.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customizing Gradients and Training Loops"
      ],
      "metadata": {
        "id": "3_-bfWpuJ6Bl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In fact, 95% of the use cases you will encounter will not require anything other than `tf.keras` and `tf.data`.\n",
        "\n",
        "But now it’s time to dive deeper into TensorFlow\n",
        "and take a look at its lower-level Python API. This will be useful when you need extra\n",
        "control to write custom loss functions, custom metrics, layers, models, initializers,\n",
        "regularizers, weight constraints, and more. \n",
        "\n",
        "You may even need to fully control the\n",
        "training loop itself, for example to apply special transformations or constraints to the\n",
        "gradients (beyond just clipping them) or to use multiple optimizers for different parts\n",
        "of the network.\n",
        "\n",
        "TensorFlow’s API revolves around tensors, which flow from operation to operation—hence the name TensorFlow.\n",
        "\n",
        "A tensor is very similar to a NumPy ndarray: it is usually\n",
        "a multidimensional array, but it can also hold a scalar (a simple value, such as 42).\n",
        "These tensors will be important when we create custom cost functions, custom metrics,\n",
        "custom layers, and more, so let’s see how to create and manipulate them.\n",
        "\n"
      ],
      "metadata": {
        "id": "JWnEp7wGJ7QY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "jGotmr4LK43k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import sklearn\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ],
      "metadata": {
        "id": "G0vsm-w4K5z-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_softplus(z): # return value is just tf.nn.softplus(z)\n",
        "  return tf.math.log(tf.exp(z) + 1.0)"
      ],
      "metadata": {
        "id": "JER4P7HGoL7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Dataset"
      ],
      "metadata": {
        "id": "ty8i8qgFMpjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by loading and preparing the California housing dataset. "
      ],
      "metadata": {
        "id": "6CGg7E0PHCP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing = fetch_california_housing()\n",
        "\n",
        "x_train_full, x_test, y_train_full, y_test = train_test_split(housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train_full, y_train_full, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_valid_scaled = scaler.transform(x_valid)\n",
        "x_test_scaled = scaler.transform(x_test)"
      ],
      "metadata": {
        "id": "YJx_TeqwMqer"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Computing Gradients using Autodiff"
      ],
      "metadata": {
        "id": "weXOA8tjZHwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand how to use autodiff compute gradients\n",
        "automatically, let’s consider a simple toy function:"
      ],
      "metadata": {
        "id": "-Rj1dnaeZCYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(w1, w2):\n",
        "  return 3 * w1 ** 2 + 2 * w1 * w2"
      ],
      "metadata": {
        "id": "8qeIH9nEaScU"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using calculus, we can analytically find that the partial derivative of this function with regard to $w_1$ and $w_2$.\n",
        "\n",
        "$$\n",
        "\\frac{d \\mathbf{f}}{d \\mathbf{w_1}} = \\frac{d \\mathbf{(3*w_1^2+2*w_1*w_2)}}{d \\mathbf{w_1}} = 3* 2* w_1^{2-1}+2*w_1^{1-1}*w_2 = 6 * w_1+2*w_2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{d \\mathbf{f}}{d \\mathbf{w_2}} = \\frac{d \\mathbf{(3*w_1^2+2*w_1*w_2)}}{d \\mathbf{w_2}} = 3* 2* 0 +2*w_1*w_2^{1-1} = 2 * w_1\n",
        "$$\n",
        "\n",
        "For example, at the point $(w1, w2) = (5, 3)$, these partial\n",
        "derivatives are equal to 36 and 10, respectively, so the gradient vector at this point is `(36, 10)`.\n"
      ],
      "metadata": {
        "id": "DYuMBV6paqqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dw1 = 6 * 5 + 2 * 3\n",
        "print(dw1) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9hH1awVc_3e",
        "outputId": "1afe2696-67fc-4267-cd45-82a6c83b3438"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dw2 = 2 * 5\n",
        "print(dw2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbvJjopNdRcw",
        "outputId": "a1fd9af7-7d40-4977-9128-c08cf5807442"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But if this were a neural network, the function would be much more complex,\n",
        "typically with tens of thousands of parameters, and finding the partial derivatives\n",
        "analytically by hand would be an almost impossible task. \n",
        "\n",
        "One solution could be\n",
        "to compute an approximation of each partial derivative by measuring how much the\n",
        "function’s output changes when you tweak the corresponding parameter:"
      ],
      "metadata": {
        "id": "dYZC-k_KdqTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w1, w2 = 5, 3\n",
        "eps = 1e-6"
      ],
      "metadata": {
        "id": "puBdBJ4ndrs4"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(f(w1 + eps, w2) - f(w1, w2)) / eps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4dc4o1YdzAq",
        "outputId": "39750d2f-309a-4916-dafc-cda8beb35fad"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36.000003007075065"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(f(w1, w2 + eps) - f(w1, w2)) / eps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nObVPoG3d6_j",
        "outputId": "1673639d-bc24-4250-9cba-f578ca83e4d5"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.000000003174137"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks about right! This works rather well and is easy to implement, but it is just an approximation, and importantly you need to call `f()` at least once per parameter (not twice, since we could compute $f(w_1, w_2)$ just once).\n",
        "\n",
        "Needing to call `f()` at least once\n",
        "per parameter makes this approach intractable for large neural networks. \n",
        "\n",
        "So instead, we should use autodiff. TensorFlow makes this pretty simple:"
      ],
      "metadata": {
        "id": "YBuEBICkeLwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = f(w1, w2)\n",
        "\n",
        "gradients = tape.gradient(z, [w1, w2])"
      ],
      "metadata": {
        "id": "e_OIM-oefsRO"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s take a look at the gradients that TensorFlow computed."
      ],
      "metadata": {
        "id": "nCMUsxr0gJeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2ARUtcJgKEZ",
        "outputId": "a8407df6-cb8a-4806-8986-35c7b6784151"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect! Not only is the result accurate, but the `gradient()` method only goes through the recorded computations once (in reverse order), no matter how many variables there are, so it is\n",
        "incredibly efficient. \n",
        "\n",
        "It’s like magic!\n",
        "\n",
        "The tape is automatically erased immediately after you call its `gradient()` method, so\n",
        "you will get an exception if you try to call `gradient()` twice:"
      ],
      "metadata": {
        "id": "9kbMuQq-gUIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = f(w1, w2)\n",
        "\n",
        "dz_dw1 = tape.gradient(z, w1)  # => tensor 36.0\n",
        "print(dz_dw1)  \n",
        "\n",
        "try:\n",
        "  dz_dw2 = tape.gradient(z, w2)  # RuntimeError!\n",
        "except RuntimeError as re:\n",
        "  print(f\"RuntimeError: {re}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSjxNgXWgjs4",
        "outputId": "446a115c-6b90-4c1b-d6eb-8d84c76bd13f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(36.0, shape=(), dtype=float32)\n",
            "RuntimeError: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you need to call `gradient()` more than once, you must make the tape persistent\n",
        "and delete it each time you are done with it to free resources."
      ],
      "metadata": {
        "id": "PrZMkcZUhtFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  z = f(w1, w2)\n",
        "\n",
        "dz_dw1 = tape.gradient(z, w1)  # => tensor 36.0\n",
        "print(dz_dw1)  \n",
        "\n",
        "dz_dw2 = tape.gradient(z, w2)  # => tensor 10.0, works fine now!\n",
        "print(dz_dw2)\n",
        "del tape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtsaO25jhvkx",
        "outputId": "13d6976d-4b74-4dfd-e450-fd9cd4e7b7c7"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(36.0, shape=(), dtype=float32)\n",
            "tf.Tensor(10.0, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, the tape will only track operations involving variables, so if you try to\n",
        "compute the gradient of z with regard to anything other than a variable, the result\n",
        "will be None:"
      ],
      "metadata": {
        "id": "rGS8dLKRiHo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = f(c1, c2)\n",
        "\n",
        "gradients = tape.gradient(z, [c1, c2])  # returns [None, None]\n",
        "gradients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r30X2BUiOQe",
        "outputId": "b44533f3-08f8-4581-9b99-795f69f44fa9"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, you can force the tape to watch any tensors you like, to record every operation\n",
        "that involves them. \n",
        "\n",
        "You can then compute gradients with regard to these tensors,\n",
        "as if they were variables."
      ],
      "metadata": {
        "id": "rzpt7zqhikyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(c1)\n",
        "  tape.watch(c2)\n",
        "  z = f(c1, c2)\n",
        "\n",
        "gradients = tape.gradient(z, [c1, c2])  # returns [tensor 36., tensor 10.]\n",
        "gradients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_F4sN17in2r",
        "outputId": "fcfdf322-1434-4862-c356-58aa065af5fb"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can be useful in some cases, like if you want to implement a regularization loss\n",
        "that penalizes activations that vary a lot when the inputs vary little: the loss will be\n",
        "based on the gradient of the activations with regard to the inputs. \n",
        "\n",
        "Since the inputs are\n",
        "not variables, you would need to tell the tape to watch them.\n",
        "\n",
        "\n",
        "In some cases you may want to stop gradients from backpropagating through some\n",
        "part of your neural network. To do this, you must use the `tf.stop_gradient()` function.\n",
        "\n",
        "The function returns its inputs during the forward pass (like `tf.identity()`), but it does not let gradients through during backpropagation (it acts like a constant)."
      ],
      "metadata": {
        "id": "W87uCmsKi6xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(w1, w2):\n",
        "  return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = f(w1, w2)  # same result as without stop_gradient()\n",
        "\n",
        "gradients = tape.gradient(z, [w1, w2])  # => returns [tensor 30., None]\n",
        "gradients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCowZeoMjPku",
        "outputId": "d9805244-5cb4-4898-9b72-1727165a5e9c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, you may occasionally run into some numerical issues when computing gradients.\n",
        "\n",
        "For example, if you compute the gradients of the `my_softplus()` function for\n",
        "large inputs, the result will be `NaN`."
      ],
      "metadata": {
        "id": "hOe7KZ_xlZlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable([100.])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = my_softplus(x)\n",
        "\n",
        "gradients = tape.gradient(z, [x])\n",
        "gradients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0zlBRs_ldXi",
        "outputId": "86271d96-3840-4e73-9a65-569dfc67366b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([nan], dtype=float32)>]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is because computing the gradients of this function using autodiff leads to some\n",
        "numerical difficulties: due to floating-point precision errors, autodiff ends up computing\n",
        "infinity divided by infinity (which returns `NaN`).\n",
        "\n",
        "Fortunately, we can analytically find that the derivative of the softplus function is just $1 / (1 + 1 / exp(x))$, which\n",
        "is numerically stable. \n",
        "\n",
        "Next, we can tell TensorFlow to use this stable function when\n",
        "computing the gradients of the `my_softplus()` function by decorating it with\n",
        "`@tf.custom_gradient` and making it return both its normal output and the function that computes the derivatives."
      ],
      "metadata": {
        "id": "8weNJY2il1Te"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.custom_gradient\n",
        "def my_netter_softplus(z): \n",
        "  exp = tf.exp(z)\n",
        "  def my_softplus_gradients(grad):\n",
        "    return grad / (1 + 1 / exp)\n",
        "  return tf.math.log(exp + 1.0), my_softplus_gradients"
      ],
      "metadata": {
        "id": "c_wlpZiumU2N"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now when we compute the gradients of the `my_better_softplus()` function, we get\n",
        "the proper result, even for large input values."
      ],
      "metadata": {
        "id": "_qe05eIcnBNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable([100.])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = my_netter_softplus(x)\n",
        "\n",
        "gradients = tape.gradient(z, [x])\n",
        "gradients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voWclNaMnBiw",
        "outputId": "7fa0ff58-1419-4f6e-de56-fd3031656828"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! You can now compute the gradients of any function (provided it is\n",
        "differentiable at the point where you compute it), even blocking backpropagation\n",
        "when needed, and write your own gradient functions! \n",
        "\n",
        "This is probably more flexibility\n",
        "than you will ever need, even if you build your own custom training loops."
      ],
      "metadata": {
        "id": "ZKN_sWucnW54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Custom Training Loops"
      ],
      "metadata": {
        "id": "MqCpUtsWl1aq"
      }
    }
  ]
}